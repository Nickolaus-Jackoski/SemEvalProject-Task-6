{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OcN72MrV6Hqg"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"transformers>=4.45.0\" \"datasets>=2.20.0\" \"peft>=0.18.0\" \"accelerate>=1.2.0\" bitsandbytes scikit-learn huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 777,
     "referenced_widgets": [
      "ffa0fc34562447e3b0e35163eb6df84a",
      "00608fe6ae5246dca2566c2747769ca4",
      "4cca4988a8bc499d919fd0fa3d42adf5",
      "efecabf86065479685050f79db6973a6",
      "2cf62e956d284422aa607c34e2f65b21",
      "d93b5744fc0b4428af7f705c9aedab0b",
      "10d4e4cf92f24eafa29b982afd75db67",
      "4cc86cd88def4a77906e9f946d45a862",
      "e0ba4ee216ba40f7a458669573a32cd6",
      "6819887385cc48bf8c9ceb427ff2e261",
      "cba38cbcee454ca38239961d72adbc05",
      "0711abdd2ad9439a9ee126b5dead7d97",
      "6b79d614695b4f949c56b71e2cbd3e76",
      "152b3b47a16d49c68dc06be71f1fe732",
      "410090ad41a645d6bf732e90a2db5614",
      "11e617ddd6214131b245534acf5ebcc2",
      "c5c61267565a4182a8026f69f5b4e3de",
      "79d634e2a372429c97c71d7a3ba8efd9",
      "0aac4cb32dc741de8a2d4780df9cd3a2",
      "73644b5969054b1eb701ea1454f8ed7c",
      "2864d0f5691848c6bcd07242a1f06a1b",
      "09fc7198057a4440b7033dfd46b232c9",
      "b288d92d862a467987c962dd56c710d9",
      "76102eec91524fb3bc54d03356f404dc",
      "f2e6c41e2b2c44499d1069256aa3b36e",
      "bc34d431b53943b9bdf6017515128296",
      "3fb9a288f8bb409da517d8fc81d4b1c1",
      "1c5d04b704e54ed3a238c3aded08bbfc",
      "2ab4b19ee1b649f8bead5310fb5c42cb",
      "f3d07baab0044ec994db0d446857c7cd",
      "39bf9350eab64592ad4aa32d263dab9a",
      "bf84988ea7ad4b15bf6ddbcb14c446bd",
      "01b9add7a2a54becb25c7b393a5126fe",
      "dcfc2446d98d4d04862f2332961d546e",
      "cac516c20ba44dedbedbd1e9ff255296",
      "df7e25f921f944a499dfb3226843e403",
      "e8d0cb78dab84a49b89cf120907b500f",
      "467b86508ae34689b8504a4df16753d0",
      "c032a4136d2a40bb8db2ec570a44ba29",
      "f23103172cbc4ba3838de109075ecfd9",
      "c0d4a78d02c44d709462d4a9f397514d",
      "bcaf77631b9342b4ad40c02264c94bbe",
      "b21c0290a3ce41dfa730c6661fe3ce5e",
      "53a82325a4a541c6bac8845fa9846ace",
      "236c699fcb4846d3bcc0df7916ad27b0",
      "ea10490649d34f53bec9a91342351a15",
      "61e53e69992a4d41a593560c348cee92",
      "a64ddb3fd8f54a81b309461f56488754",
      "9f04dce097094d83abe5abe21f3a86da",
      "583b992713dd43e292efd846562d7d8e",
      "ab485db77609490da31f1a3e838bd762",
      "73f1f039b1724852bd268a6a97cd3937",
      "419cfcc0a9874caea49db1081a0e1aaa",
      "bbac10a3805047b5932b1616779c6dbc",
      "5bd47ec5a4a64ef789a1d64277940b64"
     ]
    },
    "id": "dYKXpFjF_ht5",
    "outputId": "3f228de9-ff8e-49ed-956d-42a3e7cd9ef7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa0fc34562447e3b0e35163eb6df84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0711abdd2ad9439a9ee126b5dead7d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b288d92d862a467987c962dd56c710d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfc2446d98d4d04862f2332961d546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236c699fcb4846d3bcc0df7916ad27b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1940' max='1940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1940/1940 1:16:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.215900</td>\n",
       "      <td>1.156969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.903700</td>\n",
       "      <td>1.001858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.872299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.784149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.301500</td>\n",
       "      <td>0.729516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.184400</td>\n",
       "      <td>0.711430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.734914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.776741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.858176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.051400</td>\n",
       "      <td>0.922108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0: Ambivalent Pred: Ambivalent\n",
      "Test 50: Ambivalent Pred: Ambivalent\n",
      "Test 100: Ambivalent Pred: Ambivalent\n",
      "Test 150: Clear Reply Pred: Clear Reply\n",
      "Test 200: Ambivalent Pred: Ambivalent\n",
      "Test 250: Ambivalent Pred: Ambivalent\n",
      "Test 300: Ambivalent Pred: Ambivalent\n",
      "Task 1 Macro F1: 0.6066\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_fb1fd0c3-e7e4-41f7-a58e-e039b7bc89bc\", \"llama_task1_predictions_lora.csv\", 4716)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q -U torch transformers peft accelerate datasets scikit-learn pandas\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"/content/hf_cache\"\n",
    "set_seed(42)\n",
    "\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "OUTPUT_DIR = \"./task1_llama_lora_16bit\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "full_train = dataset[\"train\"]\n",
    "official_test = dataset[\"test\"]\n",
    "full_train = full_train.class_encode_column(\"clarity_label\")\n",
    "\n",
    "\n",
    "splits = full_train.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"clarity_label\")\n",
    "train_ds = splits[\"train\"]\n",
    "dev_ds = splits[\"test\"]\n",
    "\n",
    "def format_prompt(example):\n",
    "    q = example['question'] or example['interview_question']\n",
    "    a = example['interview_answer']\n",
    "    label_int = example['clarity_label']\n",
    "    features = full_train.features['clarity_label']\n",
    "    label_str = features.int2str(label_int)\n",
    "\n",
    "    text = (f\"Question: {q}\\nAnswer: {a}\\n\\n\"\n",
    "            \"Classify the clarity of this answer. Options:\\n\"\n",
    "            \"- Clear Reply\\n- Ambivalent\\n- Clear Non-Reply\\n\\n\"\n",
    "            f\"Label: {label_str}\")\n",
    "    return {\"text\": text, \"label\": label_str}\n",
    "\n",
    "train_ds = train_ds.map(format_prompt)\n",
    "dev_ds = dev_ds.map(format_prompt)\n",
    "\n",
    "test_ds = official_test.map(lambda x: {\n",
    "    \"text\": f\"Question: {x['question']}\\nAnswer: {x['interview_answer']}\\n\\n\"\n",
    "            \"Classify the clarity of this answer. Options:\\n\"\n",
    "            \"- Clear Reply\\n- Ambivalent\\n- Clear Non-Reply\\n\\nLabel:\",\n",
    "    \"label\": x[\"clarity_label\"]\n",
    "})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    outputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "    return outputs\n",
    "\n",
    "cols_to_remove = train_ds.column_names\n",
    "tokenized_train = train_ds.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)\n",
    "tokenized_dev = dev_ds.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps=10,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    eval_strategy=\"epoch\",        \n",
    "    save_strategy=\"epoch\",     \n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\", \n",
    "    greater_is_better=False,     \n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "gen_config = {\n",
    "    \"max_new_tokens\": 10,\n",
    "    \"do_sample\": False,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"eos_token_id\": tokenizer.eos_token_id\n",
    "}\n",
    "\n",
    "def clean_prediction(text):\n",
    "    text = text.lower()\n",
    "    if \"non-reply\" in text: return \"Clear Non-Reply\"\n",
    "    if \"ambivalent\" in text or \"ambiguous\" in text: return \"Ambivalent\"\n",
    "    if \"clear reply\" in text: return \"Clear Reply\"\n",
    "    return \"Clear Reply\"\n",
    "\n",
    "for i, row in enumerate(test_ds):\n",
    "    inputs = tokenizer(row[\"text\"], return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, **gen_config)\n",
    "\n",
    "    gen_text = tokenizer.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
    "    pred = clean_prediction(gen_text)\n",
    "    predictions.append(pred)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Test {i}: {gen_text} Pred: {pred}\")\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "label_map = {\"Clear Reply\": 0, \"Ambivalent\": 1, \"Clear Non-Reply\": 2}\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for t, p in zip(test_ds[\"label\"], predictions):\n",
    "    if t in label_map and p in label_map:\n",
    "        y_true.append(label_map[t])\n",
    "        y_pred.append(label_map[p])\n",
    "\n",
    "final_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "print(f\"Task 1 Macro F1: {final_f1:.4f}\")\n",
    "\n",
    "df = pd.DataFrame({\"index\": test_ds[\"index\"], \"clarity_label\": predictions})\n",
    "df.to_csv(\"llama_task1_predictions_lora.csv\", index=False)\n",
    "files.download(\"llama_task1_predictions_lora.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
