{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449,
     "referenced_widgets": [
      "a8527fd6c60542b5a6b434df84766283",
      "6deadaadc03d4c3aaa488089928bb52e",
      "823f9949cb17434da61420d92e9831b9",
      "a0a6a7cbe6f349f0949e614040534ba7",
      "6e1e3313bdd644a8b9f178a97d1b9c45",
      "df64f0b99ad44b19b55e3374568434c6",
      "0f3e33375df0424283c818415845a7ac",
      "029901c611ba4bab913ecdfb4778358e",
      "2f0a6d2b7e39439cb9648950be68c4e1",
      "a3d54ae55d7e4698ac9caf9c25f26fc9",
      "65227351c03b4a4083ffad9b598ae077",
      "21df517000854480a66419a173f60bd7",
      "ae49ba0ec92047fab1222e09c9901509",
      "fd3d4a8457d44d108f683d59f98ddf81",
      "b6240aee46334aceb71810fca4ccf80e",
      "5ecd1fb6f31449d1bc48e15434f1da4b",
      "8efd062b2c00433b999bf81078eefe0d",
      "36b10208dd2a4ce6bfe53e3b5cc69425",
      "258d091c3e72494491c75d84054bb8be",
      "c8883e1b438644fabdaae037f2483c2c",
      "a5184960a5b04117b515b07885f39d5e",
      "4b4e3d205baa404dae1e0bd197f54e5f",
      "8b9b210f762b4aba81c21050c6dd1392",
      "8f19f42411bf405db27c12248ad27eda",
      "59ec139efd6d412a9ccff2bec8b52965",
      "8e93f93f1f9344f0bf96f278d531c947",
      "6a2cbc7637f947dcbde6bbfcf5815052",
      "6d5a5249abf4488b8d54b8dec3cf2674",
      "9212a435735242889b38f8ecadd15b3a",
      "24aad5cc36d641d383705bdd3f2158b9",
      "16737497dac74d72a6ed48284d4cb2e0",
      "a442f6c3bbcf4822a6fa8a76e33ae591",
      "de99b5eba43b45beb34e60a00d3e3cdd",
      "6952fd3578414f2985947b61d2370eea",
      "e95a9ff244ac438b90acc570540ddea0",
      "d2e5f4874de04f3392d2f5de13ea4780",
      "01b087df3ff142deb395994db8d121ac",
      "0e8ada6806674eebbef0059bdd5d96b0",
      "c4831fdb09a8418884f5bf95ba4cb5b4",
      "97a88fe0ff1d4dd4a36dc31507e7394e",
      "cc803075bcd843b4a4e210b21f2097e5",
      "1215259fbf8d40b28c54879227dc0b68",
      "d08b44efbe484087961226049cca031e",
      "8a9659b2b12542819d15b6c9ba71021f",
      "efceb5da83e142039cacec38df908e2c",
      "9960a4306a284f0db02dce554c84573d",
      "874441903148488e931ff2ea1813b74d",
      "6835ff03e0b44d9ea4e4ce5e86d77a4e",
      "532c6dfeab9346af8d7e4482faa9c054",
      "e372d84f67004563914287c20c9fe73f",
      "c41eb04ab2a6413984526e6a784298e4",
      "087a336203624919875afc026c61ce82",
      "99638bcc263c4cd99b3e5d173ac4bfa7",
      "91a3bf15bba549f897d67c2554d28bd4",
      "eaba259696474a1bac35ce77105f84b1",
      "03fad34bccfd411bb5371419883faf14",
      "9b9549956e8046b3bde8237f138ccc6d",
      "6f545b1bf7b44b9789d4136615e5be62",
      "2159c832889c4c5f8d7ebef76146021e",
      "10eedf8aa5224a598ecbc2e41920a1ca",
      "546225f7bf5e4d88a98320f8aa9bdd83",
      "82ec06ef81634ad192fe08142aca8b73",
      "a085b28daf174c959e699f820e42ac88",
      "dd147584810a466288b8e857ee340454",
      "a4eb09db0c4f4a889cce3bd39309d92c",
      "b16159bbdbe24458a58129b9a81397d1",
      "8092aeba89c44132bb8b963e6b327a18",
      "85ae3012f9984059bfd8cc46df347fda",
      "9cbe656741d14785b7342e8b7790a052",
      "2dd7f3203cbb485c9e3b3057f31e6d39",
      "e8ba1d990fca44d393c006e5737b830e",
      "d295df6f95ac47ac93bd6e4737c4e78e",
      "30cf4a7c6b0a488eb738e279318a983a",
      "7d7f02c6bbdb453a9b945c2496273dd5",
      "5a6c743dfcda49c8b378af6237ad34ab"
     ]
    },
    "id": "O916zgqDdwxU",
    "outputId": "e9b320b0-b791-40b0-c8f2-ed3f5555d147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8527fd6c60542b5a6b434df84766283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 labels for experiment: evasion_based_clarity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b10208dd2a4ce6bfe53e3b5cc69425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9212a435735242889b38f8ecadd15b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a88fe0ff1d4dd4a36dc31507e7394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41eb04ab2a6413984526e6a784298e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ec06ef81634ad192fe08142aca8b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 3103\n",
      "Eval set size: 345\n"
     ]
    }
   ],
   "source": [
    "# Install required packages and login\n",
    "!pip install -q transformers datasets peft trl accelerate bitsandbytes scikit-learn tqdm\n",
    "\n",
    "from huggingface_hub import login\n",
    "login()\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import bfloat16\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Configuration\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "experiment = \"evasion_based_clarity\"  \n",
    "batch_size = 2\n",
    "num_epochs = 4\n",
    "lr = 1e-4\n",
    "output_dir = \"./outputs_llama3_lora_offload\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Label mapping\n",
    "if experiment == \"evasion_based_clarity\":\n",
    "    mapping_labels = {\n",
    "        \"Explicit\": 0,\n",
    "        \"Implicit\": 1,\n",
    "        \"Dodging\": 2,\n",
    "        \"General\": 3,\n",
    "        \"Deflection\": 4,\n",
    "        \"Partial/half-answer\": 5,\n",
    "        \"Declining to answer\": 6,\n",
    "        \"Claims ignorance\": 7,\n",
    "        \"Clarification\": 8,\n",
    "    }\n",
    "    label_field = \"evasion_label\"\n",
    "elif experiment == \"direct_clarity\":\n",
    "    mapping_labels = {\n",
    "        \"Clear Reply\": 0,\n",
    "        \"Ambivalent\": 1,\n",
    "        \"Clear Non-Reply\": 2,\n",
    "    }\n",
    "    label_field = \"clarity_label\"\n",
    "\n",
    "id2label = {v: k for k, v in mapping_labels.items()}\n",
    "num_labels = len(mapping_labels)\n",
    "print(f\"Using {num_labels} labels for experiment: {experiment}\")\n",
    "\n",
    "# Instruction header\n",
    "INSTRUCTION_HEADER = \"\"\"You are an expert annotator for political interview clarity classification.\n",
    "\n",
    "You are given an interview question and an answer from a politician.\n",
    "Your task is to classify how the answer addresses the question,\n",
    "using exactly one of the following labels:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if experiment == \"evasion_based_clarity\":\n",
    "    INSTRUCTION_HEADER += \"\"\"1. Explicit – The information requested is explicitly stated (in the requested form).\n",
    "2. Implicit – The information requested is given, but without being explicitly stated (not in the expected form).\n",
    "3. Dodging – Ignoring the question altogether.\n",
    "4. Deflection – Starts on topic but shifts the focus and makes a different point than what is asked.\n",
    "5. Partial/half-answer – Offers only a specific component of the requested information.\n",
    "6. General – The information provided is too general/lacks the requested specificity.\n",
    "7. Declining to answer – Acknowledge the question but directly or indirectly refusing to answer at the moment.\n",
    "8. Claims ignorance – The answerer claims/admits not to know the answer themselves.\n",
    "9. Clarification – Does not provide the requested information and asks for clarification.\n",
    "\"\"\"\n",
    "else:\n",
    "    INSTRUCTION_HEADER += \"\"\"1. Clear Reply – A clear, direct answer to the question.\n",
    "2. Ambivalent – The answer is partially addressing the question or is ambiguous.\n",
    "3. Clear Non-Reply – The answer does not address the question at all.\n",
    "\"\"\"\n",
    "\n",
    "INSTRUCTION_HEADER += \"\"\"\n",
    "\n",
    "Read the following interview question and answer segment.\n",
    "Then output the label in the format: \"Label: <LABEL>\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Prompt building functions\n",
    "def build_prompt(data):\n",
    "    system_prompt = INSTRUCTION_HEADER\n",
    "    user_prompt = f\"Interview Question: {data['interview_question']}\\n\\nFull Answer: {data['interview_answer']}\\n\\nLabel:\"\n",
    "    assistant_response = f\"{data[label_field]}\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_response},\n",
    "    ]\n",
    "\n",
    "def build_messages_dataset(raw_dataset):\n",
    "    messages_list = []\n",
    "    for row in raw_dataset:\n",
    "        msgs = build_prompt(row)\n",
    "        messages_list.append({\"messages\": msgs})\n",
    "    return messages_list\n",
    "\n",
    "# Load and split dataset\n",
    "raw_dataset = load_dataset(\"ailsntua/QEvasion\", split=\"train\")\n",
    "rows = [row for row in raw_dataset]\n",
    "labels = [row[label_field] for row in raw_dataset]\n",
    "\n",
    "train_rows, eval_rows = train_test_split(\n",
    "    rows,\n",
    "    test_size=0.1,\n",
    "    random_state=3407,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "dataset = build_messages_dataset(train_rows)\n",
    "train_dataset = Dataset.from_list(dataset)\n",
    "eval_dataset = Dataset.from_list(build_messages_dataset(eval_rows))\n",
    "\n",
    "print(f\"Train set size: {len(train_dataset)}\")\n",
    "print(f\"Eval set size: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233,
     "referenced_widgets": [
      "534062785e90450996aabe406ae051e4",
      "c27ca4ca834643ae8ddfa27e397a03be",
      "1a3ad7e92e234a2bb5e54d6c59a1331a",
      "0e05d8c8cb444de1b45ea6400b45718a",
      "290e6d9c6928409789e11a93b8be597c",
      "eacc8f67cc724cdabfb9dd59bc748c37",
      "702c076c890d4a8f910861ba9ccc983a",
      "316cb38ee5ca4ceca3efe530a90994dc",
      "128e2f129c5645c6b626d0b6fbb2890c",
      "fc004a88c3b645b8ac59e052b9a4c2bf",
      "bea3d6ab57d6437b9b483839b6f583a3",
      "2d41209bb536480a8d42c4bb09ecd89c",
      "1ea466869c754334a1cee0adee666f9f",
      "28da0fd2bfb749d3b7fc1c301f833e9d",
      "3179e1cfaeb64516be3ebef6f34da0df",
      "23e14c8e31d64c70be7f8f59a90c9ff8",
      "9ea8cc7a40a04f14b12d133f38a45730",
      "bc3b514e89c04977ac494e0cceada96a",
      "05524bec2e5245a281ea018ece17484b",
      "bbca863c9fc14152877f223cdeb9cfe4",
      "8f3220f0e5de434c8722309424d37e9c",
      "07c0c58c1f224a77922e4e05e872cc6e",
      "6a629681d0414b61a4b7360b06903192",
      "9a6b238beb6b43bcb55e04de04cfaa59",
      "4f621cb8a13c417e9d075d298dd39f01",
      "0df70f52cb6f42b891bdcbed3f426ba1",
      "a4f0a86eccfe45138e8e7390b24f519d",
      "4bea745b147940aebc5ad389c84bc048",
      "5808b0c5371e47ea9504c871c4ed71ac",
      "60a083387cc3418391ddadba7d14500f",
      "add0849886fe42eb8fb659773c3f0bad",
      "d3d2c27b879f41deb5d12466f7d633be",
      "11de69ac319a4ce6a92ad44206ac1d2c",
      "8f2ee8b111314e1cad4787739908b9ea",
      "5adfa4076a8f405194d3ab8858d6e023",
      "4260bbbadfbc4f5095fee635ed776470",
      "1c56ed2812b74da7beb2f02ae257a781",
      "8fbd113051ca412d9c2612c944724182",
      "116c7760edff40fd8655e26f47148205",
      "e06b8127678549238583e3536e512e22",
      "eca6652026d14a4e9e59f6cb5d4d864d",
      "048a9b6caa154fb7a3a6fbf7ef84e9ff",
      "3620ceca36a445bea9259ad4d7629467",
      "23bfb714911f4e3a97bcd2304edc0a87",
      "9958b464398d405cbcde102e526fe3c7",
      "d7755b318b3f4aea9c6686279d3513ab",
      "3ef238f42f1e407496a92b2ed74b3c64",
      "a536e9aeff7845e099cb008f377c4a43",
      "10ca0245c824441aa11585beda89f9fc",
      "d134ba9ee71c441e8f31764e254d2678",
      "8a759941d241474c968d86a0e40e80e4",
      "cba4459d41f14060b3735c7aee1b1815",
      "0e00d9676c7a4a9a982bbcf4755a9afd",
      "fb6a21e4120f44ea8a8a98631bfdcfd4",
      "09494deaf81843419dd2c6a32b92c892"
     ]
    },
    "id": "uIZ8nx_PetJU",
    "outputId": "8eb22637-75a2-459b-a9f9-5f1d5dff335c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534062785e90450996aabe406ae051e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d41209bb536480a8d42c4bb09ecd89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a629681d0414b61a4b7360b06903192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3103 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2ee8b111314e1cad4787739908b9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9958b464398d405cbcde102e526fe3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Ready to train.\n"
     ]
    }
   ],
   "source": [
    "# Setup tokenizer, model, and trainer\n",
    "LLAMA3_MASKING_TEMPLATE = (\n",
    "    \"{% set loop_messages = messages %}\"\n",
    "    \"{% for message in loop_messages %}\"\n",
    "        \"{% if loop.index0 == 0 %}\"\n",
    "            \"{% set start_token = bos_token %}\"\n",
    "        \"{% else %}\"\n",
    "            \"{% set start_token = '' %}\"\n",
    "        \"{% endif %}\"\n",
    "\n",
    "        \"{% if message['role'] == 'assistant' %}\"\n",
    "            \"{{ start_token + '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n\\n' }}\"\n",
    "            \"{% generation %}\"\n",
    "            \"{{ message['content'] | trim + '<|eot_id|>' }}\"\n",
    "            \"{% endgeneration %}\"\n",
    "        \"{% else %}\"\n",
    "            \"{{ start_token + '<|start_header_id|>' + message['role'] + '<|end_header_id|>' + '\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\"\n",
    "        \"{% endif %}\"\n",
    "    \"{% endfor %}\"\n",
    "    \"{% if add_generation_prompt %}\"\n",
    "        \"{{ '<|start_header_id|>assistant<|end_header_id|>' + '\\n\\n' }}\"\n",
    "    \"{% endif %}\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = LLAMA3_MASKING_TEMPLATE\n",
    "\n",
    "lora_args = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "sft_args = SFTConfig(\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_steps=len(dataset) * num_epochs // (batch_size * 8),\n",
    "    warmup_steps=len(dataset) * num_epochs // (batch_size * 8 * 20),\n",
    "    learning_rate=lr,\n",
    "    logging_steps=10,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.001,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    output_dir=output_dir,\n",
    "    batch_eval_metrics=True,\n",
    "    dataset_text_field=None,\n",
    "    packing=False,\n",
    "    assistant_only_loss=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    args=sft_args,\n",
    "    peft_config=lora_args,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "print(\"Setup complete. Ready to train.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y8bVD6BSe6fL",
    "outputId": "f9ed2e1c-597e-4dee-d49a-0a611538bfed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='775' max='775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [775/775 1:00:40, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>9.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.760800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.788300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.659800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.655500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.784400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.556400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.668000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.699700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.626100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.551000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.596000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.555700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.592000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.573000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.545400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.511400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.556700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.474500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.460300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.479700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.476400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.408700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.428300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.358900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.377100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.325500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWC1grMLfT27",
    "outputId": "e3926ee8-50f3-459d-fab2-685628634205"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/44 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "100%|██████████| 44/44 [00:54<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 345 predictions\n",
      "Valid predictions: 345/345\n",
      "\n",
      "================================================================================\n",
      "EVALUATION METRICS\n",
      "================================================================================\n",
      "\n",
      "Overall Accuracy: 0.4203\n",
      "\n",
      "Macro-averaged Metrics:\n",
      "  Precision: 0.4063\n",
      "  Recall:    0.4096\n",
      "  F1-Score:  0.4073\n",
      "\n",
      "Per-class Metrics:\n",
      "Label                     Precision    Recall       F1-Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "Explicit                  0.5327       0.5429       0.5377       105       \n",
      "Implicit                  0.2593       0.2857       0.2718       49        \n",
      "Dodging                   0.4026       0.4366       0.4189       71        \n",
      "General                   0.2857       0.2564       0.2703       39        \n",
      "Deflection                0.3158       0.3158       0.3158       38        \n",
      "Partial/half-answer       0.0000       0.0000       0.0000       8         \n",
      "Declining to answer       0.6154       0.5714       0.5926       14        \n",
      "Claims ignorance          0.5455       0.5000       0.5217       12        \n",
      "Clarification             0.7000       0.7778       0.7368       9         \n",
      "\n",
      "================================================================================\n",
      "Detailed Classification Report:\n",
      "================================================================================\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "           Explicit       0.53      0.54      0.54       105\n",
      "           Implicit       0.26      0.29      0.27        49\n",
      "            Dodging       0.40      0.44      0.42        71\n",
      "            General       0.29      0.26      0.27        39\n",
      "         Deflection       0.32      0.32      0.32        38\n",
      "Partial/half-answer       0.00      0.00      0.00         8\n",
      "Declining to answer       0.62      0.57      0.59        14\n",
      "   Claims ignorance       0.55      0.50      0.52        12\n",
      "      Clarification       0.70      0.78      0.74         9\n",
      "\n",
      "           accuracy                           0.42       345\n",
      "          macro avg       0.41      0.41      0.41       345\n",
      "       weighted avg       0.41      0.42      0.42       345\n",
      "\n",
      "\n",
      "Model saved to ./outputs_llama3_lora_offload/new_model\n",
      "Results saved to ./outputs_llama3_lora_offload/evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "# Run inference and calculate metrics\n",
    "# Check that model and tokenizer are defined (run previous cells first!)\n",
    "if 'model' not in globals() or 'tokenizer' not in globals():\n",
    "    raise NameError(\"Model or tokenizer not defined. Please run cells 1, 2, and 3 first!\")\n",
    "\n",
    "model.eval()\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "def format_prompt_for_inference(example):\n",
    "    input_messages = example[\"messages\"][:-1]\n",
    "    if input_messages[-1][\"role\"] != \"user\":\n",
    "        pass\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        input_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "ground_truths = [ex[\"messages\"][-1][\"content\"] for ex in eval_dataset]\n",
    "\n",
    "# Run batch inference\n",
    "eval_batch_size = 8\n",
    "generated_texts = []\n",
    "\n",
    "print(\"Starting Inference...\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for i in tqdm(range(0, len(eval_dataset), eval_batch_size)):\n",
    "    batch_indices = range(i, min(i + eval_batch_size, len(eval_dataset)))\n",
    "    batch_examples = [eval_dataset[idx] for idx in batch_indices]\n",
    "\n",
    "    prompts = [format_prompt_for_inference(ex) for ex in batch_examples]\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    decoded_output = tokenizer.batch_decode(outputs[:, input_len:], skip_special_tokens=True)\n",
    "    generated_texts.extend(decoded_output)\n",
    "\n",
    "print(f\"Generated {len(generated_texts)} predictions\")\n",
    "\n",
    "# Parse predictions and ground truth\n",
    "def parse_prediction(text):\n",
    "    text = text.strip().lower()\n",
    "    for label_name, label_id in mapping_labels.items():\n",
    "        if text.startswith(label_name.lower()):\n",
    "            return label_id\n",
    "    return -1\n",
    "\n",
    "y_pred = [parse_prediction(text) for text in generated_texts]\n",
    "y_true = []\n",
    "\n",
    "for gt in ground_truths:\n",
    "    if gt in mapping_labels:\n",
    "        y_true.append(mapping_labels[gt])\n",
    "    else:\n",
    "        y_true.append(-1)\n",
    "\n",
    "# Filter out invalid predictions (-1)\n",
    "valid_indices = [i for i in range(len(y_pred)) if y_pred[i] != -1 and y_true[i] != -1]\n",
    "y_pred_valid = [y_pred[i] for i in valid_indices]\n",
    "y_true_valid = [y_true[i] for i in valid_indices]\n",
    "\n",
    "print(f\"Valid predictions: {len(y_pred_valid)}/{len(y_pred)}\")\n",
    "if len(y_pred) - len(y_pred_valid) > 0:\n",
    "    print(f\"Warning: {len(y_pred) - len(y_pred_valid)} predictions could not be parsed\")\n",
    "\n",
    "# Calculate all metrics\n",
    "accuracy = accuracy_score(y_true_valid, y_pred_valid)\n",
    "\n",
    "# Macro-averaged metrics\n",
    "macro_precision = precision_recall_fscore_support(\n",
    "    y_true_valid, y_pred_valid, average=\"macro\", zero_division=0\n",
    ")[0]\n",
    "macro_recall = precision_recall_fscore_support(\n",
    "    y_true_valid, y_pred_valid, average=\"macro\", zero_division=0\n",
    ")[1]\n",
    "macro_f1 = f1_score(y_true_valid, y_pred_valid, average=\"macro\")\n",
    "\n",
    "# Per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_true_valid, y_pred_valid, average=None, zero_division=0, labels=list(mapping_labels.values())\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"\\nMacro-averaged Metrics:\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall:    {macro_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nPer-class Metrics:\")\n",
    "print(f\"{'Label':<25} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
    "print(\"-\" * 80)\n",
    "for label_name, label_id in sorted(mapping_labels.items(), key=lambda x: x[1]):\n",
    "    if label_id < len(precision):\n",
    "        print(f\"{label_name:<25} {precision[label_id]:<12.4f} {recall[label_id]:<12.4f} {f1[label_id]:<12.4f} {support[label_id]:<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Detailed Classification Report:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(\n",
    "    y_true_valid, y_pred_valid,\n",
    "    target_names=list(mapping_labels.keys()),\n",
    "    labels=list(mapping_labels.values()),\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# Save the fine-tuned model and results\n",
    "trainer.save_model(output_dir + \"/new_model\")\n",
    "print(f\"\\nModel saved to {output_dir}/new_model\")\n",
    "\n",
    "with open(os.path.join(output_dir, \"evaluation_results.txt\"), \"w\") as f:\n",
    "    f.write(f\"Model output - {experiment}\\n\")\n",
    "    f.write(f\"\\nOverall Accuracy: {accuracy:.4f}\\n\")\n",
    "    f.write(f\"Macro Precision: {macro_precision:.4f}\\n\")\n",
    "    f.write(f\"Macro Recall: {macro_recall:.4f}\\n\")\n",
    "    f.write(f\"Macro F1-Score: {macro_f1:.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    f.write(\"\\nDetailed Results:\\n\")\n",
    "    for i, text in enumerate(generated_texts):\n",
    "        f.write(f\"\\nExample {i+1}:\\n\")\n",
    "        f.write(f\"Generated: {text}\\n\")\n",
    "        f.write(f\"Ground Truth: {ground_truths[i]}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_dir}/evaluation_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75kKHO2SIgBQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KCOBJ3DfVtI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
