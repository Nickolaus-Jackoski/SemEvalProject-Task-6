{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "# Setup reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Config\n",
    "MODEL_NAME = \"roberta-large\"\n",
    "OUTPUT_DIR = \"./roberta_large_final\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 8  # Effective batch size of 32\n",
    "LR = 1e-5              # RoBERTa tolerates slightly higher LR than DeBERTa\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "# Text formatting\n",
    "def format_inputs(example):\n",
    "    # Using the pipe separator style which worked well for your DeBERTa run\n",
    "    clarity = example.get('clarity_label') or 'Unknown'\n",
    "    text = (\n",
    "        f\"Context: {clarity} | \"\n",
    "        f\"Question: {example['question']} \"\n",
    "        f\"Answer: {example['interview_answer']}\"\n",
    "    )\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Processing data...\")\n",
    "processed_ds = dataset[\"train\"].map(format_inputs)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(format_inputs)\n",
    "\n",
    "processed_ds = processed_ds.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# Splits\n",
    "# Split off a clean holdout set (10%)\n",
    "main_split = processed_ds.train_test_split(\n",
    "    test_size=0.1, seed=SEED, stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev = main_split[\"train\"]\n",
    "holdout_test = main_split[\"test\"]\n",
    "\n",
    "# Split Train into Train/Val (10% val)\n",
    "inner_split = train_dev.train_test_split(\n",
    "    test_size=0.1, seed=SEED, stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_set = inner_split[\"train\"]\n",
    "val_set = inner_split[\"test\"]\n",
    "\n",
    "# Mappings\n",
    "labels = train_set.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_set = train_set.map(tokenize_fn, batched=True)\n",
    "val_set = val_set.map(tokenize_fn, batched=True)\n",
    "holdout_test = holdout_test.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Rename label column for trainer\n",
    "train_set = train_set.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "val_set = val_set.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "holdout_test = holdout_test.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "# Class Weights\n",
    "y_vals = train_set[\"evasion_label\"]\n",
    "weights = compute_class_weight(\"balanced\", classes=np.unique(y_vals), y=y_vals)\n",
    "weights_tensor = torch.tensor(weights, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Weighted CE with smoothing\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=weights_tensor, label_smoothing=0.1)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Training Args\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating on Holdout Test Set...\")\n",
    "res = trainer.evaluate(holdout_test)\n",
    "print(f\"Holdout Macro F1: {res['eval_macro_f1']:.4f}\")\n",
    "\n",
    "# Submission file generation\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission file...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "    # Ensure index column exists\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    preds_output = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(preds_output.predictions, axis=-1)\n",
    "    pred_labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"index\": comp_test_ds[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })\n",
    "\n",
    "    csv_name = \"roberta_submission.csv\"\n",
    "    df.to_csv(csv_name, index=False)\n",
    "    print(f\"Saved {csv_name}\")\n",
    "    files.download(csv_name)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "0675f3d3e6ab43208ce1ada100ddbb0f",
      "3c73b6e058f44a359c0729811f9d3dfa",
      "aeeb124fb4b84c418045de24d0cafcc9",
      "835f466fb2e64709b4de24f3d8a3db35",
      "52de4e78c12342c8b4d43dcc54ae6d74",
      "edd9a5076712424baee485c15b829eb6",
      "46bcf29d52f148b99053e3e2e7bea90a",
      "70f1419c34594101ad52fcc5a4f91c80",
      "2934588fb3594245bf86da9c73d2c3b6",
      "811ab75db38c4811994e9b2a3e053a0e",
      "314977170ddd4fbb89a6fa1317689b44",
      "762f7fe783904c26b1030eff74e059b2",
      "52c6976322344c2a873f5519ec050dfd",
      "26ea6706196444cdaeb67338579ff8f3",
      "fa3f06168def48c39fa990adb8976f0d",
      "80f75c7df72041efa5f031bb14f7d879",
      "3610fc5767d3424b9f45f634b35b97a0",
      "2b55be0115624dd7b7ebc9baf1904039",
      "67b63ee26ffa40b381df7a84fb5170a9",
      "a5b3bb9b61ec4f64923c691f413606e6",
      "6bb0fd84546d4bca9eda331175aa6969",
      "47230c29270e41408a47342245a8ff16",
      "43fd99a1c8d4445289ec5ed3bfc9a47e",
      "bac8e53e8dfb4bb19b8282dcd3c06035",
      "a9ad9bde843b4f63851e702c2e6e1816",
      "1f04781fd913433c83a0ddabd3a7ee41",
      "77f4571464eb4f97bf157245e848025f",
      "474034b02e974c0f86d8372ffcb2a701",
      "0e3eec559a8a4fc58398d77f67dc1d95",
      "9d477f934a2c4c918a04bb07accaeec5",
      "9ad79bfab3df4fafba192f62aa637542",
      "9d1588834e854affb58e9a3ff6639f74",
      "64ee1e7f5dba4440a318c97dda6f979b",
      "8e9471bf57e347d297bacdcc132cfb5a",
      "aa7869de5f6047159d48ff7b6ddad9ff",
      "d03004b0651d4ff59359bb4c166dc019",
      "000f297427044d72824d83211f8f4987",
      "5e7d21ab7c6e4cf9bf44bb9f20f88266",
      "14709f2a6b344d4fac651ba93d5b8054",
      "58dea1517f234367bbf1525ab570d3ef",
      "363e45bacf5543f5bfb459f7986f8dc5",
      "80d4b8187e774a3ca8d99bb1bd70a08d",
      "5e5a68e831cc4646a1de1782e84639a8",
      "a8785e86d25e4b5a9309d1ce40476958",
      "0e0e3799ebba46ffad2fb3df7b845ffb",
      "a62494e7aae34038980aee4d01902eb8",
      "60c85a9e1c50403e84bb57ce118f285b",
      "8923817ef96a4c919a8a84c1fb5bf12b",
      "7974eca866744e0090c642be75564d9d",
      "e7a986f747934f54b37d9b0ca81aff9d",
      "446452f8890344cb9f53cea05a1f86af",
      "7de5c39d994b4897962d3cd02c4f33c2",
      "be22956d2133467cbb91c6f3cdcf5d0a",
      "befdeac49ea449bab0b59a38b0d97dfb",
      "25e176732cbe4a2b8f360a0d727d9ea1",
      "bf93900eef8a4d6aaded37c43dd14ffd",
      "03711215e29c493a950b60b5d208b9d1",
      "8795caa8991f4118b0dc174c9c69783f",
      "d5daf4b00c344196aa8eb3c86482e738",
      "9bf5c4175dc146b3af55d3103dccd6ed",
      "6b323a88d4e641f0abe22e3c7f78ac73",
      "434b952bd5ff460996cd4e8780e60cb8",
      "fde2f9c184e84ead981a711b269465f0",
      "963e9eedee324190bd8bc77c93208e97",
      "8551d8ffdd0a4128b40037eb52968cd6",
      "2b9b55f842a44e6bb9ed513c7b82db18",
      "fa1d95d717cc456a83b123d1359bb0ed",
      "74dc21ef06fb409cabddd8acfe42ea28",
      "fed0863d82af41ce977971e0f164736a",
      "75663d5cee4e479abd69b73d34640c13",
      "35a30e73cd8e405cb0e3fef3cbf9f744",
      "1902b3dbbf2e4028979540c7b831cb0a",
      "7391b185cd694c6384c91ccce0011f55",
      "12e79b21ec5f439ebaf8d2d754b73ba7",
      "696ac6b08538402da015289ff0ac8abe",
      "4722ec8b398d42d482d285b0dc343342",
      "af2ea450129841068b00e1e540a10e17",
      "5be69ac8c1664936a1e89f0ee40dea4c",
      "c5ffe907ddb94156bc7e2e414177d52a",
      "01683ae268524069b1fd37db4af93884",
      "5bc0a47775084ff4b50ba59e386b9a71",
      "8f5077ddc6ab479e8d027ef263595e34",
      "195a683ce2f64ddfb40d675a24644b91",
      "f12c0094f70e419689c41c0f97c0812d",
      "a18df0d378264c7086095eca8820d438",
      "2201f86b1d1941aab69e2d9b499379be",
      "32dfa27f15be4e2fb269d91d9d5031e1",
      "56c0cc5d1e51428693e748b0ae9e4dea",
      "63bf79b2a7d5461eafb6d609074fc55f",
      "d46bf7219ccd4812b4bece9a6df2adcd",
      "155a3b075b144f45ae27d4ca6eabc3db",
      "05f0c9f68b6d403d8a999635b83118d6",
      "ce06c06b25b14b528ec37618da430665",
      "c945d6f6f4544114bc8c5c24b45d6a4c",
      "e5492c37ef5f437db9b36a59fd821626",
      "671469b536c54d88923861bf984c85aa",
      "d550ed05ada44363a96eeb8424596660",
      "66f14ee1df9c4c0b96c20d4371fc8157",
      "9ba710c0fb3e445395f1f62fda62a018",
      "9c544eeee0124c96a61a4d44deb5b0c4",
      "c131158735004e29b318c9306bdc81b6",
      "c7c714daac3940588b3a662808984fe6",
      "b169adbb61384442a4837247848af165",
      "42dd0fff04d942c188cf10dc1b4539f0",
      "ac83c11511e14bf2ad58de66a2673163",
      "b3af2e30688748cbbc610b013f145b18",
      "f82bb64731e7420cab6ae2bc8e5087e3",
      "aa25b2c7704e4c3ab6a750726eea4dcc",
      "c249602962e447cebb7dccf6ed8ead6f",
      "ea18b97a2f97441d8b0dff933f385159"
     ]
    },
    "id": "vf_B_0Nx0X7g",
    "outputId": "6df33119-e3ba-4ae7-e916-ccbe325e73a0"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading dataset...\n",
      "Processing data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0675f3d3e6ab43208ce1ada100ddbb0f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "762f7fe783904c26b1030eff74e059b2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43fd99a1c8d4445289ec5ed3bfc9a47e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e9471bf57e347d297bacdcc132cfb5a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e0e3799ebba46ffad2fb3df7b845ffb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf93900eef8a4d6aaded37c43dd14ffd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa1d95d717cc456a83b123d1359bb0ed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5be69ac8c1664936a1e89f0ee40dea4c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63bf79b2a7d5461eafb6d609074fc55f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 16:19, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.334232</td>\n",
       "      <td>0.453376</td>\n",
       "      <td>0.211084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.898406</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.337304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.827412</td>\n",
       "      <td>0.546624</td>\n",
       "      <td>0.404571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.710673</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.578972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.760718</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.512345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.737058</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.577149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.753983</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.584343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.757748</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.583255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.816134</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.607920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.838868</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.603288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>1.858799</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.616022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.392800</td>\n",
       "      <td>1.892210</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.610872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.392800</td>\n",
       "      <td>1.933980</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.613904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.392800</td>\n",
       "      <td>1.938078</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.608281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.392800</td>\n",
       "      <td>1.937824</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.608216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Evaluating on Holdout Test Set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Holdout Macro F1: 0.5757\n",
      "\n",
      "Generating submission file...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c544eeee0124c96a61a4d44deb5b0c4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved roberta_submission.csv\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_8ea9844b-590a-4e2f-b70b-0290ad9b9ae0\", \"roberta_submission.csv\", 4239)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Roberta did not do as well as Deberta by about .04 for macro f1 but preformed significantly better than llama. Although this could be do to random factors like seed. So we will go further to compare the two models."
   ],
   "metadata": {
    "id": "yB_DJzxC0YMt"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "f6ECkibv9Csq"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}