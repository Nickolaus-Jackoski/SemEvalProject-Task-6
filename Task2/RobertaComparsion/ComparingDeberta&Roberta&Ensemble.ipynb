{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Compared DeBERTa and RoBERTa then tried an ensemble method to see if the both combined would be the best but it turns out DeBERTa performs the best"
   ],
   "metadata": {
    "id": "waNFmLt8IwIC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8f8f7a9844054e7bb6fa394defb6ca59",
      "cf615126cbc2475b926463c119c6edb3",
      "5daece1c95e54d7c84b246096b2039cf",
      "3458754f5fbf44419cd63f23ecfd77b5",
      "7f2e27faa840403091a2e2afddb6f549",
      "ef719d28c6e54f5c9d7b4318e3c5dc4a",
      "fe06ee1b63e646c8a791ab56f40d80a2",
      "9e269250dd324ed5870c94ebe2f02e10",
      "1a520767ef65463391ccf02892083cc2",
      "f6e37cd1af2543fbaf0306e315c6e1ea",
      "0bf2cfe2ec814b38b07d1db7ce1f071d",
      "d2a6287fbc1c49e899be27ad034d7be8",
      "281c06e3295148759b65997231b16da5",
      "fb06c3cc443e41ee8c6b5a4003dde906",
      "f32a193d019642e0a7bbed84b62df076",
      "a10a113bf6ed40e7b4721a4492dfe164",
      "374fbf7f27c7421e8fc6e0a820a2fc22",
      "ba7b84a258cf44848068ec59f418ec87",
      "594eeb2aab584cf6b10acda0ab1811ab",
      "f3ed571c46eb4b5d8d9c4d07c6a87d2b",
      "f8cba772d391432e8059874311436f99",
      "4cc3d26767fc47a0b70420400f1452f9",
      "77d72477d56942bbb9addd5e9d319af6",
      "1370668ba42644cf8d43f887f0090fda",
      "538ef220569d49b0a1acb43637cf18b8",
      "bd10937052ae4b00b81df0bff68f1569",
      "bcf7f1af140749ffb57dc737d4884aab",
      "4ba2c78fa05e464297ac0bba723d7248",
      "91d4029cf2694d8ea4b63335cedac0ae",
      "267f670a3adc4f31b6771460569e245d",
      "55201e00fef248df8714b3beb4afe85b",
      "a6249a5a00d84c748e1252e4dc040113",
      "ec1ca82316fe4c9ea61d665746338a40",
      "056043b558374631bc643c4be9f537aa",
      "527dd8829b4a4f0297b8febc95920a1c",
      "927d7a8c927849e88cf79068bca7d777",
      "37706497c68c451fb280a2b2aa66af71",
      "5d360996ebb3457a956d849649a7f083",
      "1eefed78d864495e8182b44569d67e11",
      "8c165c3a99bc4d7e8dec8495c063d62c",
      "f6c9f46b5c7c43dd9b699be1d87c2127",
      "23729949b1464b3c99914f9cf6e2f73e",
      "94d97cbc9c894f8daceae2e20a1eb5a2",
      "d0c961db42eb49a081b8bfe609878b38",
      "2a831a8b95ef44a28844401ee9554b8c",
      "f6643708116d4e2ba4be3663d5dcf173",
      "05e25c0f52d24215b398d505fffae111",
      "d2f55ae052114e5da4f2742fec692c83",
      "44d51726d1684de587931c9246d78fab",
      "ad033ea0c6fa428e864b4b900113673f",
      "dd89d560ee5242f6a7d914ed2f0bc140",
      "2c78ae438f6742f88bfb2457bad8a23d",
      "5377365f6dcc46a888ed8f751c060e4a",
      "20ae43d22108407aa8cfa7b1d0d4c675",
      "faaf5ad262594758ba6fda63ea043690",
      "a8f53bd27e4749e4a3ebf84f5df605f5",
      "ae246e25e5864271adca9d5f1cf04697",
      "aaa78da3a8c74fc4a4bcbb5b75747190",
      "1c79f1cbd0d54089a88813465cf94e46",
      "21fe1b8107964b0eafe96f2071ef1a63",
      "b4e904f483e34e6fb31123e0990ecd52",
      "2386654631144efe866b2f3856468850",
      "619983d25eeb4b2e95b8073e418e7116",
      "bd8a21590b64475f83e25e813054ae44",
      "97b857fc22404a4f8d21f54310ce1ae6",
      "5d97df09d711473196be78166bd9509e",
      "de6e08f9d2eb443ca2a7ee288b5a8933",
      "2deedf97e5514a6382809f2693d3a76f",
      "226f65cbdfd248aa99de8161b5e7702c",
      "7269af793e8b4e9c9d6c5388c239b58c",
      "454d1860364643ee8a2de16775f7bd06",
      "720ccd85b0fc4621aa92188c59295e9c",
      "612e33534dd5411c90fb0a7e31e0a5fa",
      "3dbe60d888464601887b4487b5203e2a",
      "5aa72ccfd37a4586aa48ecc942b53b1a",
      "d77aa8145815439dbf6bfe9ff9190fc9",
      "f3ca49675f2f465aade1251d665f876c",
      "3d110ae2a3a74fe6919d2b36746bbc18",
      "6c4bdff0db114a3fa95657adb0f7ae86",
      "8b945097eb1a449da1ee0c6b01824094",
      "73a89046812a47738ad7b82894cf3cea",
      "d800c6dd103947fc9e88d0735dd568ba",
      "75e7f896340a422d9302fdc579ccaae5",
      "eb75d367c8934a1eb939e2882e210991",
      "2b304b978da54af3bb8b9596f83da496",
      "3d6a2a5e1c7149dbbfa068ee6e3d4594",
      "dbfbeac9ae794fa189b247a2b03fc195",
      "756a10d814e54637ba8efd0d70badb96",
      "44bfd60ec5874aac98ec17157589b0ae",
      "b71cc3861146426793d9e3cf111ed0bd",
      "11806215742040e2aa716fe67e01c6d6",
      "1c2301d165b44f11b7321fecb3ec6e36",
      "1e8d590bb17e4180bd208b3efaaff10b",
      "b74fad0df65a4931bed11b55fe6c94df",
      "d2d0fe5ed3144489af9ef2565742b3d0",
      "7e27b818928444fc8c383cf95c257740",
      "1b549f30c8c24c7aacb831f95af65e34",
      "653a6dad3bd747dbb5d80aa64af41d08",
      "a4d729665769402e8134ab1b4f750402",
      "a937c43745604abfb3026c56f7a41349",
      "b6454720a0dc4edeb9eee4a42fc88140",
      "df785fac44aa498ca03f68e54cd64c39",
      "c5ada5f9a52b498caa5f86c325e06a3f",
      "514a22ef8e464997a843375ee81f55ba",
      "5323ce9c4cec4efb932aa349ca147716",
      "3449791a90e445348e2706a765dcf689",
      "5996f0e322954321a395edb8a828154c",
      "73c6b486e14c46b19a1311e3f52e7a55",
      "900ba2b7fb5e420a89aa2d64e946889f",
      "0fb782819d9e409998250131f580921c",
      "71c17d9f5fa94f198afe397aff18bccf",
      "d5e99bd6382747d1bb36542ecbb2ed93",
      "9ac41d858510473488a2468b625df6ba",
      "07960b7d0c994fa0a3a0f680655ba750",
      "a019526d1e644595b6b125c421a4fb1a",
      "f711618d514343e2b4ea5a2648482482",
      "3d4f9d6261ce442ba84470a4d5b73cf6",
      "f0d41598d2b44a5db3bb106a3eab31c4",
      "6e225a1e13c04819a2d8eb58687efc50",
      "c80458130566481eb3e8137504f568bc",
      "79ba4ca3c60949fdb33a5c5888e7d6e9",
      "0c64100513a445db8d1d7af23c500392",
      "ff0fc4ea492440b3bc22b53d7cad3d21",
      "67ef36fbd5eb447da5dabee86f0a54e4",
      "ab722f44df464e85be6405d933b11954",
      "57088536431143fb9b8fa8398584ac7d",
      "94b0e0eb812b406aa7837f344a807987",
      "fa807366149d4ee0a042fe316730e595",
      "c5339c2673504bdba7c39960e043940d",
      "74b307cb7cff403ba777afe98ce8e5d8",
      "4367d7379a0b4ecca840a9a1d4b7475e",
      "e14057a2ed854842b0f2e0828f32b26a",
      "dbb7512c3766418d84177fb3e37a292c",
      "e065682cd229470e856c07473a0fdd9d",
      "b21472c9c0a54804a103c69039a44fd5",
      "562cf9011fed4575ae4f6eaaf391b72a",
      "2543ed2ef66d4007966e41bbc8bdf4fe",
      "89a4647fc6af4c4080ef6e7cf9bb913d",
      "15f9127550f14c6f990d3e93c2f1a155",
      "c1e62efedd0546399833e545145d3504",
      "127a6bc4097d4530bd4535bb497ccb75",
      "d1a4daa5909441a287ce852329da7b23",
      "8ac29ed2fef04de8b627f06cdedb2372",
      "be9aaa6bfd1146c5b2749e903886a205",
      "987c615abe8344ec82f302541be5330c",
      "077300d3901f449ca48424473eaa895a",
      "85d663ea7a2547628c1a38b77a4e770b",
      "4520549005224ab8a3393942d5f516ca",
      "857f789211de4294af81325174829138",
      "e1d0187f911140b6b1a0df497c52dbbf",
      "6e3e52722b724e1f8404ff8cdbb6ac6d",
      "a44bd3f57f4b4e90a3994ba4643cbf66",
      "3aa66d9eebe74bd0a24aabb27ecfae33",
      "eb3ca5b6a53c44c8ab2c4d509f4cec43",
      "acc2b63dc265484e85dc8ff3a9a94ed9",
      "2a5d1579b8a44465908dc93b87ff5cec",
      "40fab2b76fec431eab74fbed5d92e271",
      "13ccd3cab63642b0b6c9b3f1b5ad04c8",
      "0acd483597be4201bbe407bb31c73972",
      "1a8e89708b7646f68472c376ac19f9b4",
      "ac6554eed8ac43ae861991b353fce5b1",
      "ed7c28f55d914229a9317feddb851d67",
      "aed56db3d4a44fe3aef2ef85b00cbdcf",
      "a0829a92dc724acda0892cbc43096e9b",
      "84a56528d3454759a364900d24f089cb",
      "d194333a05b74accbcc42d966f97fa7a",
      "7e50658fd7b74f6b8d79bc480da10415",
      "67588d7c6f29424cb0d1a9669efce117",
      "79e5255a7d354592a86ef6633d8b10ca",
      "e5c8e52f9a104c5297c6844e53f148ce",
      "733e1cf653e04fbb84c297823dfac2ee",
      "92db4004873d4d0485bd76515d98ff14",
      "cc46b29ba8f5436d955f4a5b23c61acc",
      "23e2a60c816a4985a0e614efcdf6dc2e",
      "480c5ab588f94d879d3c0abee4ceff35",
      "549c66c37dc64eed80ecca3b6fb81849",
      "90f52f90c2434895a2297d3f19cb451b",
      "878c6ae59fad42d0a23b2753f72cbe33",
      "cf26aac6f6d04d97b4b0badbcbea8045",
      "ffdcd8b8bf6a4831b346b97a814b41a7",
      "71a28b79af3743d5acdc1f9dc4f53123",
      "0a2f6166014d47ed8f8000ffd986833b",
      "6855f5aa48654f559ec216dbc0a43099",
      "7c1922e4f86a4cd3b524f013fbef1c03",
      "e53aed26a1484a3f86948309d3655f54",
      "15f6dfa7010d4146bcf82ca9d337e3f2",
      "46f475a6e612479e910b244e14c67985",
      "b1718a07d3e44adfb933fd2e42f4f36b",
      "2bbccff8cbb3486cb73c8bb0451a9c37",
      "22a13d2050b842aa8e6c755517d09ac6",
      "818874290f69454d83da23495a435085",
      "df3b63b326e44b3d934089bbffce9b4d",
      "78febc6108ea4351bc255f5fa5d936fb",
      "4e01e33198d442eaa084d5693b7aee20",
      "1f3f6ecdc2d144029a325e6929969943",
      "432d9f9fb81749dc99b6e8da3c251efc",
      "d4ef29bf9ce248e7ac9a7c4d4e3e5e39",
      "9f3db63f05344db2bccb40c73db2846a",
      "36a65d7aadde45469c9ee4ab740d3d4e",
      "6b1bb82e41414fd58daaa7c53bd665bf",
      "4cbade416e984ea68f112b597a0dd969",
      "44b4c20f673747259791f2e397c6dca7",
      "0eadabc7e88c4a91bbc7aafa78615246",
      "70a575dab12c497787992f87467e6474",
      "20bf862d2e804602a5c23c8c9a1b03e7",
      "51ae05b43372459ca31b76101c1c4cd5",
      "bebcdfa34b034fb6838e16a469771521",
      "b6663b404020406099c3770b402ed4e9",
      "cfd578bf0cb8419c9e067b79a125c986",
      "9f4190b4bc414cb3b35bbde721c60ceb",
      "8e5c06d5ffa94df19682ffefd40eaf38",
      "ea8c9847791d4bdabb020209445505b1",
      "9be6a3d0201d4818b4decc3d7cc899e7",
      "0b9b368aa5af47b894bbf13b3607b71d",
      "0fe7067afe904b6197e881bb30a712b0",
      "1f6da86ba0e149d9b1845ede554e5a31",
      "599adc2b0425453abb332214954dd90d",
      "4acceef265e14835b95ad880f3936dcc",
      "0cba09c1a2ee4166b8fcc4bad8fdeb90",
      "59833bda4bcf4155825085065b3b2323",
      "6b748b27a4f040ac89ed117ed74feae9",
      "71289b3ae00e4c80b5fd06795b5378c8",
      "b4d2f60d38c34613b018cc7846ac2b5a",
      "51d22f653d4f4939b0b8dfa84d0d9290",
      "da18ab27d7a64334a3ce64edaab3ef62",
      "748d6adf767741539383f2c9722b6111",
      "35da3b85ed0c4d96b590c3bbbd379c76",
      "6242a509dd334d3ab475b969d8c63728",
      "3398cc79ebac467c83b0c6e0f2c724ec",
      "4dd684e714bb436393b036abb9b14b59",
      "72fd29533f3e457d946f7cea820c857c",
      "e6ed8ec1404043c1b98a185a911ec0f2",
      "689121e5cfdd42a8a671c0f6279f7b2e",
      "ce91cf83421a4ddf93d9ed1866ce6218",
      "f14c80a3b8fe4fa486e337d281d6d17f",
      "9ce813ea0c994edd9763a95684bf36f5",
      "cfcf4f107cde4bda8f35eb564cbc06fc",
      "0d4946c5b22a4819a90e14fc49db940b",
      "f63452f5bb20452f818cc3c2bc38d875",
      "b1acbaaa995342988b456066eb880780",
      "05412f13b79c4f43922d5c370e8e515d",
      "6fcc050783c74e4f81a00b78433ec29a",
      "1789c6670ee2450cba87420a118d5171",
      "7c3ff6147b294b6894bfb270347533d5",
      "e52602b180e34e03a82f75f02dd2999c",
      "d7232476195f4320825548b49d320a9a",
      "bdeac20382f541ab978917a682dbf16d",
      "751bea132a4b4a9d927737fcd39482a3",
      "05176776f50a47b58016300ce40cd6ed",
      "dfa7284291dd4358a5066d312591188d",
      "2ec6fb0d3435444c8fd7aad640d0aefd",
      "3d14f76b959343d8b07e6db3b53e5391",
      "887fbd4a40c24a80b5f07651132c776c",
      "ff2871442889404f8d03644765868362",
      "b2fbd4d3467349b0a314ee19d1274533",
      "1d96434d3a4447a89f41d5b7459750cf",
      "2c83654e91bb4234829fac89098a8892",
      "d3e7eec02b184686b660c67ef12da0ea",
      "57641b68b01f4159a854da2817be82d4",
      "1764342cc4c44dd5a6fdd996b1685afd",
      "82c48681f2dd4e3684c1f3ff767b6980",
      "b99b5f25a53141f7ae58dd9c5940bdb8",
      "de48bd5ec5cc48139a378b93d840806b",
      "5e6601235f114c189ba623e0355959d1",
      "b7b48fd8d8154ad39089badc80916091",
      "429cf3c900ea4ad5ae48bdebd86c0ede",
      "260559584d71409b837f6b71fbceabec",
      "053355eaf355465b9be24679a7bf20fb",
      "6bfe8df9a7bd49f39e54f52337924ab7",
      "579081cf41454a01b02d4593f7367988",
      "a7fc4667fc3245de9bb39f3102f0aa0b",
      "74ecd3b171874be7a86ade1301d917f1",
      "8c62d3bcaa2f4d98b4902971392329ff",
      "7a5acaeb8d6b48ad9739c76870b2b092",
      "534c5fefa7ff4e73aec8840caf2cfd04",
      "c53a5b0513144f57be8363f4fb0120f9",
      "14cd89a59f0e4871b7a6dbaab6750b22",
      "6d9a0356e63447479ec2ee32bf859e8f",
      "b3a3a9cacdeb49b18d55e83365bc9a02",
      "be5f93a55eea4c85855f14d52aa2a468",
      "372eb437441944b8a3ac21cf82506944",
      "df198049be8d4d83af271de0954f4f35",
      "f38b294b43324635ae1a7146d487c60b",
      "65bf6c5891f74e20b31b7fb9c2fe9072",
      "066ba3afea2f461580d34b74dabce477",
      "fcccc17115f743118aa7bcfa7abc3e0d",
      "850c7f6705934f35b832857ca45a32f3",
      "d9a5e97b7d24414f9cd709e8279b6160",
      "afcef8009cf548b38dd0290929b51ccd",
      "3c98bb10b55f447bab92417035112c3e",
      "120aae2f1f3f459a9d1e4988cdf96ac5",
      "69aa96fd0c374971805523515a6d19c1",
      "ea5d33ff6cf34d78b20208460f7aaae4",
      "598c95dac8ad4e98a5284712ec2ef528",
      "cf16d5f85ece40fb8ab7e17eba0e2417",
      "1526e212e4604e8dab6ecc154756a528",
      "20caaf7d48574c8da710822736e93299",
      "d9b0011019864e67bafd7468a899185f",
      "1165396c4ebc486db4a94bda5caab682",
      "11e70a09474241c3a4c93a4f48570c31",
      "21873a3e173341f18263344a5f4877c7",
      "4372c6661305405abf4b3669b884b2a9",
      "8389c537e82c472d9ae160dc26b7c5d5",
      "0562cd44a73c4075bfe25a6805920c0a",
      "2c302d8b06804b2d8b03730f051d28b1",
      "5a6a62b7858943e19f9a16950f064ce0",
      "e0e8df0d3f1f4c208e2f8f477537571e",
      "a800a97a5925417785aff35d5a29c736",
      "594ecd495d494bf7ac9b005ccb2cd243",
      "f48b7f504e47496dadd1ee6e23356842",
      "1f876f89581c40568327ce2ba5d15bc4",
      "df23e9eac81e4d679f7bc84260b3fe25",
      "aa4570f69bd44c65b872ee6a61a9315f",
      "66164eacf0d843d5ac736b1da3a9e11c",
      "3278fdbe41194316bc8f70625d195f4e",
      "f7f402da94d54471b10515803b4e5c53",
      "bed71dfd66054ebfa58bc1f32acd269f",
      "dc62e51eec944f528f1d3704d92c5ae4",
      "a194d0cd9c3e483ab1d178f22c9072e0",
      "80d96435613a42a7b13640be61d12c11",
      "3c0ab1379d704b14a81733b5cb4d57a1",
      "e8980ef7290c4d288f946f520dc6c284",
      "40e91c7751fd4ae2926c55fbe87fa4bc",
      "f4df967694024d4bae83cef03a1bc2d8",
      "b3580afa94514652863950c889b5e59b",
      "1858bbac92f9457a8009fcaa792fc2cf",
      "88dbabb5d5484c2d9fa0198bf39a052d",
      "85f428d169d9438ea4164f1f742739f7",
      "52fd6d9f96c04046b064faeed8129b59",
      "93b9026702da43208a23eb34dfdc5a05"
     ]
    },
    "id": "r6t3PsHd8q3o",
    "outputId": "d2fd6e40-de15-42a7-b334-a02b73e74aa4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading QEvasion dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f8f7a9844054e7bb6fa394defb6ca59"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2a6287fbc1c49e899be27ad034d7be8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77d72477d56942bbb9addd5e9d319af6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "056043b558374631bc643c4be9f537aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a831a8b95ef44a28844401ee9554b8c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Formatting training data...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8f53bd27e4749e4a3ebf84f5df605f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de6e08f9d2eb443ca2a7ee288b5a8933"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Training model: roberta_large\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d110ae2a3a74fe6919d2b36746bbc18"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44bfd60ec5874aac98ec17157589b0ae"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a937c43745604abfb3026c56f7a41349"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71c17d9f5fa94f198afe397aff18bccf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c64100513a445db8d1d7af23c500392"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dbb7512c3766418d84177fb3e37a292c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be9aaa6bfd1146c5b2749e903886a205"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acc2b63dc265484e85dc8ff3a9a94ed9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d194333a05b74accbcc42d966f97fa7a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90f52f90c2434895a2297d3f19cb451b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1718a07d3e44adfb933fd2e42f4f36b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "36a65d7aadde45469c9ee4ab740d3d4e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2275' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2275/2625 14:32 < 02:14, 2.61 it/s, Epoch 13/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.973068</td>\n",
       "      <td>0.495177</td>\n",
       "      <td>0.296483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.924180</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.279280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.120700</td>\n",
       "      <td>1.773336</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.470437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.120700</td>\n",
       "      <td>1.764205</td>\n",
       "      <td>0.562701</td>\n",
       "      <td>0.517985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.120700</td>\n",
       "      <td>1.717235</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.573023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.674200</td>\n",
       "      <td>1.728956</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.616583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.674200</td>\n",
       "      <td>1.768509</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.621502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.674200</td>\n",
       "      <td>1.846902</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.603409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.911313</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.606773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.998660</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.618190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>2.031479</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.601284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.131400</td>\n",
       "      <td>2.029686</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.616237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.131400</td>\n",
       "      <td>2.044524</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.612417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "roberta_large holdout macro F1: 0.5698\n",
      "\n",
      "Training model: deberta_v3_large\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f4190b4bc414cb3b35bbde721c60ceb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b748b27a4f040ac89ed117ed74feae9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6ed8ec1404043c1b98a185a911ec0f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1789c6670ee2450cba87420a118d5171"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff2871442889404f8d03644765868362"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7b48fd8d8154ad39089badc80916091"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c53a5b0513144f57be8363f4fb0120f9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "850c7f6705934f35b832857ca45a32f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9b0011019864e67bafd7468a899185f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "594ecd495d494bf7ac9b005ccb2cd243"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "80d96435613a42a7b13640be61d12c11"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 32:29, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.208538</td>\n",
       "      <td>0.340836</td>\n",
       "      <td>0.152734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.882195</td>\n",
       "      <td>0.517685</td>\n",
       "      <td>0.313330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>1.807287</td>\n",
       "      <td>0.575563</td>\n",
       "      <td>0.445569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>1.736839</td>\n",
       "      <td>0.578778</td>\n",
       "      <td>0.534436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.135600</td>\n",
       "      <td>1.711958</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.545147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.749200</td>\n",
       "      <td>1.777269</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.534727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.749200</td>\n",
       "      <td>1.817059</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.523237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.749200</td>\n",
       "      <td>1.881907</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.577411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.509200</td>\n",
       "      <td>1.950593</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.583942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.509200</td>\n",
       "      <td>2.142204</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.568020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.509200</td>\n",
       "      <td>2.121046</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.572469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.243600</td>\n",
       "      <td>2.146230</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.575513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.243600</td>\n",
       "      <td>2.212392</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.556612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.243600</td>\n",
       "      <td>2.188334</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.565717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.135700</td>\n",
       "      <td>2.187016</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.565586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "deberta_v3_large holdout macro F1: 0.6055\n",
      "\n",
      "Summary on the same holdout set\n",
      "RoBERTa large macro F1:   0.5698\n",
      "DeBERTa v3 large macro F1:0.6055\n",
      "Ensemble macro F1:        0.5827\n",
      "Ensemble accuracy:        0.6348\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Seed and reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Shared config\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 1e-5\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"Loading QEvasion dataset...\")\n",
    "raw_ds = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def build_text(example):\n",
    "    clarity = example.get(\"clarity_label\") or \"Unknown\"\n",
    "    text = (\n",
    "        f\"Context: {clarity} | \"\n",
    "        f\"Question: {example['question']} \"\n",
    "        f\"Answer: {example['interview_answer']}\"\n",
    "    )\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"evasion_label\": example[\"evasion_label\"],\n",
    "    }\n",
    "\n",
    "print(\"Formatting training data...\")\n",
    "proc_train = raw_ds[\"train\"].map(build_text)\n",
    "proc_train = proc_train.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# One stratified split used for both models\n",
    "outer_split = proc_train.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev = outer_split[\"train\"]\n",
    "holdout = outer_split[\"test\"]\n",
    "\n",
    "inner_split = train_dev.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "base_train = inner_split[\"train\"]\n",
    "base_val = inner_split[\"test\"]\n",
    "\n",
    "label_names = base_train.features[\"evasion_label\"].names\n",
    "label2id = {label: i for i, label in enumerate(label_names)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "\n",
    "def make_weighted_trainer(model_name, train_base, val_base, holdout_base, output_dir):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "        )\n",
    "\n",
    "    train_ds = train_base.map(tokenize, batched=True)\n",
    "    val_ds = val_base.map(tokenize, batched=True)\n",
    "    holdout_ds = holdout_base.map(tokenize, batched=True)\n",
    "\n",
    "    train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    val_ds = val_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    holdout_ds = holdout_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "    y = train_ds[\"evasion_label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y),\n",
    "        y=y,\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        class_weights,\n",
    "        dtype=torch.float32,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def __init__(self, *args, class_weights=None, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.class_weights = class_weights\n",
    "\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\", None)\n",
    "            if logits is None:\n",
    "                logits = outputs[0]\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss(\n",
    "                weight=self.class_weights.to(logits.device),\n",
    "                label_smoothing=0.1,\n",
    "            )\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, model.config.num_labels),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        acc = (preds == labels).mean()\n",
    "        macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        return {\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": macro_f1,\n",
    "        }\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(label_names),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.05,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=2,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],\n",
    "        class_weights=class_weights_tensor,\n",
    "    )\n",
    "\n",
    "    return trainer, holdout_ds\n",
    "\n",
    "\n",
    "def run_model(model_name, short_name):\n",
    "    output_dir = f\"./results_{short_name}\"\n",
    "    print(f\"\\nTraining model: {short_name}\")\n",
    "    trainer, holdout_ds = make_weighted_trainer(\n",
    "        model_name=model_name,\n",
    "        train_base=base_train,\n",
    "        val_base=base_val,\n",
    "        holdout_base=holdout,\n",
    "        output_dir=output_dir,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    eval_metrics = trainer.evaluate(holdout_ds)\n",
    "    print(\n",
    "        f\"{short_name} holdout macro F1: \"\n",
    "        f\"{eval_metrics['eval_macro_f1']:.4f}\"\n",
    "    )\n",
    "\n",
    "    preds = trainer.predict(holdout_ds)\n",
    "    logits = preds.predictions\n",
    "    labels = preds.label_ids\n",
    "\n",
    "    return eval_metrics, logits, labels\n",
    "\n",
    "\n",
    "roberta_metrics, roberta_logits, labels = run_model(\n",
    "    \"roberta-large\",\n",
    "    \"roberta_large\",\n",
    ")\n",
    "\n",
    "deberta_metrics, deberta_logits, _ = run_model(\n",
    "    \"microsoft/deberta-v3-large\",\n",
    "    \"deberta_v3_large\",\n",
    ")\n",
    "\n",
    "# Ensemble: average probabilities from both models\n",
    "def softmax_np(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "roberta_probs = softmax_np(roberta_logits)\n",
    "deberta_probs = softmax_np(deberta_logits)\n",
    "\n",
    "ensemble_probs = (roberta_probs + deberta_probs) / 2.0\n",
    "ensemble_preds = np.argmax(ensemble_probs, axis=-1)\n",
    "\n",
    "ensemble_macro_f1 = f1_score(labels, ensemble_preds, average=\"macro\")\n",
    "ensemble_acc = (ensemble_preds == labels).mean()\n",
    "\n",
    "print(\"\\nSummary on the same holdout set\")\n",
    "print(f\"RoBERTa large macro F1:   {roberta_metrics['eval_macro_f1']:.4f}\")\n",
    "print(f\"DeBERTa v3 large macro F1:{deberta_metrics['eval_macro_f1']:.4f}\")\n",
    "print(f\"Ensemble macro F1:        {ensemble_macro_f1:.4f}\")\n",
    "print(f\"Ensemble accuracy:        {ensemble_acc:.4f}\")"
   ]
  }
 ]
}