{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZyTAW4KdM_M",
    "outputId": "9393a6a1-0603-437b-bff1-7d68dc94eecb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "object address  : 0x7f0b3d827640\n",
      "object refcount : 3\n",
      "object type     : 0xa2a4e0\n",
      "object type name: KeyboardInterrupt\n",
      "object repr     : KeyboardInterrupt()\n",
      "lost sys.stderr\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets scikit-learn accelerate torch pandas setfit sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "x4liXDijdJMR",
    "outputId": "a15359ac-9243-4102-facc-7ee96b61042c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: transformers 4.57.3\n",
      "Uninstalling transformers-4.57.3:\n",
      "  Successfully uninstalled transformers-4.57.3\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging>=20.0 (from transformers)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m261.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m301.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.6.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m304.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m400.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m382.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m279.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m420.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m387.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m417.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m390.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m327.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m288.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.4/159.4 kB\u001b[0m \u001b[31m355.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m367.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m363.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m343.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m274.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m244.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.6.1-py3-none-any.whl (131 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m363.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.5.0\n",
      "    Uninstalling urllib3-2.5.0:\n",
      "      Successfully uninstalled urllib3-2.5.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.1\n",
      "    Uninstalling tqdm-4.67.1:\n",
      "      Successfully uninstalled tqdm-4.67.1\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2025.11.3\n",
      "    Uninstalling regex-2025.11.3:\n",
      "      Successfully uninstalled regex-2025.11.3\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.3\n",
      "    Uninstalling PyYAML-6.0.3:\n",
      "      Successfully uninstalled PyYAML-6.0.3\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.20.0\n",
      "    Uninstalling filelock-3.20.0:\n",
      "      Successfully uninstalled filelock-3.20.0\n",
      "  Attempting uninstall: charset_normalizer\n",
      "    Found existing installation: charset-normalizer 3.4.4\n",
      "    Uninstalling charset-normalizer-3.4.4:\n",
      "      Successfully uninstalled charset-normalizer-3.4.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2025.11.12\n",
      "    Uninstalling certifi-2025.11.12:\n",
      "      Successfully uninstalled certifi-2025.11.12\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.4\n",
      "    Uninstalling requests-2.32.4:\n",
      "      Successfully uninstalled requests-2.32.4\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.36.0\n",
      "    Uninstalling huggingface-hub-0.36.0:\n",
      "      Successfully uninstalled huggingface-hub-0.36.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.12.0 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
      "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.12.0 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.11.12 charset_normalizer-3.4.4 filelock-3.20.0 fsspec-2025.12.0 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 numpy-2.3.5 packaging-25.0 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 typing-extensions-4.15.0 urllib3-2.6.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "35997fd479f1495f90ace058fbfcb523",
       "pip_warning": {
        "packages": [
         "certifi",
         "filelock",
         "huggingface_hub",
         "idna",
         "numpy",
         "packaging",
         "regex",
         "requests",
         "safetensors",
         "tokenizers",
         "tqdm",
         "transformers",
         "urllib3"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.3.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.2 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (2.9.0+cu126)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.12/dist-packages (from transformers[torch]) (1.12.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2->transformers[torch]) (3.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers[torch]) (2025.11.12)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.2->transformers[torch]) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir transformers\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f0ecc86ba384aeaa79ecf43f6c75e5c",
      "96156409f2e44681a76423a669fb0d45",
      "3df4bb30395a49a1a439def6ebaacf02",
      "c86a229acf124fb3bb1292d7d40fee4f",
      "b59d83e69010477782d21f825716eec9",
      "0e5b877c897d4b9e8c45104e52ae9c70",
      "4b94a8f5726343e1b9396f6b0474300f",
      "463f786ed82a46a6a08231111e8c3cb2",
      "7972846833ff4d66ba7bf80801043fb1",
      "bdaea79a733f4544bd0988481c34135d",
      "6cb13ce898474989bf98b79b2fda144c",
      "201fc9f27f27439fbe6da7cd118bfbb5",
      "7c15e59eb73940eaa6e031cfe73577c4",
      "77f0327f8002427ab2417088ac6fef9a",
      "8463fabdbecd49e99205c910ddfd406d",
      "1a0d7f6fe40c4be9972cdaf3c7ee521a",
      "7f06e7ac2ca14c5a8e0c46bb2f3a413c",
      "09260091da814e75acabae226f7f30df",
      "ef6fe43b86a248e88f672ffcabac4d90",
      "2ab1fb39b0274fd09826ab8f40c91bb7",
      "d69cff145cf14cb1abab129d6de2cfe5",
      "b651c451c6854e678674accd8fd646ed",
      "1f4fe8e68a584726890a884782be9a47",
      "8c9f63fcc42840cc8f0b7d4696449c18",
      "84efc98f05824c39af5c24c5e638421c",
      "18373b54f03d4fbab7dea0b027bd3a48",
      "637498c2984c480eb0f3f3e463729bc2",
      "21326643367a427c8d4b92ce5d848ced",
      "636e8c0e0079415d8f5174339092e3ee",
      "15073a147b9241ecaf4fcda4ed94b422",
      "2a8cc570699248d884ebf96ae77e7d64",
      "c7338fd6c4a94adc94b44131a65370b1",
      "822cdc4c866e488f9491b4cd3366e401",
      "f600b87486f948bcb455d3d6e418ac61",
      "15d84aa5d29944c4959c032626cb3870",
      "819b46b59b3f49d08cba0bbbde7f7b36",
      "db7034c26be34695b6dff740c068fc01",
      "169ac3cce423496daf89d01c174a18bf",
      "d0149463fa774c2ca94bdaed1c15531f",
      "f2549637889c408d8c7b329e54fcdc7c",
      "cb1f0077a3b74bf3867d7a5d0f9dbd4d",
      "253bd1fd7edd49f1a55bd635843667c2",
      "92064c2cfd11446691eac8b580313588",
      "25a4711946534b7c9469c1b4008649ca",
      "0f03154fc9164ff7ab712bb656e4ac99",
      "af5daf4987a0437e96f4c0234b31a823",
      "a965f74f9e6c402396bbb489a07eb761",
      "c015795ab40c495e82633c8f4654731b",
      "2b8057a7c5b149c195f9d84c3cea6a7d",
      "eeea52ccb8c841e0ac2f66fd7f21c31c",
      "2719f2f4aed94775baf7540186e2ef23",
      "b65fe9404dd648cba32a8e03210d13cd",
      "06447518580a47a49deddd3bd94f4221",
      "bc395812061e4082b3080c3c657fa792",
      "de5cee4c56f34d2b8f0926729d9bbee2",
      "cc442ec99d3a43f8ab7bba8ea7614c06",
      "4cf35f219b5248e6bc55f6d3a7ae0123",
      "9231651d6d094193b80e0184b2b97ffc",
      "7443f0ee94a94e97ba95994f754de1f7",
      "f9c828cd496c4ca4b68e1f137168e5b7",
      "a1143d86d6d74b0f9e576d3e6bf40814",
      "595f92b40b2349799a282862bdc373b4",
      "4f1279aa41964b7b9363bef3b07b1029",
      "461ef905ba3e4b85be31cc743322f5f2",
      "16b0015d6c224809adc672f356819ceb",
      "d142a5469b5d49578e7e7c398c60df56",
      "8df397adb76543e9b12bf9fe0eaff1b3",
      "3801d79a479f497895649dacbbe70d08",
      "7914fa5169f646a1a00ad9fb3a42868b",
      "6d23c562b6d244f999199e84e3a0b6a3",
      "792210fc631a4f359c961826b1cd8f6c",
      "04c101f48a2a4b06be27e5ba27d76283",
      "12c6bd9622104fbfbd9b0ff3483d6e4a",
      "ffae676baad540999ef39e709ad94694",
      "bdf9bdade26442faaa062445a061bb6d",
      "9b56fe43bfe541ef801a200ca978040a",
      "0e2109eaeae04e178d3ef6f295a78d9a",
      "b36ffd63a17e4e098d36fb392aa2a5ce",
      "78b6dea0ab0046caa6848b3e3b95897c",
      "95746056c0b743edbd833110be6690e3",
      "a35465c6137b42d4a51821f7e72bef44",
      "38720f7413664b84932eb54433817f28",
      "885caf6479594660b7439c8717405f92",
      "5faaba361c174507a5ff522352a4ee6e",
      "ec82fba7efe6461db1ffed501a8431e1",
      "d6964be06a1647babec3590816f578b2",
      "c45b2b68d89742ec9aca17a4b93f2d1a",
      "e139e8eaa40844d2906900c9c6ddcf25",
      "50b3253e984548ce916f0aa15768065f",
      "461ecea4ea884f0f9bcededf75894fef",
      "7f9682385e014e3cad010b9876b63588",
      "555d7658d8534bfaa316b13aa3aef442",
      "37a01dce0d19414e99f19472ff535f65",
      "f092bb61dc714e08a36ba1f253df4655",
      "9078afb7ee8744b5a186155017b5be46",
      "1410ba36d3134d0ebd7386644a9887cd",
      "811c072c0be84e669f21fa0fd60f70d2",
      "0b1614a4a15947e093d7a38985a472be",
      "9664499c1c7c41ba9e99c780fec1da9d",
      "323d9c206413445db43203975c57564f",
      "44bc53fdc1cb48f4b856f73d9cc92e1e",
      "65887256102649f7b7ca7a4658ea82ff",
      "344eb3756d13425ea5490851a8fdba47",
      "61f788dd15614e41a495d6ab3babff79",
      "033aefbec12f4698b3ef5a4e375a1986",
      "52b677f55a62467cac96696b11a25cb0",
      "b0c92b01ac4e4e7da825aa44fbdd8769",
      "3b816ac258be49108178e44bfd32c6b6",
      "0bc320d0c88f4cfa9e5731ea2df34ad7",
      "ca48dd40fd224ceca9272fd5183b6d34",
      "8210b038a8234d89a9ba7ed429c02c90",
      "8cc0670cf8c04798ad5dbfb522f3037d",
      "2c8502b7e7834f46934f243e2e76a31c",
      "5a783e2b88b846c9af0d779e474a5c0f",
      "b09d18512eb44ea8942c5fafa0584ef6",
      "65b06128e1094026b882d6aa0d7ea9f9",
      "e387738210cf46428d4decfc4e95be7d",
      "d0767776554b41e9a8376877b6fba4d7",
      "1f1f1642ce18478088256643eda4fef4",
      "7912aa9205ab44868446f3e0994be805",
      "a65131d706dd4771aa10b24ee6b709fc",
      "b2ac5ecc80ae445eb14744ad71518b0f",
      "8b2629ecd97c4c81b034a30cccd8f5ed",
      "606619dc42f045bb92a69fb43a3f3655",
      "4a0554faf57c408cac45bb5935d8319f",
      "6436bf807c8d4fdbb98204e17fe8588e",
      "5dcea70e94c3434ebf008c90987e0391",
      "f04aa9369e7f4e00b52c8e0d6d4ae9bd",
      "d051c75d176749fd810f6a097042a73e",
      "2118ea9e9afb4316a68ca70c65584e55",
      "40cfaa75ee16499993f7dcec4c507628",
      "d234c09a8241458daf6ebd44a41625d1",
      "60d5d4de9b604aaea21435e9071e6ddf",
      "799fca7b86ca47928c63651d7ef8105a",
      "2999a2d93c9a4f7ab71d362551775a13",
      "32c779da600d426fa3ed44379463e763",
      "a6e6c2d1396e4ed9a36a92a8d4896cdf",
      "f8710df85b864f8fba5b5b4c9fb8ecf2",
      "0bb611b72d21473c844a95ec8d146b2a",
      "5aaa656a72d04edbbf320e086c55a6bb",
      "f4ff7ae65fd548d99ab1c88d8e3561fb",
      "52475f7eed9e402095112c6d682e357c",
      "a2ea2417c1224c2db08539895eab3298",
      "7576f431f9b84ac0b2b17df39a1c49c1",
      "5460a24b34884ae5b6d8a0e5bba5187e",
      "836149eb2fee47c6a80d14562257c1cb",
      "cd2d9d70fcf949b1bb345f804ae0f82f",
      "2cbd7b500f2b47d4a50100cce953e1d9",
      "266f718654c44818808fc3405c2fb82d",
      "aa9a77835d574490bde901d31be19d9d",
      "ff078156577e415bb5ce666d64cd83dc",
      "7ef542e9913a47b699930c8ecd510266",
      "887b86e7fa1d4a50a19167f896c7f6ca",
      "45cd820284f347ab832c8b562a83fe91",
      "602ffd9194744f2e94b8738d67876eed",
      "f252da30738849af8ffdb8649676791f",
      "7f9698e336cf4f1d87ec149c30097e34",
      "357a539784614ef4928b699a83952cc4",
      "73975765b15f476892bcc0f7c25fe5f8",
      "0517bcf2c3a64877a254ddd1255f437a",
      "39381bf38c794de986899e80cbe4729c",
      "9a94e4a4953648c78a90cf98ba00e574",
      "a953db7a183846b8b05f6ae779b4fab9",
      "1dbeec9b0c854a449c80c8d6deb1d105",
      "6eeecc9d8a5a4fc2ac28b6f09fe9d468",
      "3d2ef1a85e2445b6aa1529064eea5afa",
      "fe94767d98c646b589846b70119921e9",
      "e1efd5a8cbc54f16a11de61ab6da2355",
      "a95ebaeb12ab44e38da102f13c5d086d",
      "7c320ec376e6465db295399dca4630e3",
      "eae183973fc84d2282fdb70afef5b66b",
      "fe2dc1cb668948cc8d4cb70378fbd919",
      "068a26bbfe8f417e9f386829cad7ce29",
      "ea83c37ad0cc4943a8071bf36febfbf1",
      "8b9fb6c0aa164c8c8f6a8bc79f5dcdd7",
      "52463ac609d1454fac0b97c4ad7546b2",
      "bc0557284eb54d3695bc311b2825741e",
      "81d28ffd890147dd853fa27291592edd",
      "abfc6940fdc748d7a7196fab5316cff0",
      "531c6593cfae4777afd9fec9270c1f2c",
      "6e1831333002472a862be1e548465679",
      "9735d429a9364997a4fc75d950001a36",
      "3b20e1bbc7d34ecaad7d420da1c77aad",
      "0ac9473d4fa44b80b842415ff06b8a71",
      "905e63b9813f43bdb3f17ab38699bb5a",
      "10dad02952cd492386c0fab298d88e1e",
      "7d4361290c4b414fa71f5d00b1aec202",
      "a8520e8a70624e0098e9c461045a80a3",
      "9f61e249f3a047cb97ab7b034bdd45f9",
      "e1b5577788aa46d1950c2315940d726d",
      "00978865c4404153ac52e9292ce1d511",
      "541c92a3fc4c40f9a28c81d0aacfc31a",
      "752dadbbb3ba4ddea1ee611fad76331c",
      "c4d690e88adc47bc8df88fa8f1571e1c",
      "a14aa523d0d14ed3bf28085fabe14725",
      "090fbf83150a40ceb601e05a7a401766",
      "07f170165d3648b2a84ae40ac50c93ca",
      "307bd37f82224f1bb2760a4117be30e2",
      "1af87e5f5ecf4f85a523cba3420796fb",
      "f9aec8d5aef9452fa1ea14cc017a1c45",
      "9d8dbeb739d042dda9c5ece8619f35aa",
      "7c124b68bbad437b903ffdca21422a78",
      "975ccd40d7294b8aa8506e463e2ed018",
      "4031c921f4754026a85ef00cb2e5136f",
      "172d58dac210499d995cadc97cfa4c43",
      "53f0dd56c21a4004a487c109cc2243e9",
      "c23bef4d74c34a7f8eb67401b14b85eb",
      "f3d5da4c24cf48cd9d6107580055ff5b",
      "e4934756ead94d5eb5d7ba7aa2c11f19",
      "eba4691b108245599e984cd3b520bee4",
      "ba0b259bdbba4e68b4b082f1f5172937",
      "a4691599adff4e3a9051cdfef647db88",
      "2818bb163fb6445392907e88e981c26a",
      "cb2f843a45d84d91b24bef9e4ce71b54",
      "bcbc5b165c6745b8954058c6e8c26fe2",
      "54080ec6add2403fb791e3f78d2227bf",
      "468096ada6eb41ffa75235f083e75713",
      "6489356d21a7467ebfde9747e4d56164",
      "37a7bd54ddfd4385aa5afd1cfc1114e0",
      "58172b7205b74f34a504f4465f685417",
      "8b913eb2a57649659e275696c62c9533",
      "4ea5b49492464815861f7f48cd84ea2e",
      "fa6eef54e34b49948dcada0da9335c68",
      "86bc4fd08c0c4e6aad310917c3a78e49",
      "3220083a4c30450c8b84bee884b24783",
      "ef1cf754a49c4e4ba3a9f1091a7e27f3",
      "0c719d5efc6e49d0a3edcbfe3ff87aff",
      "069ab7b9c1ab4d17aa0c5a05129b49ca",
      "c75405c7063e478c8918f648f4f5ac57",
      "941505dd237a494aa6cf1c541d23903e",
      "2240467d637848efb932ad17d2587968",
      "f6a7a5e81e1c4c1f8c78150f6122de9e",
      "51f766af52b6466482ad5f0cd98cbe23",
      "8739b2b3979743198fd9aa12797832fd",
      "20bba37c08aa475e90dd9a3629230513",
      "4be1fbd6c7df4b07a7e2b0d992d97491",
      "99f26b31f1864233a30a87c61f5dc317",
      "e255bbb1384e4ac0adef90274e2b7f32",
      "749a7627608a4981922276dd776f3dcd",
      "636ef0100b95491fa51ce616141f61b6",
      "f18a4d4a537641dc814d1fc83b0749ba",
      "b5d8935362a440f5b545d6e81cee59e6",
      "85ab528821234acd8e78e8eee5317c7c",
      "aec5b38f92a04e2a95190b236235ee4d",
      "b95e9ad3099448d685a7eae9f8eb8781",
      "b1f9ecbd708e4f548b9d424771523760",
      "d6e5e28e7e744ad09d4da6857d5e5b94",
      "79704b5d7fe34d0dbb9307f4f01d2da6",
      "e9971a2830514ee1866cc2aa3be060f4",
      "22cffe8fa77a4286b59302e438bfbe3a",
      "b1740a5dab094b839c309d38dc52c379",
      "9da736f50b37408caf324f5bcc6936bd",
      "3bbbbe2ef3b943ec939bd959c1c7f935",
      "f6f32e7844b44a21a5e876979daa4825",
      "70c61f4131bd49ae8788188e6e97b870",
      "0c6c25fa44d14e19a2b35cd946417d64",
      "98e5c06c1d0f4f9284315cef44fef2a0",
      "f72d000c13b9481ab2da30b4bff394ec",
      "c7ea67b3493347c4ab000468fc0a7cce",
      "c1d8c041033c4b82812086d6745cc0c2",
      "d6c1568e41d64a5dab6dad8fdfa84ef7",
      "3581e7149fe440249328881446be3bd4",
      "ebc6d7342be34b2b8508d333c7fd0379",
      "02c07bc961274dc3b90bf4478637be77",
      "a9e0f89d126144b6880905885d7e8b4c",
      "f837724f674743c684dec6f530504ca0",
      "4a4d09cd3ead44ef900bddd873d478ec",
      "c007c9d0ee3b4886865a1eafe9f39edf",
      "8eb257af2070413abe3271047990a79d",
      "23aaefdc4d8c4bb7a671cd971d38c65c",
      "c55e1cee6fb042f499e2738b86e7c660",
      "b08081c1c4da4cb998c50cc5960382b3",
      "653ae54f76284a23b832360e3e4c12e1",
      "6a222d1817614cc5a3ec6c4d8d4c4747",
      "27225cac17f14502bca19722e8b2841f",
      "2c7903d533994a75af0e7cb948e74328",
      "8c6f93267f63440382099529e9ac5968",
      "00c90e07bc4041cba3d16f8ed79d5bc8",
      "c788b9bee0344ffeb3451977d3a55684",
      "b9c7ab59812c4c9589622997e50e52ce",
      "2a184550de4448949f1d5733f7b29122",
      "8c892c912b5e4011abd69d76abbffa92",
      "6035ee065ff94a84bb39c0f3a78f643a",
      "d1555164df4547cab7ce1d9a77ffa35e",
      "d0b377068b16473f97c560471d1620a0",
      "554ea9fd388a402486c9b602d82c5561",
      "bc17148e153a4d18b6c6c7248a47b255",
      "c37e7ee29545425b8b8424c879d697c6",
      "2aff6539c74f459898933adc05618273",
      "364a1963c38b4d93b74445c8b9380bf5",
      "db542727056e453c9ce51162d2938aa3",
      "5bab45fec4e14340a965f5b63207d5ac",
      "926ad55f3f624693a4ae4c4531c312d1",
      "716b0f1a7f2b4d3dbc9c4f8d95314ef8",
      "435cdd7c01d3470ab65b11be1d8232f1",
      "4cd490902c3d40049849eae375f6aaa5",
      "90b17ac92a98437f9a05bf6f71ee922c",
      "879b0604a40445b5baea3e8df74490cb",
      "0bd8acb2c9be471cba64d78f0fe0d88e",
      "baab14890ac848d7934b065bac93002b",
      "8bd149565bb641b5bd21af3632dfd273",
      "b2f7b2e864114ab7a4ce80c1ba2931eb",
      "cf9e461e998a4f52a477d8de992c6fdd",
      "0abdac2b06c74357adae80070d5ad1b0",
      "41d94031101947e3a6b411a734d50f1e",
      "cf5bbebc41ba4f46b0c2294e1f8cce74",
      "0810684d4e174872a0deb481250b700f",
      "3854943ed7424d49bfa32647cb83a479",
      "9555174863824302b69bf3f6c924a2bd",
      "389316963fd14b35a8e8302af31b200f",
      "7b7cead502d34ff989870804e3ccc005",
      "21aef022b42544b494e20e8171328b29",
      "9dab773b8a054947a82355bd5fd62e92",
      "0dd9d318b7704358b3231faedc8a5209",
      "aa47fcad015a43f9ad770b875502dfe1",
      "c1aed2bb24024c9ca5b98a87cee7e441",
      "9f3ba8033cd84b45b077deb989dd5465",
      "074baf66b6e64491a0d0ee1176952d7b",
      "5661c7110d344f278c2076efa16db2de",
      "1d80c4b6cc224413839ed325e8ecb127",
      "4818855eaabf46f4bccc099b388ccda5",
      "f18d5af8d88048909bda3fe9e282a060",
      "57ed5fd71d574fbdbaff156fbe6bcc6d",
      "0c9b9e722f2d4218bb43be281a6b36b3",
      "94e72d77596b4d63afd10773c27f682b",
      "6dfd71cb774f47139a0976bff633c6fa",
      "aa430720ff444f16b14f44ccdc2a4f8e",
      "23c9f91683a94fa58527dac86632ee56",
      "b3c489671779414a8f9a9fb742dbc6c3",
      "bcbc77badf0e49bcb65fbbfe79774c70",
      "3c0016c3d3234d0491f6f149216cbf89",
      "fdf20fc2aaae4ffabe9408bb1470717f",
      "4783fcf1e15f4e4fb083d8354bddc263",
      "1ab9c9ef5ee34119a9647bb87dfab124",
      "3c536b44de9c467dadb26f0cc363354b",
      "429c50220df54f9aa842a77a1a17865f",
      "1d24f27946904553afbaef2f42c287b7",
      "6b432783af1b4f8e851e7fe465b96f23",
      "7e45d44374e74f498d5c9289415cb1ba",
      "7ccef7a0dcb646ed92b049a00ddd01a6",
      "58f7762000ab4b0f8394824028f7271e",
      "caa0c6fa428c4595a01002cc45e35d2b",
      "4e8f7972ad3541099fb5e204a4c6f9bd",
      "f8710e3b1ee340c7afdb38e1c0377dbc",
      "b816f3b79dc54a2a80c18e74b19e23da",
      "80e71e1e7f1c4027b65b6e161ea2b003",
      "5d704c3a167040acb6e8c738ddb476bd",
      "ffce04d16d764942ad62ce65fbbe50b0",
      "2a3f1e40bc5e4cc3af139425a6168bb4",
      "e0721b146f4f4768a3f4d4b52bd7bb9e",
      "fb43fff56e8345fca1fbf366a445a72c",
      "a547bc81883a425790b77892afa78597",
      "7bae02485f9d4d7595dcea9960997731",
      "513d365f93e644a6a6e6c70d7eb6ae2f",
      "c52c02c870ea406091aa13870358b62c",
      "394ff0dbee624b40a7fce64900624298",
      "161b0a5b7c464c598143252fa3d591d0",
      "83ac8bfb8c484496beee311741628df3",
      "9966a13ec1964c38b7032f0a73ff4ab8",
      "90fab11eeb1d4d8898eba98850777ee2",
      "f1ccdca32d80457c848d72ad3c362a70",
      "9cc5f2db9bcf454ebe65dccad4b08339",
      "833ac0bb16824e97a058ee5ac3ca58da"
     ]
    },
    "id": "wdSUedglaWoj",
    "outputId": "01b668f0-a87c-467f-df05-130fe5c0e434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING AND PREPROCESSING DATA\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0ecc86ba384aeaa79ecf43f6c75e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201fc9f27f27439fbe6da7cd118bfbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2792\n",
      "Eval size: 311\n",
      "Held-out test size: 345\n",
      "Number of classes: 9\n",
      "Labels: ['Claims ignorance', 'Clarification', 'Declining to answer', 'Deflection', 'Dodging', 'Explicit', 'General', 'Implicit', 'Partial/half-answer']\n",
      "\n",
      "================================================================================\n",
      "TRAINING: Logistic Regression (TF-IDF)\n",
      "================================================================================\n",
      "Training Logistic Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: Logistic Regression (TF-IDF)\n",
      "================================================================================\n",
      "Macro F1 Score:     0.4708\n",
      "Accuracy:           0.5420\n",
      "Macro Precision:    0.4679\n",
      "Macro Recall:       0.4874\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING: DeBERTa-v3-large (Best Run)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4fe8e68a584726890a884782be9a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f600b87486f948bcb455d3d6e418ac61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f03154fc9164ff7ab712bb656e4ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc442ec99d3a43f8ab7bba8ea7614c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df397adb76543e9b12bf9fe0eaff1b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36ffd63a17e4e098d36fb392aa2a5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DeBERTa-v3-large with BEST config: SEED=777, LR=8e-06...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/2625 19:50 < 09:56, 1.47 it/s, Epoch 10/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.968240</td>\n",
       "      <td>0.517685</td>\n",
       "      <td>0.311372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.881637</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.367892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.087200</td>\n",
       "      <td>1.771602</td>\n",
       "      <td>0.575563</td>\n",
       "      <td>0.496934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.087200</td>\n",
       "      <td>1.741801</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.570789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.087200</td>\n",
       "      <td>1.725636</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.577772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.684600</td>\n",
       "      <td>1.798261</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.616613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.684600</td>\n",
       "      <td>1.864015</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.585424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.684600</td>\n",
       "      <td>1.897147</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.602864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>2.033954</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.575462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.369100</td>\n",
       "      <td>2.109884</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.571622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: DeBERTa-v3-large (Best Run)\n",
      "================================================================================\n",
      "Macro F1 Score:     0.5770\n",
      "Accuracy:           0.6290\n",
      "Macro Precision:    0.5911\n",
      "Macro Recall:       0.5801\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING: RoBERTa-large\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b3253e984548ce916f0aa15768065f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "323d9c206413445db43203975c57564f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8210b038a8234d89a9ba7ed429c02c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2ac5ecc80ae445eb14744ad71518b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d5d4de9b604aaea21435e9071e6ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7576f431f9b84ac0b2b17df39a1c49c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602ffd9194744f2e94b8738d67876eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d2ef1a85e2445b6aa1529064eea5afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0557284eb54d3695bc311b2825741e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8520e8a70624e0098e9c461045a80a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af87e5f5ecf4f85a523cba3420796fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba4691b108245599e984cd3b520bee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RoBERTa-large with BEST config: SEED=42, LR=1e-05, grad_accum=8...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 15:50, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.327952</td>\n",
       "      <td>0.463023</td>\n",
       "      <td>0.241138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.925616</td>\n",
       "      <td>0.543408</td>\n",
       "      <td>0.305455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.869986</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.406938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.748328</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.545329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.762902</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.547445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.747752</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.574701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.761032</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.558068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.732219</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.587030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.809152</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.585435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.842830</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.589323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>1.876401</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.587926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.392700</td>\n",
       "      <td>1.896070</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.597725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.392700</td>\n",
       "      <td>1.925611</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.593122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.392700</td>\n",
       "      <td>1.925798</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.586440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.392700</td>\n",
       "      <td>1.927766</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.588327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: RoBERTa-large\n",
      "================================================================================\n",
      "Macro F1 Score:     0.5862\n",
      "Accuracy:           0.6203\n",
      "Macro Precision:    0.5922\n",
      "Macro Recall:       0.5844\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING: SetFit (Paraphrase-Mpnet-Base-V2)\n",
      "================================================================================\n",
      "SetFit not available. Install with: pip install setfit\n",
      "\n",
      "================================================================================\n",
      "TRAINING: XLNet-large\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b913eb2a57649659e275696c62c9533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/761 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a7a5e81e1c4c1f8c78150f6122de9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ab528821234acd8e78e8eee5317c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f32e7844b44a21a5e876979daa4825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e0f89d126144b6880905885d7e8b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7903d533994a75af0e7cb948e74328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc17148e153a4d18b6c6c7248a47b255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "879b0604a40445b5baea3e8df74490cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9555174863824302b69bf3f6c924a2bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d80c4b6cc224413839ed325e8ecb127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0016c3d3234d0491f6f149216cbf89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XLNet-large with BEST config: SEED=42, LR=1e-05, epochs=10...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 20:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.009560</td>\n",
       "      <td>0.450161</td>\n",
       "      <td>0.268619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.897874</td>\n",
       "      <td>0.530547</td>\n",
       "      <td>0.360086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>8.652900</td>\n",
       "      <td>1.853642</td>\n",
       "      <td>0.540193</td>\n",
       "      <td>0.371574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.652900</td>\n",
       "      <td>1.759020</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.482820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.652900</td>\n",
       "      <td>1.721151</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.508384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7.144900</td>\n",
       "      <td>1.733062</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.556083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7.144900</td>\n",
       "      <td>1.735185</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.554888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>7.144900</td>\n",
       "      <td>1.746186</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.568418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>6.212600</td>\n",
       "      <td>1.749102</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.565442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.212600</td>\n",
       "      <td>1.763128</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.569794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: XLNet-large\n",
      "================================================================================\n",
      "Macro F1 Score:     0.5775\n",
      "Accuracy:           0.6261\n",
      "Macro Precision:    0.5931\n",
      "Macro Recall:       0.5762\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CREATING: Ensemble (RoBERTa + DeBERTa)\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa0c6fa428c4595a01002cc45e35d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bae02485f9d4d7595dcea9960997731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS: Ensemble (RoBERTa + DeBERTa)\n",
      "================================================================================\n",
      "Macro F1 Score:     0.5874\n",
      "Accuracy:           0.6261\n",
      "Macro Precision:    0.5959\n",
      "Macro Recall:       0.5843\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "FINAL SUMMARY - ALL MODELS\n",
      "================================================================================\n",
      "\n",
      "                       Model Macro F1 Accuracy Macro Precision Macro Recall\n",
      "Logistic Regression (TF-IDF)   0.4708   0.5420          0.4679       0.4874\n",
      " DeBERTa-v3-large (Best Run)   0.5770   0.6290          0.5911       0.5801\n",
      "               RoBERTa-large   0.5862   0.6203          0.5922       0.5844\n",
      "                 XLNet-large   0.5775   0.6261          0.5931       0.5762\n",
      "Ensemble (RoBERTa + DeBERTa)   0.5874   0.6261          0.5959       0.5843\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Results saved to: task2_all_models_results.csv\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSequenceClassification,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        EarlyStoppingCallback,\n",
    "        set_seed\n",
    "    )\n",
    "except ImportError as e:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ERROR: Transformers import failed!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"QUICK FIX - Run these commands in Colab cells (in order):\")\n",
    "    print(\"\")\n",
    "    print(\"Cell 1:\")\n",
    "    print(\"  !pip uninstall -y transformers\")\n",
    "    print(\"  !pip install --upgrade --force-reinstall --no-cache-dir transformers\")\n",
    "    print(\"  !pip install transformers[torch]\")\n",
    "    print(\"\")\n",
    "    print(\"Cell 2:\")\n",
    "    print(\"  !pip install datasets scikit-learn accelerate torch pandas setfit sentence-transformers\")\n",
    "    print(\"\")\n",
    "    print(\"THEN: Runtime -> Restart runtime\")\n",
    "    print(\"THEN: Run this script again\")\n",
    "    print(\"=\"*80)\n",
    "    raise e\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING AND PREPROCESSING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=42,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "print(f\"Train size: {len(train_ds)}\")\n",
    "print(f\"Eval size: {len(eval_ds)}\")\n",
    "print(f\"Held-out test size: {len(held_out_test_ds)}\")\n",
    "print(f\"Number of classes: {len(labels)}\")\n",
    "print(f\"Labels: {labels}\")\n",
    "\n",
    "test_texts = [ex[\"text\"] for ex in held_out_test_ds]\n",
    "test_labels = [ex[\"evasion_label\"] for ex in held_out_test_ds]\n",
    "\n",
    "def compute_all_metrics(y_true, y_pred, labels):\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    macro_recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    return {\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall\n",
    "    }\n",
    "\n",
    "def print_results(model_name, metrics):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"RESULTS: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Macro F1 Score:     {metrics['macro_f1']:.4f}\")\n",
    "    print(f\"Accuracy:           {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Macro Precision:    {metrics['macro_precision']:.4f}\")\n",
    "    print(f\"Macro Recall:       {metrics['macro_recall']:.4f}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "train_texts = [ex[\"text\"] for ex in train_ds]\n",
    "train_labels = [ex[\"evasion_label\"] for ex in train_ds]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=40000,\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_vec = vectorizer.fit_transform(train_texts)\n",
    "X_test_vec = vectorizer.transform(test_texts)\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=2500,\n",
    "    class_weight=\"balanced\",\n",
    "    multi_class=\"auto\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model.fit(X_train_vec, train_labels)\n",
    "\n",
    "lr_preds = lr_model.predict(X_test_vec)\n",
    "lr_metrics = compute_all_metrics(test_labels, lr_preds, labels)\n",
    "print_results(\"Logistic Regression (TF-IDF)\", lr_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: DeBERTa-v3-large (Best Run)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def train_deberta():\n",
    "    MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "    OUTPUT_DIR = \"./deberta_v3_best\"\n",
    "\n",
    "    DEBERTA_SEED = 777\n",
    "    DEBERTA_LR = 8e-6\n",
    "    DEBERTA_EPOCHS = 15\n",
    "    DEBERTA_GRAD_ACCUM = 4\n",
    "\n",
    "    random.seed(DEBERTA_SEED)\n",
    "    np.random.seed(DEBERTA_SEED)\n",
    "    torch.manual_seed(DEBERTA_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(DEBERTA_SEED)\n",
    "        torch.cuda.manual_seed_all(DEBERTA_SEED)\n",
    "    set_seed(DEBERTA_SEED)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    train_tokenized = train_ds.map(tokenize_fn, batched=True)\n",
    "    eval_tokenized = eval_ds.map(tokenize_fn, batched=True)\n",
    "    test_tokenized = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "    train_tokenized = train_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    eval_tokenized = eval_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    test_tokenized = test_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "    y_train = train_tokenized[\"evasion_label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        class_weights,\n",
    "        dtype=torch.float\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            loss_fct = nn.CrossEntropyLoss(\n",
    "                weight=class_weights_tensor,\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.model.config.num_labels),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (predictions == labels).mean(),\n",
    "            \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=DEBERTA_LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=DEBERTA_GRAD_ACCUM,\n",
    "        num_train_epochs=DEBERTA_EPOCHS,\n",
    "        weight_decay=0.05,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        seed=DEBERTA_SEED,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    )\n",
    "\n",
    "    print(f\"Training DeBERTa-v3-large with BEST config: SEED={DEBERTA_SEED}, LR={DEBERTA_LR}...\")\n",
    "    trainer.train()\n",
    "\n",
    "    test_preds = trainer.predict(test_tokenized)\n",
    "    test_pred_ids = np.argmax(test_preds.predictions, axis=-1)\n",
    "\n",
    "    return test_pred_ids, trainer, test_tokenized\n",
    "\n",
    "deberta_preds, deberta_trainer, deberta_test_tokenized = train_deberta()\n",
    "deberta_metrics = compute_all_metrics(test_labels, deberta_preds, labels)\n",
    "print_results(\"DeBERTa-v3-large (Best Run)\", deberta_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: RoBERTa-large\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def train_roberta():\n",
    "    MODEL_NAME = \"roberta-large\"\n",
    "    OUTPUT_DIR = \"./roberta_large_final\"\n",
    "\n",
    "    ROBERTA_SEED = 42\n",
    "    ROBERTA_LR = 1e-5\n",
    "    ROBERTA_EPOCHS = 15\n",
    "    ROBERTA_GRAD_ACCUM = 8\n",
    "\n",
    "    random.seed(ROBERTA_SEED)\n",
    "    np.random.seed(ROBERTA_SEED)\n",
    "    torch.manual_seed(ROBERTA_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(ROBERTA_SEED)\n",
    "        torch.cuda.manual_seed_all(ROBERTA_SEED)\n",
    "    set_seed(ROBERTA_SEED)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    train_tokenized = train_ds.map(tokenize_fn, batched=True)\n",
    "    eval_tokenized = eval_ds.map(tokenize_fn, batched=True)\n",
    "    test_tokenized = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "    train_tokenized = train_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    eval_tokenized = eval_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    test_tokenized = test_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "    y_train = train_tokenized[\"evasion_label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        class_weights,\n",
    "        dtype=torch.float32\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            loss_fct = nn.CrossEntropyLoss(\n",
    "                weight=class_weights_tensor,\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.model.config.num_labels),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (preds == labels).mean(),\n",
    "            \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=ROBERTA_LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=ROBERTA_GRAD_ACCUM,\n",
    "        num_train_epochs=ROBERTA_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=2,\n",
    "        seed=ROBERTA_SEED,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    "    )\n",
    "\n",
    "    print(f\"Training RoBERTa-large with BEST config: SEED={ROBERTA_SEED}, LR={ROBERTA_LR}, grad_accum={ROBERTA_GRAD_ACCUM}...\")\n",
    "    trainer.train()\n",
    "\n",
    "    test_preds = trainer.predict(test_tokenized)\n",
    "    test_pred_ids = np.argmax(test_preds.predictions, axis=-1)\n",
    "\n",
    "    return test_pred_ids, trainer\n",
    "\n",
    "roberta_preds, roberta_trainer = train_roberta()\n",
    "roberta_metrics = compute_all_metrics(test_labels, roberta_preds, labels)\n",
    "print_results(\"RoBERTa-large\", roberta_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: SetFit (Paraphrase-Mpnet-Base-V2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    from setfit import SetFitModel, Trainer as SetFitTrainer, TrainingArguments as SetFitTrainingArguments\n",
    "\n",
    "    def train_setfit():\n",
    "        MODEL_NAME = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "\n",
    "        train_df = train_ds.to_pandas()\n",
    "        eval_df = eval_ds.to_pandas()\n",
    "        test_df = held_out_test_ds.to_pandas()\n",
    "\n",
    "        from datasets import Dataset\n",
    "        train_ds_setfit = Dataset.from_pandas(train_df[[\"text\", \"evasion_label\"]])\n",
    "        test_ds_setfit = Dataset.from_pandas(test_df[[\"text\", \"evasion_label\"]])\n",
    "\n",
    "        model = SetFitModel.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            labels=sorted(labels)\n",
    "        )\n",
    "\n",
    "        args = SetFitTrainingArguments(\n",
    "            num_iterations=5,\n",
    "            batch_size=32,\n",
    "            num_epochs=15,\n",
    "            body_learning_rate=2e-5,\n",
    "            head_learning_rate=1e-2,\n",
    "            use_amp=True,\n",
    "            report_to=[],\n",
    "        )\n",
    "\n",
    "        trainer = SetFitTrainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds_setfit,\n",
    "            column_mapping={\"text\": \"text\", \"evasion_label\": \"label\"},\n",
    "        )\n",
    "\n",
    "        print(\"Training SetFit...\")\n",
    "        trainer.train()\n",
    "\n",
    "        test_texts_list = test_df[\"text\"].tolist()\n",
    "        test_preds = model.predict(test_texts_list)\n",
    "\n",
    "        label2id_map = {l: i for i, l in enumerate(labels)}\n",
    "        test_pred_ids = [label2id_map.get(p, 0) for p in test_preds]\n",
    "\n",
    "        return test_pred_ids\n",
    "\n",
    "    setfit_preds = train_setfit()\n",
    "    setfit_metrics = compute_all_metrics(test_labels, setfit_preds, labels)\n",
    "    print_results(\"SetFit (Paraphrase-Mpnet-Base-V2)\", setfit_metrics)\n",
    "\n",
    "except ImportError:\n",
    "    print(\"SetFit not available. Install with: pip install setfit\")\n",
    "    setfit_metrics = None\n",
    "    setfit_preds = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING: XLNet-large\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def train_xlnet():\n",
    "    MODEL_NAME = \"xlnet-large-cased\"\n",
    "    OUTPUT_DIR = \"./xlnet_large\"\n",
    "\n",
    "    XLNET_SEED = 42\n",
    "    XLNET_LR = 1e-5\n",
    "    XLNET_EPOCHS = 10\n",
    "    XLNET_GRAD_ACCUM = 4\n",
    "\n",
    "    random.seed(XLNET_SEED)\n",
    "    np.random.seed(XLNET_SEED)\n",
    "    torch.manual_seed(XLNET_SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(XLNET_SEED)\n",
    "        torch.cuda.manual_seed_all(XLNET_SEED)\n",
    "    set_seed(XLNET_SEED)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def tokenize_fn(examples):\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    train_tokenized = train_ds.map(tokenize_fn, batched=True)\n",
    "    eval_tokenized = eval_ds.map(tokenize_fn, batched=True)\n",
    "    test_tokenized = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "    train_tokenized = train_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    eval_tokenized = eval_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "    test_tokenized = test_tokenized.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "    y_train = train_tokenized[\"evasion_label\"]\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights_tensor = torch.tensor(\n",
    "        class_weights,\n",
    "        dtype=torch.float\n",
    "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    class WeightedTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            loss_fct = nn.CrossEntropyLoss(\n",
    "                weight=class_weights_tensor,\n",
    "                label_smoothing=0.1\n",
    "            )\n",
    "            loss = loss_fct(\n",
    "                logits.view(-1, self.model.config.num_labels),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (preds == labels).mean(),\n",
    "            \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        learning_rate=XLNET_LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "        gradient_accumulation_steps=XLNET_GRAD_ACCUM,\n",
    "        num_train_epochs=XLNET_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        report_to=\"none\",\n",
    "        seed=XLNET_SEED,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_tokenized,\n",
    "        eval_dataset=eval_tokenized,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    print(f\"Training XLNet-large with BEST config: SEED={XLNET_SEED}, LR={XLNET_LR}, epochs={XLNET_EPOCHS}...\")\n",
    "    trainer.train()\n",
    "\n",
    "    test_preds = trainer.predict(test_tokenized)\n",
    "    test_pred_ids = np.argmax(test_preds.predictions, axis=-1)\n",
    "\n",
    "    return test_pred_ids\n",
    "\n",
    "xlnet_preds = train_xlnet()\n",
    "xlnet_metrics = compute_all_metrics(test_labels, xlnet_preds, labels)\n",
    "print_results(\"XLNet-large\", xlnet_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING: Ensemble (RoBERTa + DeBERTa)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_ensemble_predictions():\n",
    "    roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large\")\n",
    "    def roberta_tokenize_fn(examples):\n",
    "        return roberta_tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "    test_tokenized_roberta = held_out_test_ds.map(roberta_tokenize_fn, batched=True)\n",
    "    test_tokenized_roberta = test_tokenized_roberta.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "    roberta_test_preds = roberta_trainer.predict(test_tokenized_roberta)\n",
    "    roberta_logits = roberta_test_preds.predictions\n",
    "\n",
    "    deberta_test_preds = deberta_trainer.predict(deberta_test_tokenized)\n",
    "    deberta_logits = deberta_test_preds.predictions\n",
    "\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "    roberta_probs = softmax(roberta_logits)\n",
    "    deberta_probs = softmax(deberta_logits)\n",
    "\n",
    "    ensemble_probs = (roberta_probs + deberta_probs) / 2.0\n",
    "    ensemble_preds = np.argmax(ensemble_probs, axis=-1)\n",
    "\n",
    "    return ensemble_preds\n",
    "\n",
    "ensemble_preds = get_ensemble_predictions()\n",
    "ensemble_metrics = compute_all_metrics(test_labels, ensemble_preds, labels)\n",
    "print_results(\"Ensemble (RoBERTa + DeBERTa)\", ensemble_metrics)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY - ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = [\n",
    "    (\"Logistic Regression (TF-IDF)\", lr_metrics),\n",
    "    (\"DeBERTa-v3-large (Best Run)\", deberta_metrics),\n",
    "    (\"RoBERTa-large\", roberta_metrics),\n",
    "]\n",
    "\n",
    "if setfit_metrics:\n",
    "    results.append((\"SetFit (Paraphrase-Mpnet-Base-V2)\", setfit_metrics))\n",
    "\n",
    "results.extend([\n",
    "    (\"XLNet-large\", xlnet_metrics),\n",
    "    (\"Ensemble (RoBERTa + DeBERTa)\", ensemble_metrics),\n",
    "])\n",
    "\n",
    "summary_data = []\n",
    "for model_name, metrics in results:\n",
    "    summary_data.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Macro F1\": f\"{metrics['macro_f1']:.4f}\",\n",
    "        \"Accuracy\": f\"{metrics['accuracy']:.4f}\",\n",
    "        \"Macro Precision\": f\"{metrics['macro_precision']:.4f}\",\n",
    "        \"Macro Recall\": f\"{metrics['macro_recall']:.4f}\"\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + df_summary.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "df_summary.to_csv(\"task2_all_models_results.csv\", index=False)\n",
    "print(\"\\nResults saved to: task2_all_models_results.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
