{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b9bcfc93eb3549d7b99c76adf7ce11f5",
      "d2f984db145f4d5d97873ec8c5e0ba63",
      "6e470e1d22664669a5d2c93f6efd7d9f",
      "c83a7209d36f4c638038e7db18ffaa79",
      "52570a84d51d4ce7a1b5f73f5d816eec",
      "5f4e9559c53a4db9a7a506a341ac6bfb",
      "c69b4c061c7643bcaf41b21b65daaf0e",
      "fdcb912c78414864a86c8bc01160cab1",
      "e2c755aa0d7442c590d972a54044eafb",
      "14379592f08e44d9b94a38b66badda7d",
      "91eb9fd39f284a6e915680d12a9c79bf",
      "572e8997ea3445bda7624d5d6b67fea7",
      "3458688e169946a3948cfc734ca4ab05",
      "dd354f0a23564d949efb997ec9a01650",
      "51735d4eaf9c43a1a081b322f4c04142",
      "38c7cbf47a944b798d84bf4e3dc7087e",
      "b2ef69e4715e4aa2b333478ecd43efb6",
      "90501eaa91394a83a48858a2483e1d83",
      "555936b6d0f34d26b3739bf72669b6fc",
      "967de9a8fc3d477ca0b9a67788024a3d",
      "b0327145e368472a9b5d843b74ddb1ed",
      "37d71e3059e64aa39dd0056380683368",
      "06f0a96fcca44c819c071af7153f89e2",
      "f9b9a86b65c542b7974d833fc0b4e3f6",
      "70765b0465ad437ba86236fbc130d53e",
      "d383fc71d93d45508558ca3965d5af8b",
      "38b54dd62c1a465595d9cacb66e7163b",
      "ea178f610c69431d8958499e3ab365bf",
      "cf05907cc8a348fa8e7fa14b47714938",
      "2af7d5caab174ee2bb510956b2535b9d",
      "b2ae7e9a03034a35a1e4372f57ace963",
      "e750a1109220467bad7e5a3c474ad531",
      "e6efce8525f74770986907be9b76edcf",
      "1630ec277a0e4e8f9117322157a35f8c",
      "842e69104fc74f75b8ef84dcf944cbfa",
      "42e0b3770cff488eb0baf4670d646e32",
      "9d8d5d843e5a4b929cc5e1c53269b477",
      "7bc0e78332624af884395b74a9945641",
      "dd5af466fbdc42e6b70e6e91834a9798",
      "fa00a22908c941969324abbce3444bdb",
      "1623807ffb074cf5b0f5fd0e4a6c77e6",
      "e63177e6d2b5451b851a940c7d22d30d",
      "2bc4f5c1326244e3828b4088186ec618",
      "f8feadbab4fb4b22b2bbb69d257c616d",
      "265cb1c797a842ba8b7084261758d8fd",
      "49adfe2075094b928c8ffd2d037c7054",
      "a6983b883605498ebe5c70415ed8dd46",
      "bb462749f1d34f998382a90821937c92",
      "9c6f8b68f64746cf8fe96166fec53327",
      "edad25386f654e6fba123c1e3e90b1d6",
      "d6f47645b42e40eb954633c34acc4df2",
      "5c66465f1fa64cd788c672a8ea754c96",
      "5463c9c2f5ee46a29ef1d63b38ab94c2",
      "33e1f08587b24edead25eb4cff5a1269",
      "bb1b8cc4fc6146fe85b378e848f354ed",
      "ede23376d9bb4104b33ec09ea32a3d51",
      "3e2114d245b244978ec3bdfada6325ce",
      "a6d6632b44394a9ea9afd056c6a071f2",
      "b0082fd5433842cd9a2b384dd43a00aa",
      "d851785caf9649c3849a3f84ff8aac96",
      "095285d7fc0747169a245308a74098de",
      "d983a70f08e84f918b9c0344a28b1ec4",
      "3d7883d1e09040bea835acabc920a645",
      "058bacdc96b147c4a9acb56690456e32",
      "ecb0d3d4b4b64c56bc1477bb2c79551e",
      "520a353ad9434740824cefb3acd7d335",
      "ef83b48f9ae64b138384bf0974e93cec",
      "3f6ec005f2634d12aff06dc422a1a12b",
      "fb86a663d2504644b24fc3361a4cdc99",
      "709e6422868c419e96720cbcef96d690",
      "644444d57dbc4f3caeabb6468fe38864",
      "852f414bd58d418b8db57c0e80636116",
      "58894e3b7cdb48d5ad8e8f3ee55d841c",
      "4f087ebe34ec4c499d04115dbff4e7b8",
      "506f56ab807748fd865660f25c284651",
      "a0d2fc012deb407cbd2e72944fa67ffc",
      "170f626e07c9493cbe5b81c2caf2e1c1",
      "a690a1fd33914ea4be3037e8b7c149c8",
      "887d29f3c1f140d899c9a24d45e5d33a",
      "706d17a5052d4f5ba087b4e8ea2ba887",
      "2fea486e3986467f972313221f376851",
      "9aecdcac1aee41cd9777fc592b9b06ce",
      "22cbd75b158d4b73bc0aafdf87424e88",
      "ac544de1a88b4de3b9039fb9d2e949ea",
      "d0745b7207644b11a258a8d8da9c4553",
      "71b001fa7cf64137b43ce28d03b3aab0",
      "091b65af47c14d32936a3ae66196edf9",
      "e51713bc0e0944c6a42abbfb011fee83",
      "1ffe6e874e704b01a699b7b8728b597d",
      "f7be2ec2c4834960af82ba1b6a061d31",
      "08042217e6bd48cd8013fb5eaa9ace52",
      "4b0682bf5e5d4e9e8a02bee6a84fbe71",
      "c8ae302f6387403f8019d051e5054f23",
      "d573a6587a164504b976bfb4a09220ad",
      "8ea28b8061394eca86df2322c0ae560e",
      "e40c8c23891e4ea08663efb30d247a45",
      "90d19cdd623d453485d5dfba7a883055",
      "6deecd084bea410db6f1ea412d967d49",
      "76e213863c4a47d59d746f51712217ad",
      "e826a00fc9414f37a0cb1c592fd9be34",
      "8b4b2ae96bba45e2bc1fc6258492eece",
      "1b017f5b2ec54f51b899f251e8648aea",
      "0a72ea54797c4c16ba8c1996b8f994e4",
      "4745f77c9273405d8a0db263f87ee2a9",
      "e422a6b565e344b2accd229851c4b650",
      "7c17abd90def41e69e886272fef1a6d1",
      "c490dc9a038743cda92600107ad77d29",
      "a3275ce501cf4f049d400cfd1c1794a8",
      "7224e1d758a74fd29eca7227b0f4b34f",
      "9fbe0b92621e44bdaf7a3a2c4b0a19eb",
      "1e013ff88b75415e810e2fe426f82473",
      "c5d52ab3308143aa9b41aa3aabe7e693",
      "e88bb636dcd94869b8e6fa34ef3f10be",
      "d822de2a61564afeaa355711ad2d4265",
      "2e77868239974236b3e7c660d7d145bb",
      "a35f9271c1ca4b00bb5c4760ced09d25",
      "2e32adc635fc4bfdba3a6cf2df7c7488",
      "7b542e0dd50a4f04a3a9551993cc2b8f",
      "44cbc05078bd4999b54ac20ecd749af3",
      "9483d49c3a7347ddb478e1b0a41bac64",
      "eca348a859ac4ceba6642262fe8bf85d",
      "bcd3b8fc195e40df950b934d64599552",
      "3ef1b95304cb4a1f94d76eb1efc7d0ab",
      "32ce5ea1bf954674bdc58709243f92f5",
      "78b93e34bf8643df9896db768857e62a",
      "dfdb6600e46642488fc633a1ef833c4d",
      "449fa9c1adca400180ee632c01881591",
      "aa69c2c273414909bb336d13b3d2c79a",
      "c617962b9f8e4b67a4216f31ec6cd1fe",
      "b6b037b806314329b34cfb2d30b813d8",
      "b23a2745fbd64bb3b5a99be8476fe743",
      "6299c97fc3e44fb381d0b5a638e718a5",
      "f763e5d092bc4ba7a175e2bcf4a59b6d",
      "6c4c5cae2bcd4108a1624d3271eea467",
      "f440b27538f94c019cfe11b21b827580",
      "b1b064e418b14bc2a822ebf8ef784ea2",
      "aebbbe5fdb084daebcc89d6ee66cb395",
      "38580aa50ac84a7f94fa2314bc8ffd6b",
      "e896764eaeda4b4ba1ad4f93c2bc8fbd",
      "7f2b8f8e68df43dab91c78af4c57b139",
      "0fdc66bd3b464bc09216a13ddeb82482",
      "fed0c557487e448bb61d7603dbe9ac1d",
      "3812afbaa91149ef84826c53efc82ea6",
      "1c3e57fae203435db46a4e6b946f26c8",
      "fc1f97fdb9f849af87957fe1a2cf8497",
      "126123467b6b4b54b4bff729c6fd5401",
      "91f5c6a241fc4b86a13b527ce3d4b06e",
      "2adbc182a4b547f1aa2ddf9514b610b2",
      "3efd8ff272954c18a34c0681ce9ab74d",
      "5570f2cae330496583a2e126202e7bc3",
      "efa1ccad61b149d5b9380b2cda23f53d",
      "8ac2244a753a407ab7b68ef0a7c7bddc",
      "83b5a86675e74192b2c3d8a76a2e056d",
      "223e11efeaac4d528eee1dd9c7e1791a",
      "bcd93b99e0f24f6f949bf7aef78ca964",
      "f1283153d9d74d4faadca5d213260fbc",
      "fbcc1b3b87a54effb8a32889b337b2e5",
      "8f7faeb47a5c4611bfffeb83f4f39aa9",
      "94c63e767f0241cb844e5761fa3e7dc2",
      "b3b8f262b5074ec59397ecfa093298a4",
      "fdef4239974e416eabdf07346bc5d9a8",
      "75d9a72d41dc4b1f84c829af960b4d69",
      "dc2a33e2113d4135a7390a3020f4e37b",
      "02ccf169153a4e7d9f94e077db70bd18",
      "e46713667dd746099c50cee1b93abc8c",
      "022eeee084334c14858daf6436e7a91b",
      "d759be7503574e2a89482442e6fd6295",
      "9c578cd869234eba81e3305965a6b228",
      "aa3a1896d1b2479fa0e1598c9e5809ea",
      "17255bac9aae4261b8d33dc9c638b900",
      "3624443d09d14a19a8d7b1cfed7d36b6",
      "19a5a411b97f4ee08e202b367040effe",
      "3a46e2ad3ee54e6a9a6349dc997431d2",
      "29dd43c29e694e0786573d3dbea7c1e5",
      "7129347d92c54cd9be0c9505b39862f9",
      "39c17430c5ef41288819d7ff8a185cff",
      "94aa2d5e35c84a6ca0dd095b5646d1d5",
      "dad76d050835424a94a010fc121a0c7e",
      "679a8866674a4a5d9130d70cda977526",
      "c768abac96a14078a15520307503b4cb",
      "457eb923bcde4c3b873a369ca8fd0624",
      "af9929d8fb414dc1b955c3c1ef717778",
      "0aa9d20debac4e5fb958b8589db78c93",
      "b3fc492b3bdd49b0aa7f75789485ab9a",
      "f972c82955eb4729ad73eb657d3d0003",
      "5685431cdaca41c4981dfde4ab5f19a7",
      "1d9d02d6982b4640ad57ea2a082f281f",
      "76bf11ec884c4b42a2a606e60eb47201",
      "b4f112bde59f44b3abe1e8fdca1d81b3",
      "28d3465436e241259e151e547249059d",
      "00927a8766b14791ac655e726dc96f49",
      "3a708f38c3924ea1b80e419f5a801884",
      "94d33885aeb34db18a6a25c2d85365f2",
      "b0cc3eab5a0b4d618c8fbe139089d7b6",
      "4ab33be7e70f409dbd67df980d26784c",
      "4f5379406f394d21acd961a1314499f6",
      "039d93f4e7d44bdeae1f885bac5312ec",
      "f50df73771b24287a9e2260fe6cad55c",
      "6e5e86792bd348ba9cafd4d05a935af0",
      "08a15e29ed8a4ccda24fbddbbbe91fda",
      "6705d252fdb84075b88c31038a645dae",
      "65853930aa9c41648b08d60ca7b02ba0",
      "b7435f55e7434071bcad98de2279ede3",
      "de2dd8278b7340829c785c63d8d7023c",
      "c824f2a2c7a4496fa16a781f43d28a69",
      "f2b9fa35a42540c2befb270301d15cec",
      "7d8756edead7419dab67be2516b9167d",
      "c131bff00f9a46e2b840528f0d7be4e8",
      "46bfbc795724473da02236a37cfbde97",
      "6793bbedf1d04bf4b6f16f6d1e0bb86e",
      "71dd820a592545819661e1830b0b3caa",
      "bc955e779ad74e9a8a0727bd7d78b24c",
      "22e0a0b07ef54c9eabee4a19bc7232f3",
      "5db096b425a74d878a2cc31ffa1fbc3d",
      "ea197b0201b348efa4d4ee1e182e89a6",
      "d445ac3cf617443081c6ee7a002a646b",
      "2c7a55a4229a44b7ad3878c11cc7b3c4",
      "28b562ad2e0c43f4b91f88ecfdd13057",
      "e3e899cf6582427b83ee3d1116a2f151",
      "0fba8c581c964dab9af3d9973461e9c4"
     ]
    },
    "id": "ZFPbrsFEx9Ta",
    "outputId": "bc5455ef-b2fc-4b12-baa0-45886df732c7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Data...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b9bcfc93eb3549d7b99c76adf7ce11f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "572e8997ea3445bda7624d5d6b67fea7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "06f0a96fcca44c819c071af7153f89e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1630ec277a0e4e8f9117322157a35f8c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "265cb1c797a842ba8b7084261758d8fd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preprocessing...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ede23376d9bb4104b33ec09ea32a3d51"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef83b48f9ae64b138384bf0974e93cec"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a690a1fd33914ea4be3037e8b7c149c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ffe6e874e704b01a699b7b8728b597d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e826a00fc9414f37a0cb1c592fd9be34"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e013ff88b75415e810e2fe426f82473"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcd3b8fc195e40df950b934d64599552"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f763e5d092bc4ba7a175e2bcf4a59b6d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c3e57fae203435db46a4e6b946f26c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcd93b99e0f24f6f949bf7aef78ca964"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "022eeee084334c14858daf6436e7a91b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94aa2d5e35c84a6ca0dd095b5646d1d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76bf11ec884c4b42a2a606e60eb47201"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e5e86792bd348ba9cafd4d05a935af0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting DeBERTa Training (Aggressive Mode)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='880' max='880' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [880/880 19:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.804013</td>\n",
       "      <td>0.514469</td>\n",
       "      <td>0.327759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.620513</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.360308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.433151</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.549278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.421762</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.546757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.339869</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.597048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>1.457686</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.593007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>1.496824</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.579195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>1.602072</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.606085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>1.648070</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.575930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>1.668981</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.589294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.5699\n",
      "\n",
      "Generating submission.csv...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6793bbedf1d04bf4b6f16f6d1e0bb86e"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "SEED = 999  # trying a new seed\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 8  # Effective Batch = 32\n",
    "LR = 1.5e-5            # Higher Learning Rate\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess_text)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# Double Split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        # Slightly reduced smoothing\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.05\n",
    "        )\n",
    "\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "    }\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "\n",
    "    weight_decay=0.01,             # Low decay\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",    # Linear scheduler\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = ProTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(\"Starting DeBERTa Training (Aggressive Mode)...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\n FINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission.csv...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"index\": comp_test_ds[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })\n",
    "    # out_df.to_csv(\"submission_pro.csv\", index=False)\n",
    "    # files.download(\"submission_pro.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "SEED = 2024\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 8\n",
    "LR = 9e-6\n",
    "EPOCHS = 12\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess_text)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# double Split the data\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.05\n",
    "        )\n",
    "\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "    }\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "\n",
    "    weight_decay=0.01,             # Flexible decay\n",
    "    warmup_ratio=0.2,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = ProTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]\n",
    ")\n",
    "\n",
    "print(f\"Starting Training with LR={LR}, Warmup=0.2, Cosine...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\n FINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission.csv...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"index\": comp_test_ds[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "13aa3c8a87ad4520812691d37baeeb7d",
      "2259bf25924a4f4da2270e97e626ae86",
      "6bb987eac56942368ce70fa5b6d67b54",
      "c71b017da6a74a7ab51f6e5dc97ebfe8",
      "7d170ab1876a4aea89a8fc230a58847a",
      "75d7436798a84b37b73926788318925d",
      "4bc702b37fb64741bc43e63e8992f1eb",
      "6eb380d5e1b543d08f3b5acee1e911ab",
      "0351fb2dca7943f3b25d8e13a0e1d772",
      "9f36120e0b39454397761eab447fb156",
      "0817c54ed39445e6b26b78e44134d7a0",
      "b210b786cc234076a4fd91e4798b45d5",
      "1d756ef02e0c4608a76be50e440bac6c",
      "6bbc181c6b4c4f00b6e79ccc0cca26cd",
      "efebffd9f9e04260a4795524100cd9ac",
      "486ec06639a74117911b5e3ac6984e11",
      "6fccd22deda64027912c330bb18c169e",
      "00968dc3373d4364b28a19c38acdad82",
      "019e3238c12549a8b0979915244213bd",
      "9b5fd2d76424411e8cad86d3d667bedb",
      "6e4c8f0945474074b9f30d35dde28834",
      "9fbd2d40388e495c8c7e539f3932d637",
      "cdb89d38c20f44cf82a3f2efd019fd54",
      "6039fb9d97f84f5a955ec7604fe98639",
      "554c7f6c8e1941419a051ca44da93dbf",
      "f4f7057d3e1748a2a91d0f9a077e8cda",
      "a624cb4fcffa46649a070315d60395b4",
      "d0774b5d87f04ed8a43042f395c640b4",
      "6dd5e7474165448b9c56f936139ef06e",
      "0c8d3875173e4c75b8a16a819de218de",
      "7bafe0b28331483285ca82d6aa37042e",
      "0f82cfba0d6e42a58a75b53d93eba11d",
      "07618100311340c4b5b33a5c0ad6a3b8",
      "3933517c4d8c4e029f31c81e5adb587b",
      "9c1e89bd85794c8fae1d4291d465bbec",
      "3db56e42b02640dfa93a157ae4a18d46",
      "9a9f1dc8abf8436b9b5947b1a334e089",
      "d2bb344c93e54c7499b4fa4af145f038",
      "52076655ee6f4448b309125cf71da7a0",
      "84dd5fa4ca5a40c8b2b4e63274102eb3",
      "59d05d965b0f4289a5610297ca226cfb",
      "bf0e200423a14286a50cee9b4a208a71",
      "d66aa94f54ea42ea9bfaf2f79d4fbbc6",
      "a780a4f7bc94402a9a63816e9c03b35a",
      "762e14dcb7b841b1b6bb3986a4ed2ff3",
      "7ec261dcdf48478f8eb9b1a781e52829",
      "065cdcb1fa2d4b7f9ad2b9be5163adbf",
      "721d64fdac024a6aa4af297f1ff3c186",
      "aa83bcf22ccc4e60a9150c8bd729ee31",
      "c24884467a1240dba5d3cf9b15b511c4",
      "80499ae6595b44ae9f8ceb850ac7c489",
      "75c3aad77e604e62ab7516127718e5fa",
      "b6b5308c4fb54d0988066a69d8bc7cff",
      "8b79cc14ba944a16bdc85969a5b78df3",
      "53bde53960ae407a80ee4abc12fcc6ad",
      "e3895aab130f40e4b48a5dcdf2902894",
      "2312551416aa48c0a0cc1e67e64257f0",
      "478785fe1b2e459495a10d5ec71769ee",
      "697b2e84e44f4913b5063f89f4587181",
      "fc371074ee104aa296d0790efe2e0551",
      "3091231a08d64844b7076ed5f6d99a7a",
      "4431a40ef8af4340b056a60f68fa73c7",
      "cded22b8f7aa483491853893acacf38e",
      "289d1f67ffe94211859701a371bb00fd",
      "5390577f82604ec4843d56ff2a03bcac",
      "2fbbec24e05c48bcbe85b830843f2e7d",
      "84ec78f1dc0047248b769775148d7178",
      "df4dd86d045e47bf802846bbd4a1a14f",
      "797f802c1e864d42b1a001dd26c42205",
      "3bf73f1943e948edbbd876adfaa87963",
      "19b03c8e84194e2793dd200969085a03",
      "d9c6fb09cb7047bab7ad4483500e6354",
      "5df1208dba6a4eaea6cfaf69c409b9a6",
      "3a412c7d0fcb49688fe9eb64583fb3f3",
      "cd300b8d30d3418ba22dc8afbc2797d6",
      "ee11b50817d94749bb30741990ca8987",
      "d1968cc63e414fb8a742e5c1cd161d5c",
      "a15b419e2b3d4083a88b8fa4bedbf077",
      "df31e358a03f4a0fbbcb4d2fba5d1647",
      "a666004282b74f8c94a019c28a884241",
      "f9882639bb20411a832e71a5351b4d4e",
      "0c4399b145b3470da0021b0774c8e910",
      "d27622aa227c4f4592328a1d455831ca",
      "6e2f8a1bf208403781a712cfe6734c1c",
      "cd2d37c3a21d4bfa89fcbb556484a7ac",
      "ffaefdb08aea408cb713fa723e9ab642",
      "f90728807f2f4a0ba0c14d612d163322",
      "5ae736fd8e2f4c2e99a7699478f16532",
      "022dd07b70b24bd689c8e8e7316215bb",
      "8e701266c35c4c42bc7e3e98d5e783c9",
      "0ced9f4ee2b14294838b6c5650ca2a97",
      "d8c76bcdd2654207851ae5d294aa7682",
      "e8171bbe970a4dc0a43835e78260a0e5",
      "62c2e5965203433189c18ae0e1c608dc",
      "8b5898e05f074ea2a164d7698974f505",
      "ab7bd37416c04ee8af8a8a1670cf2fd6",
      "d532b55d62184d9ca1867c1bd406108d",
      "0abfd658591b4b4f9fb155854db23b8e",
      "14575be66f374cb68e9fdf79e6fbe90c",
      "95cfd33d42d64f2f960a1789ba514e98",
      "37b709e2d8e34637858e6acb549cae9f",
      "5562f51e22d74e84bc65c60f062809b1",
      "f9845ac2b6bc41a7a47d51efd2b01f4e",
      "0e63073921f34c639327d6f0af67f947",
      "af22b57cdc234525884bcd3186f8d481",
      "72ec007c087248689c507c8921ed5668",
      "94116d5a9f354d0fa2b58c9ab7a5b337",
      "fb320959cc0d43798eaaffccbf3e9bf1",
      "b20f68b362a748c498445d36bd1f6790",
      "efa488dabee7484a9ebeb8895d8fc020"
     ]
    },
    "id": "xuDpGkbk4Lcz",
    "outputId": "e645c1da-ff21-40de-a934-9286fc0e014f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Data...\n",
      "Preprocessing...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13aa3c8a87ad4520812691d37baeeb7d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b210b786cc234076a4fd91e4798b45d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdb89d38c20f44cf82a3f2efd019fd54"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3933517c4d8c4e029f31c81e5adb587b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "762e14dcb7b841b1b6bb3986a4ed2ff3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e3895aab130f40e4b48a5dcdf2902894"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84ec78f1dc0047248b769775148d7178"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a15b419e2b3d4083a88b8fa4bedbf077"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "022dd07b70b24bd689c8e8e7316215bb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training with LR=9e-06, Warmup=0.2, Cosine...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1056' max='1056' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1056/1056 24:40, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.225367</td>\n",
       "      <td>0.199357</td>\n",
       "      <td>0.117574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.598595</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.373642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.539939</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.466785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.483558</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.478754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.490345</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.496041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.541500</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.513451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.575139</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.516627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.646242</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.568506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.702231</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.559208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.724672</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.579802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.717800</td>\n",
       "      <td>1.744096</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.094400</td>\n",
       "      <td>1.748069</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.5379\n",
      "\n",
      "Generating submission.csv...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "95cfd33d42d64f2f960a1789ba514e98"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR_HEAD = 1e-5\n",
    "LR_DECAY = 0.9\n",
    "EPOCHS = 10\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess_text)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "#Uses LLRD optimizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# impliments Layer-wise Learning Rate Decay\n",
    "def get_optimizer_grouped_parameters(model, lr_head, lr_decay):\n",
    "    opt_parameters = []\n",
    "    named_parameters = list(model.named_parameters())\n",
    "    classifier_params = ['classifier', 'pooler']\n",
    "\n",
    "    head_params = [p for n, p in named_parameters if any(nd in n for nd in classifier_params)]\n",
    "    opt_parameters.append({\"params\": head_params, \"lr\": lr_head})\n",
    "\n",
    "\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "\n",
    "    for layer_i in range(num_layers - 1, -1, -1):\n",
    "        layer_params = [p for n, p in named_parameters if f\"encoder.layer.{layer_i}.\" in n]\n",
    "        lr_layer = lr_head * (lr_decay ** (num_layers - layer_i))\n",
    "        opt_parameters.append({\"params\": layer_params, \"lr\": lr_layer})\n",
    "\n",
    "    embedding_params = [p for n, p in named_parameters if \"embeddings\" in n]\n",
    "    lr_embed = lr_head * (lr_decay ** (num_layers + 1))\n",
    "    opt_parameters.append({\"params\": embedding_params, \"lr\": lr_embed})\n",
    "\n",
    "    return opt_parameters\n",
    "\n",
    "# Manually create the optimizer with special groups\n",
    "optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, LR_HEAD, LR_DECAY)\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=LR_HEAD, weight_decay=0.01)\n",
    "\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # set learning_rate=0 here because since a custom optimizer is being used\n",
    "    learning_rate=0.0,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = ProTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    optimizers=(optimizer, None), # Pass the LLRD optimizer here\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(f\"Starting Training with LLRD (Head LR={LR_HEAD}, Decay={LR_DECAY})...\")\n",
    "trainer.train()\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\n FINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission.csv...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"index\": comp_test_ds[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })\n",
    "    out_df.to_csv(\"submission_pro.csv\", index=False)\n",
    "    files.download(\"submission_pro.csv\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997,
     "referenced_widgets": [
      "370e18ff4eac4ac0b3e0c1ec6651935e",
      "9185e76a0bf64170ae27cb27024a11a9",
      "6c78752c84b2476db951f30be80d37e1",
      "69d8ba1818e644a08b3e88f76567812d",
      "85f790aeb7074ad2ae7b72ca4e3a76c0",
      "7cd4d4e1cbf94daca439a9233d3ee5fb",
      "d3a7374fb1ef473cbd512b3267c531b1",
      "793420d1c755435688f00e51c5ae51cb",
      "c13aa0fed4df4ae4a78b5844884afbd2",
      "863b528e550047f5b4b595ff4d20b7df",
      "d1e5e5c7aa344080a6cacb85db464ddc",
      "b614d64107c943e09c6eef44dee4308d",
      "e55a0b5141c9453cbb904c4f897156cc",
      "a38db86816e94d209e550bdd6d760113",
      "9b0ca6dc879b4e1e93e873c2318a4c9b",
      "20b48b7d2c0d4872b2701bbc88dbf29a",
      "e790e6a690df4c69b88e3778432f9750",
      "b768329b74484c1a80d65f61cddb7d2c",
      "9e0faf626f404bd58cab3f3bbda7908c",
      "7742ba63c21a4d6fbebf97ca99f81b1a",
      "b83b1d93285948ec918291e8d260b40b",
      "66f956a6497743b392be36c07d0a7080",
      "6234e7d980784df9aef6d19e78bfc40e",
      "128e5750e1d6468f88dda9f09b2e6a57",
      "187817496096465d8a8eb18f91fc1e02",
      "798e018dd6424540908ca4462f326a5d",
      "bab6adab7cf74380aaf3e1101277a0f1",
      "5aa0a6a9a40b4d8783602119d0dc1896",
      "0d2abc79ba044dcf954175e5bc6da18a",
      "0ef1564427974d98897b9cf69eabdd2a",
      "6bee5728c8f64efa94099e3b8d9f3f95",
      "c15ff0e4a7ce4f4ea3ebe6e8f22f5a6f",
      "3d55fbe1431a4664b4db4abb04e89312",
      "cd660f4069d94f638c13af99849d2d1c",
      "26e5c2e463d74a7388ade3eed513000e",
      "f64f8cb0f4a4461eb92156daaaa83a19",
      "9b0bca0651b94091923ebf0ad1d184d8",
      "de2a4645f49541a0a48b24fba9abf5f2",
      "998310529f814304bab1172707623e0c",
      "d67ca37f94654807a1e834d5da401c68",
      "2b54af7bc0664d3fa072793238f2a172",
      "8d7119b76329446dbd6c2c5f6e51552d",
      "bf1b203b7d62461e98a1f8ee7137f06b",
      "61305684f18947878b84549f9fc64254",
      "e12b7887ce764e78b14f41ad7f7ce26f",
      "414ec502e7554095b64ff5c1d53f3265",
      "83523bf0dba945bd93832abfab13f691",
      "3d71094a7b2a46f2ad3aa3975bed0cd3",
      "bca74ae10e4a48609197bb387af54805",
      "b5df6dcbbf9942aeaad94843345d4bcc",
      "e12339ce8e274ec1aba6a284dd8a5597",
      "e02f003914154959906b6f05ef63b09f",
      "ba36658506fa4d7cad1e28501abcc813",
      "d7c4a0d30f2644da8ec751fce5611884",
      "a1c4a9a491a24f41af1158e0119304aa",
      "a1a95780075e4d14b5d769bae54a9b1d",
      "712ed501c90c43c09b4ec76547606331",
      "d55ab586a171499eb3d8dee1ab26b8ec",
      "6a58a8bb3f504d66a10b7a6cd6551cd2",
      "3ad4a60157cd48fc9726241c53e7e953",
      "0ccd1b6cd7344aa7882c04b98e50509f",
      "47ac9e83e0f7412e8a22e3d0e48aa60d",
      "b64651006cc74d1e9b2dece7abf179fc",
      "825934f6bfa8440e9cb3ab0ae5d4a08c",
      "32bc4bf76d2244f9ba81753a3dee6a73",
      "7870f451c3e84eef911dfcf0920f7ae5",
      "07b44ad18a8d4a25bde9d5e3769eee53",
      "6d1c804846f349adad846cfb28d20ba4",
      "7d4eed5caf164c40b46b16401335f057",
      "fb5220f87270497fbdec841ab2caddeb",
      "ae951380a43c433784b238aa3cefdaa7",
      "b90ae216b9ae42b986b5ec826fc87af9",
      "bed047b26bde4280b41a54397007fb5b",
      "027c41bde9db4e0ea7253b1e4de25a1c",
      "8867f42dbd194885b3de962976a24380",
      "17bd21221af647c6aaf34fa87181a0b0",
      "6059f8077fdc4bcba3eef4b51dba9624",
      "5a9859efb9844d0cbf92df07be947005",
      "d9ea90a89dec4be18b45996f5e7d2840",
      "a044550ce2c6417597b88f5a310c31d9",
      "b9e17be17cc6410c9430439b38ab6a93",
      "1a04fe744c6e4cec8909a95a52530ef2",
      "e8ca0c8c3e0b45628176bb9386ea8a29",
      "a5a5b05d0c35426b9523defb098da4c7",
      "0a9dbbffbd26402ca88e15ad3a814886",
      "7c1508365a7f4cf9abe5166416a6f954",
      "5a3c4fd149404cc2abdba68b41d9637d",
      "c1120c637a8f49f980a80973e5d722a5",
      "7733dd7579324a17b47266bba8da493a",
      "f8e3bff48591474da393d72f31b10baa",
      "7509b9ef8fc7432d943d803f77955dc9",
      "069740ad141b4a0badc3539d8c8fca29",
      "1dbe43a2e05a47ebb1f383579aa40b8a",
      "dec581d7eefc4134951c320de9a9c443",
      "32c96b57964a4e73b2aeb41f72372613",
      "f4b59dd533ce4c6b949e30341565cdf9",
      "9a33eeae832243b7844b540ce20d5a76",
      "1e898f094fff44b8a5ae2b0ed08657b4",
      "1fbc59eaee9643bfa86217c36b5194a9",
      "0f17a90347d24a04bae895f5e3a9ae8b",
      "40ed64e4420e44d6ab91c72b1e47e6aa",
      "7c5bc3d265ce41febbd8a84c88074d83",
      "f9f40172a5074344a8160e308c50340f",
      "18a1ad7a16f64e7ebb1de3fcaab3629f",
      "db239c933bfa432db70fce7e7f5bb7af",
      "30358af4f69b4387a2a9a83b36dc7396",
      "a11e1dd2c9c748fd95cf800b7e430ee8",
      "eceff2a3f0e8414888147a8b3f3d48b1",
      "3a9f188df4bf4615a01b31bc068955a0",
      "96924d076763466d94b0dd3808434565"
     ]
    },
    "id": "EKkp_bJ3_bOz",
    "outputId": "cb76e3f5-f3a3-44e9-f140-e23dac48e4cc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Data...\n",
      "Preprocessing...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "370e18ff4eac4ac0b3e0c1ec6651935e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b614d64107c943e09c6eef44dee4308d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6234e7d980784df9aef6d19e78bfc40e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd660f4069d94f638c13af99849d2d1c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e12b7887ce764e78b14f41ad7f7ce26f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1a95780075e4d14b5d769bae54a9b1d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07b44ad18a8d4a25bde9d5e3769eee53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a9859efb9844d0cbf92df07be947005"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7733dd7579324a17b47266bba8da493a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training with LLRD (Head LR=1e-05, Decay=0.9)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 20:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.330458</td>\n",
       "      <td>0.453376</td>\n",
       "      <td>0.291736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.906703</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.330486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.170600</td>\n",
       "      <td>1.867629</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.406208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.170600</td>\n",
       "      <td>1.823107</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.442891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.170600</td>\n",
       "      <td>1.804360</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.480734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.777420</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.518958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.755295</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.529833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.759881</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.543755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.761659</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.549634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.643700</td>\n",
       "      <td>1.763380</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.555846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.5395\n",
      "\n",
      "Generating submission.csv...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f17a90347d24a04bae895f5e3a9ae8b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_29469500-5a3a-46df-abd8-ce72b30809ac\", \"submission_pro.csv\", 4166)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is hte best run I had for seeing how to fine tune the model"
   ],
   "metadata": {
    "id": "tgA5QCMdzHLn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 777\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Configurations\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"Fetching and prepping dataset...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "# Formatting each example\n",
    "def build_text(example):\n",
    "    clarity = example.get('clarity_label') or 'Unknown'\n",
    "    return {\n",
    "        \"text\": f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\",\n",
    "        \"evasion_label\": example[\"evasion_label\"]\n",
    "    }\n",
    "\n",
    "processed_data = dataset[\"train\"].map(build_text)\n",
    "if \"test\" in dataset:\n",
    "    test_dataset = dataset[\"test\"].map(build_text)\n",
    "\n",
    "processed_data = processed_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# Splits\n",
    "split_1 = processed_data.train_test_split(test_size=0.1, seed=SEED, stratify_by_column=\"evasion_label\")\n",
    "train_dev = split_1[\"train\"]\n",
    "test_holdout = split_1[\"test\"]\n",
    "\n",
    "split_2 = train_dev.train_test_split(test_size=0.1, seed=SEED, stratify_by_column=\"evasion_label\")\n",
    "train_set = split_2[\"train\"]\n",
    "val_set = split_2[\"test\"]\n",
    "\n",
    "# Get label mappings\n",
    "label_names = train_set.features[\"evasion_label\"].names\n",
    "label2id = {label: idx for idx, label in enumerate(label_names)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_set = train_set.map(tokenize, batched=True)\n",
    "val_set = val_set.map(tokenize, batched=True)\n",
    "test_holdout = test_holdout.map(tokenize, batched=True)\n",
    "\n",
    "train_set = train_set.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "val_set = val_set.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "test_holdout = test_holdout.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "# Compute class weights\n",
    "y = train_set[\"evasion_label\"]\n",
    "weights = compute_class_weight(\"balanced\", classes=np.unique(y), y=y)\n",
    "weight_tensor = torch.tensor(weights, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Robust logits extraction\n",
    "        if hasattr(outputs, \"logits\"):\n",
    "            logits = outputs.logits\n",
    "        else:\n",
    "            logits = outputs[1]\n",
    "\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=weight_tensor, label_smoothing=0.1)\n",
    "        loss = loss_fn(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Evaluation metrics\n",
    "def metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n",
    "    }\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Training config\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    compute_metrics=metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Final test evaluation\n",
    "print(\"\\n======================\")\n",
    "print(\"Evaluating on test set\")\n",
    "print(\"======================\")\n",
    "final_scores = trainer.evaluate(test_holdout)\n",
    "print(f\"Test Set Macro F1: {final_scores['eval_macro_f1']:.4f}\")\n",
    "\n",
    "# Optional: prepare submission\n",
    "if \"test\" in dataset:\n",
    "    print(\"Generating predictions for competition test set...\")\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    if \"index\" not in test_dataset.column_names:\n",
    "        test_dataset = test_dataset.add_column(\"index\", list(range(len(test_dataset))))\n",
    "\n",
    "    pred_output = trainer.predict(test_dataset)\n",
    "    pred_ids = np.argmax(pred_output.predictions, axis=-1)\n",
    "    pred_labels = [id2label[idx] for idx in pred_ids]\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"index\": test_dataset[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(\"submission_pro.csv\", index=False)\n",
    "    files.download(\"submission_pro.csv\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "537eb9d185714047b9d12f1081848881",
      "c5297ca262d34039b089b505dcc88d74",
      "6004c4fe1269493095eafb2c75081195",
      "99643d118d6849068beadc42276eccea",
      "3e03e8e44b9d4afa8332e17277651158",
      "b563035394dc4d5389dbfc31c36aa637",
      "204540cea74c43228213f341cbf7791e",
      "091f58a4d7a64083be7918815c013882",
      "ffa51d0c5b12419384e0cd3fabcac379",
      "bdb8f79c0f0a4e4fabfb8e2236163787",
      "4e09dc8078b0455d9c761c3fdab4f244",
      "26e58b2a96c74eb88f116d727c0f14a1",
      "d772f8f14c6d43918e4324c2219dabcc",
      "1c646898f7734218ab4f7f1fea920216",
      "8e6debad04bb459497f9dcf3a4068889",
      "c2b40f68e7434a41b86f810d9f86d1bc",
      "886b7ff5a6f84dd1aef479417561f98d",
      "b23dc473cb11418f9cce468656e6e604",
      "8e1226c5f8e24cc4a7e75be829f67aaa",
      "0b1c8064f42e417c84fc993f1d3835ba",
      "dc7f0a2ebe5a4f89b4fe2f9ea92f33fd",
      "ba3b528ff69b4c34a9dea77de375da8f",
      "2356de8b6f6441888c60d3697ab07c63",
      "55f657550a6740d69c1ab90adc63f8d3",
      "276373dacd7b4347bd804c5cd04dcd38",
      "cf86543e13f64f18b477e9b94ec75630",
      "38c11a8eedf44e9a98b3f2715048b969",
      "486fe42a32724a0f85efd7f6660fc43d",
      "92af57b3a0bf440283929bc1eb266fb1",
      "73f1a4df9ee64c0184651172d61eec43",
      "77ca0ac8a6e84e6cbe89d9ebd3600049",
      "8253ad790c714935b75970a052ce3798",
      "b91b1c892041434295d7b9a2796f9a21",
      "05c1f62346b440909f37c5966ef3e69b",
      "099602cebc294e22aeb9994f917141f4",
      "6c0725a63ff64bbc8c3e70c0905729b4",
      "0ab1d1fe45f7437f832d74135050e1ae",
      "99cf3cb5ac2e4b7c8a27835a2a6ee7b6",
      "aa57bca977be485fa29a19b975924522",
      "e33d2f956ae34b5382078ae9f4844074",
      "d269768da258430d822ffdf4eeeab2ae",
      "046bcfd33af64df1b60e188e329485b4",
      "0b720217d3b34c5b9659bc42ee68025e",
      "bfa23c6003344dd8b6d50e658a98aba0",
      "c44b74718d0d44448d403786ee585204",
      "49b72c5a2bd14b01a7a3d942ba4c3577",
      "dc5ee4d459e74b9d9be119f36ec638d2",
      "e750f540490d4e28a15e5ee097adbae6",
      "77dd7263c86e4ebf8ddcb15f6d6266ca",
      "05b6920cbd164e36857655850321f85e",
      "2c024a9da71140d183f63478993b83a3",
      "6dcb8691e47e4483bd93e3828f0d1017",
      "50f46b49ae1d433a8d594cb8ca0f63ae",
      "bb5ceb67f4344de6ba75971fd56768eb",
      "08721edc585a40b5a5704a4cb3f4407e",
      "6b87bf720e2c437c8e1129ec3e5a9258",
      "47e9c94bb8204703a3da30e3184bfc52",
      "55de7b1969654dc5bcd05a0c6ac8dbb0",
      "edb457c79e974d949f214c548fdacedd",
      "9bbfc763870741f29eda7b251b53d764",
      "0ca702c7984647d9972fae7cf28e974b",
      "0a154f2c7f8d43e4985040ff9d393275",
      "ac6d5264ae214994882c29f9263367f0",
      "5b4b616e130d4138882e479cf12d2d08",
      "519f0289bf8e4868ac3774944c8c545b",
      "de8896a790724c1ab942749e9c28159f",
      "38ee863622f74845b5b6aceb51fe9058",
      "91782da48370478aa1b7b779e672fdd2",
      "72aa52bc46524dcfacc760f0037701fe",
      "e3c7b7bef0b444c79f4ca490ca7da7b7",
      "ad368ea6a6204e21ae798efb4a804ef0",
      "240b7a0525da4730ae488e6ed8b60628",
      "f2cd65b672d947c8a85086ab6d620c9e",
      "2658dd03c5f245788fe2cee728f7132a",
      "884e763c17a54822bf3b3351b4419736",
      "7428ab1b66834592813a46d93f9be6f4",
      "059f9420356a4d63bc213800ca0b21c3",
      "c4d39c3864bb4d5f973f5d0f5d2ec0b0",
      "ce8d5ddb672c4fa0974fa87531fd4d66",
      "dfebc8be954744ada71e8fac0396ea4c",
      "2374ac6d76c642d694b2b927197da37d",
      "1f91287fa8f94fa4b2ef6af7be53d004",
      "25f4571b285841da96306d31bb9f709a",
      "5a36ec685eb748a5ab6a146d40c0ba52",
      "12eb1fea0c6f4b3ab711d148eb94a498",
      "23a32c5925364093b5eb198c56b9b34a",
      "a223022ae8e7430ba9a46cdaae21854b",
      "3edcb9031b5c431f8e45c5a052b21329",
      "699b28f5e79842718e9e51dc88cfc5a9",
      "472d810e51ef4fddb07b8762ac1ac0e3",
      "a55f408c14bc4371b7e6a4c5cc107c56",
      "5f4ee291f8034743987019150e2b17f6",
      "f3f364502e9b4227960db5256519823e",
      "234defffd2864e0d96ad9140fcb0240b",
      "43a9fcfcfd8d4da2921ce9e51369508f",
      "728875fcbabd4c0bb82ff1a7fecddd7d",
      "cf8098cfa67a438a87c01f6caac1fe0a",
      "efce050ab240493d8ad9e0d43068370f",
      "b5346679d35c4967829e6d84a7b9bd8c",
      "68764539b8cd4b3ba548b3fb1154d268",
      "77f1897a310449469ea7021f1fd1b1f8",
      "20bc0ef5ae444834884dd1eeee7e993e",
      "a30a1cfc6f0442e38f9fc512ba6b75ad",
      "aa04c3f4f02a45e4a1a1c7e0cde02f74",
      "6f702c6a4d044125a6a210b8551a7fec",
      "331504f80791438fb45bb6b23d12734b",
      "eddb00b6501e42deb34b3ce212f745d9",
      "2e68a6c6f55241cd9376309dd5736afa",
      "3863a8ce14184d018138e01174d68588",
      "c4823cf224cc4240a86e4aa1074a8d3e"
     ]
    },
    "id": "mFzj1tWD_cXE",
    "outputId": "7847f8eb-3443-48a2-b12e-8524f31f35e8"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fetching and prepping dataset...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "537eb9d185714047b9d12f1081848881"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26e58b2a96c74eb88f116d727c0f14a1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2356de8b6f6441888c60d3697ab07c63"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05c1f62346b440909f37c5966ef3e69b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c44b74718d0d44448d403786ee585204"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b87bf720e2c437c8e1129ec3e5a9258"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38ee863622f74845b5b6aceb51fe9058"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4d39c3864bb4d5f973f5d0f5d2ec0b0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "699b28f5e79842718e9e51dc88cfc5a9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training begins... fingers crossed \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2450' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2450/2625 29:22 < 02:06, 1.39 it/s, Epoch 14/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.932442</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.310894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.820891</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.368565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.079100</td>\n",
       "      <td>1.750850</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.513794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.079100</td>\n",
       "      <td>1.747446</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.514343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.079100</td>\n",
       "      <td>1.706125</td>\n",
       "      <td>0.672026</td>\n",
       "      <td>0.579710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.689300</td>\n",
       "      <td>1.786313</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.570905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.689300</td>\n",
       "      <td>1.794326</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.630692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.689300</td>\n",
       "      <td>1.814512</td>\n",
       "      <td>0.675241</td>\n",
       "      <td>0.641820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.378800</td>\n",
       "      <td>1.941438</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.601879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.378800</td>\n",
       "      <td>1.979787</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.630296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.378800</td>\n",
       "      <td>2.026153</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>0.625054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.131300</td>\n",
       "      <td>2.087116</td>\n",
       "      <td>0.672026</td>\n",
       "      <td>0.634081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.131300</td>\n",
       "      <td>2.101293</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.632863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.131300</td>\n",
       "      <td>2.105971</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.622406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "======================\n",
      "Evaluating on test set\n",
      "======================\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test Set Macro F1: 0.6166\n",
      "Generating predictions for competition test set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68764539b8cd4b3ba548b3fb1154d268"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_276f9ce0-937d-41ac-a9ae-347bb35ad28d\", \"submission_pro.csv\", 4239)"
      ]
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "zExMkwB-kAsc"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}