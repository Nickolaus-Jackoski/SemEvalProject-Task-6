{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6841ef171f8245d78665afd938a45d34",
      "ff98dd5a31804491be710f2c4fa2f5c0",
      "2ba1ff728c53468f884b7cbc5867c980",
      "6b0e5a287cc64bb08ea9fa7954f46444",
      "94d34268ebfb4a4bae86a0e36fb965e1",
      "e17ad486be054976b9224fc42ca2663b",
      "1ce99e2dd2be4b928a2ae11eebb37701",
      "d7d7573fd7db4f8bb1e4b2ebdc7b6fde",
      "37367f14c3474f7a9dd127d1f06dc588",
      "f65026c68ff440da9911499aeeb3312e",
      "7a14cea6cdd040bb9a9ac551b21c6e2f",
      "4ddd8f7455d742deaf4d498d5eab7f27",
      "402ea371248f42c19a366fc203c9d1b8",
      "6d8e0e84e2ef4d14b07b4078d4de9649",
      "39099aeb77f64307acc89de970325d45",
      "af1b892db3924868adb58640212c4e8a",
      "d077e2f8ad434cd788e5002c32f8a392",
      "03be7a1ac06e4d939725c69edd1bb043",
      "51b46d9ee8564434bea973f841159c23",
      "1e1a1f4752a340f28527cd616135a28a",
      "afb08a6be9b54cdebb6e0640d224938f",
      "41f6ba59a6ab4cf4a7c3862bf3b0ffa6",
      "a7bb822e737c49bc8a8dfa556fadd634",
      "83beb4d95a214e9bb90a9fcaead6fe22",
      "52df81d80dd5461eba1e101f75a7a924",
      "bc07b95e07414316a4bb46a2465a5d5a",
      "6275a85a38564f378b08b720a6d6d096",
      "d646af2b313340829ca46615c9d9adf8",
      "cb6b4ab06f354e52a04932c8692f4fdc",
      "cd7d4ab44a4f4dfaa43e913111bcc7db",
      "56d1067850764fc893acd327862c00c0",
      "2a3ab565bb5e4e259517867d8fb0ce14",
      "8971cfb1dd24431181e794cd7c13531e",
      "d82fa8dda17740e6897f06f73e163c03",
      "487ba147f39542d4b9b9344a49bead97",
      "541decb4f02142538a4ab2efa1ecc773",
      "974e428c9c844a1a8850e4ca6c3b33cd",
      "01710359f62a47f8a4c233dde5edfef9",
      "59b0b88fcd4545f7896d3de126fe3963",
      "011eba4872b7452db4d5043e803f68e2",
      "af1ed04fd3b44468a3e8c399422240d5",
      "f8a9ae16c9894ea0bc5e1bf1f707526a",
      "2da9187a1f5b4d9cacfef41de5f7865b",
      "19c70b01dca147d3904c7c96e212b9ed",
      "dd40d1d6611e4f639a74dd823d68655a",
      "b83d511bbb7e4dc4acc11afd26777f2b",
      "f8659e7e973646dcb4b6217a2b9c3692",
      "c770bba423a04d28abdfb5d27416cc64",
      "9ad3a8599d9c4ad5ad23d76d8d1b8c49",
      "d6bd51c33edf483682d70ba7f3275ff7",
      "fcd23e162a464201b11787d9f34d1019",
      "bb7c0c93256a4b5984c8a2a43b3f307f",
      "9a2eb94f1ff24612841e8111bff96fcd",
      "c341610f1fd04c6087da9c45d405b86b",
      "bdc0c76b54ee466997d96982cdb19bfd",
      "e63bb5dece534a0fafc430d1b0ea4d4a",
      "4c22f5bb386f473fb06b3dd85d40b269",
      "e8a8e71702f849d8b45436aba5072435",
      "0cadc594ab844db8aa1f4423353af8fc",
      "db7755e4510c4bc18073c5084b8a4a13",
      "a56b2e86659c4901b64239782a3c47e5",
      "2dca1811fb0a433a9697fa1a7728de45",
      "eb7f61cfd80440098ac27f6b53e1bde4",
      "55f20adee62d4aa4845022268044f791",
      "048ac4137ba946d2a6acce249e79cbad",
      "361ca0e1295b4e42a76ae256fc8b912a",
      "94d00e2b5b3c42c2b850be93adc83853",
      "3814242e133b4da494ec3a1ed65efcfb",
      "d19bde76d00044c499ae9daca2b46fa5",
      "7286bb71ad01470c9a4a2274897c3c69",
      "015c1fd93c554f52b243b0e61014add4",
      "8fed944709754b449c7aa3edded79a7a",
      "cb2d2db2d5af4299b58aee5811b041db",
      "c68ea37a3abd4796a89a76697d76548b",
      "f8bb0c091ed447a29dcf63528a4d6b69",
      "013bc23cb21d4d599208bd0f8bee469e",
      "ec310c50f4534333a418e9fe53f6ff12",
      "34eab48ca5ce4c28b2a502eec24714ea",
      "5c7a2ec5a9c14ccab7400206110c0b40",
      "044e4091ba574c2e97ee8b400d143316",
      "bbaa859c82654000888de9516abf6b6c",
      "e089c1ba3db04779bab7f28f132c0f59",
      "03b9b0b91b3542588d0c3c03b1004fe5",
      "5089425dadb74d11ba4ca1906965b5aa",
      "b063f4237ca54ff38d63cd6773b0bc63",
      "b148f5d0a28f4bd18b3b3b8260ab7782",
      "724741f164bb4caeae3ddb00e0ff695e",
      "ca7d9afe5a9c47fa9796edf2795fc65a",
      "3bdfcec7c46f4d9e8566550b10f80960",
      "4cfce658fcb643af89cf28eeffdd3ed7",
      "5b9f3a1768d94506a938662db822e2cf",
      "939c46be1b794989a7be38305deca563",
      "1abf2af2cbb641cb9274cd6e3e64ac39",
      "1993b5ca82c44811a0aa1bde15bf95a0",
      "d57f8fec60af438da1e0788bad67dbd0",
      "ce870b96bdd14a7d9e4fc8d80f4485b2",
      "86a324f8a2244b6cba90c2730290ad22",
      "d0c498729e0e462f863fc6dea9a98ccb",
      "ae53a7d974fa4ada97ca18bedb62f1a7",
      "03fd7bc1cfa74e999eb10069413606aa",
      "e919ee73f9964e448d5e99e2a587b6b6",
      "4464527a2306477cac6f36b8e40c241e",
      "642dfbb3f2ca4be4b981347ced29e621",
      "2dc46f11cab84c448be187652cfcef83",
      "f11e8b4c84504f28b33e844fd4dad5bc",
      "c8ffd47337d34a66ba3aebc64bede7e4",
      "3a6830f057424000a6843e02ba48ae79",
      "fce352d5ffa142e2844b8f77f65c1abc",
      "ed25723336504c199720b21a17005819",
      "dc4f7a719df4418d80591bdc0aa3713a"
     ]
    },
    "id": "Y6nDKI61vG9D",
    "outputId": "0f7b1719-1158-4659-90e5-8b4e703f3873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Preprocessing and Injecting Context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6841ef171f8245d78665afd938a45d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ddd8f7455d742deaf4d498d5eab7f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bb822e737c49bc8a8dfa556fadd634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split -> Train: 2792 | Eval: 311 | Held-Out Test: 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82fa8dda17740e6897f06f73e163c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd40d1d6611e4f639a74dd823d68655a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63bb5dece534a0fafc430d1b0ea4d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d00e2b5b3c42c2b850be93adc83853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34eab48ca5ce4c28b2a502eec24714ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdfcec7c46f4d9e8566550b10f80960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeBERTa Training (Supercharged Mode)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1925' max='4375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1925/4375 24:00 < 30:35, 1.33 it/s, Epoch 11/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.268523</td>\n",
       "      <td>0.530547</td>\n",
       "      <td>0.354149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.216714</td>\n",
       "      <td>0.575563</td>\n",
       "      <td>0.369107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.313900</td>\n",
       "      <td>1.083423</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.460134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.313900</td>\n",
       "      <td>1.114185</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.479814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.313900</td>\n",
       "      <td>1.124381</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.518609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>1.143923</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.564933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>1.161873</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.580201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>1.385697</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.600877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>1.539355</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.553185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>1.739431</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.600167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.494200</td>\n",
       "      <td>2.079266</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.594130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.5858\n",
      " FINAL TEST SET ACCURACY: 0.6348\n",
      "\n",
      "Generating submission.csv for competition...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fd7bc1cfa74e999eb10069413606aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloading submission.csv...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_977180dc-c09f-4c62-96b4-5267f9e755ec\", \"submission.csv\", 4118)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_results\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 1e-5\n",
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None: clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "# map Preprocessing to all data\n",
    "print(\"Preprocessing and Injecting Context...\")\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess_text)\n",
    "\n",
    "# encode Labels (converts strs to ints)\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# Creates the \"Held-Out\" Test Set (10%)\n",
    "split1 = full_data.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"evasion_label\")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"evasion_label\")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "print(f\"Data Split -> Train: {len(train_ds)} | Eval: {len(eval_ds)} | Held-Out Test: {len(held_out_test_ds)}\")\n",
    "\n",
    "# Get the label names from the features\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# maps labels to 'labels' column\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "    }\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",               # optimize F1 directly\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)] # early stopping\n",
    ")\n",
    "\n",
    "print(\"Starting DeBERTa Training (Supercharged Mode)...\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# evaluate on the held-out set (Simulating the competition)\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\n FINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "print(f\" FINAL TEST SET ACCURACY: {test_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "# generate Prediction File\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission.csv for competition...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    # predict\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    # save\n",
    "    out_df = pd.DataFrame({\"index\": comp_test_ds[\"index\"], \"evasion_label\": pred_labels})\n",
    "    out_df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\" Downloading submission.csv...\")\n",
    "    files.download(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "91a2a70569584e2b98793c60d8ec0d7e",
      "c9969d8a88c8431983163cc4dce92f17",
      "839933b0fb7840f49c537be49cfee38c",
      "3a4ff62a695f455199524af7a9b9101f",
      "eabbab86b1eb41b98e91b0a1ab7d5c1b",
      "ad244c0a9c114cd8875d9abb57c1bfd5",
      "b4fd16aaef8d4cc5924607094664a089",
      "fc04048c772243629f80261a7321ddb0",
      "eda9eaf3addf4b0fa450912264bdd4da",
      "aec1efa7389c43c59f2233d41ac1042f",
      "dd66aee370af46b8ba4f6025529f4b6f",
      "c4163fd03b474b9b9a424439f82e90cd",
      "922ac848f4bd42609c53b5555bf23c95",
      "be4d25357acc4c75bc9bb57fd3762571",
      "0faecfa014b843af8638ce3d8fad0399",
      "3a9eac2f10bc4f2d8063140990fd88d2",
      "743225d7d457424287a6cf0c872d1c62",
      "5939296db62f4ae2b324fd374f1cf8ba",
      "7a436b4785ad476aab0d77ce0f11be20",
      "bc56fdf644c54490abb52173755c5bd3",
      "ca1d729a92ef4d2ab4c73c2b66972ace",
      "e79dd01e2bb14cd1bc2bf5b3eb572e4d",
      "a2d0794f2dc244e9889aad909e707923",
      "4ef955babfb14e369dd5ba07f167212c",
      "ed2bb34209ec461e8c54f3d91d96c425",
      "837df85b7b0348349226f81d7796f08f",
      "919d684c5f2844e895791fb407ef2631",
      "362fad34740c4bca98369dd2b2c43581",
      "d7b6a147cfbf4d5d9d6f99bbc7502963",
      "8bac95ee494849ef8843772d2560b893",
      "689893566ecd4f1881110ba69f316ca8",
      "f7eab7097b3a47a5ae323eebcb62960d",
      "c36c4dff8edc463488a979e597bb4978",
      "4065504afae14da9ae54802a1f3f147d",
      "9d6a5314e4fe4e3f96d8448498523e4c",
      "7ae9e05b1f6242eca631c62ea525b4fd",
      "1e3ef14e8ede44aa8f70c984f165386a",
      "27ecd421b75f473c82d9ecf198d3884f",
      "37aa372a57cb48d0997ea2428042b76a",
      "8233c532b9804d1891b0b4c7d3e6a120",
      "6a0e97e4cd3e45efb8284d1fe433928c",
      "cf3d9203c842429896cc2862a3b67777",
      "506bda09484f43358f631c07c7ecca78",
      "b82b15ba33e845b4b1f9854c94d10c2e",
      "c9e24bb084a14842b2536c5d9066c226",
      "8bb3d69a688246c3adecdf39f7ac7e5f",
      "6e4e10c9a81f417dba310fe4679ba483",
      "83a1c0e27dce441a8b96e0c3a359809b",
      "fafe3ead58b246e09fffb26d0e87a235",
      "4f97a498a3334b8e9e66ef40dcb644db",
      "11e7f730def040d6941e1b8d22b65940",
      "1cc660f1973c476dbca0e029e7b32b46",
      "d631d31e1dbb46d6905e9b837e549e8e",
      "1043f404c6214d498c7d891e1e6fd79a",
      "6087e242c9234b86b60a5bb7febaccd6",
      "479b5be973bc4b7fa64db04925840084",
      "204bea478a574da0aed7e34fd796d505",
      "d76a721e5ab14e27af12019aec7713a5",
      "b930d2d869904549aaa24069d7663c9d",
      "1dd5499682c44c708f87c8ac9a5cbc82",
      "e2907c4fa2614c0e83ebf48f7d05d49c",
      "57ba3e3ac221448ba1ab8099072fb27e",
      "76732b5e503945338fc4c84b5fb4cbca",
      "061177c83e0845dca835ea3fc07e1585",
      "42b1265abb93420c98e2861dd2ba3f6d",
      "2594e19db54c4b6bb62afed215118440",
      "ed407f0637104485965adf3f2b6b2fc4",
      "938544c7e8e04793974a45b7412867e2",
      "fb137ee393754cedb128832ef84bb870",
      "d7c41a459e6744a194ee60402ebc6c5d",
      "d10c913e7c9247fe8cb83f28adbb7a96",
      "a5014f58cc7a47b1a27dbb7ef2b2e75f",
      "e9c97a3b6059450daf8c6b32ccdacbe7",
      "a24310cb31b842d998dbb958e5d2ac9f",
      "bebf45c32f4949818c55e1f3c262730a",
      "7e19fc0e2b6e4cbc88f1f9b0e4684103",
      "aa9ff46daacf4a9397445a60626edbae",
      "79b0a3841d96420e9485b1c3144ca87c",
      "056e3ec6d4894dbca65a307cc7bd956e",
      "d78f9c40b1ce497090ab89870f7635df",
      "4b325e7d3c68441a8aed49304d9a1581",
      "97b771101a234c34b69349d331050cd4",
      "8ef669933fb64fecb1a82fbb844eb960",
      "25f5571df28d4b05b83ba2f926085412",
      "c078b1880ead47e982fb94b279050811",
      "fc852e1d900f4b4581933b6bea04a334",
      "784f8ac2094e4958af2571b7736e1024",
      "02dc09b1101e49b095a9aca1e1de0eec",
      "941a582dd4004dca89705d634ee6d7a8",
      "0bc661e7537f4c0db38fd277e1bf63a7",
      "902636b513b04a07a8e726e690d058e6",
      "00d4864c0e05416e8fb10d2ae5892ccd",
      "7765003183af4c53b41adc7236b2a742",
      "276f77f420b346aeb895cc3a45e5114c",
      "867ae3e641c144cc98a92c944cd26604",
      "f2f3dfb0beb04bdc943208fddbbde97e",
      "b4d8ce5cb6144f16b2c7d709aba05f43",
      "0b7b9e65aace44dc9f5eefa6008c1909",
      "c2ecbaf572d744e1aa87db0f429d4fdc",
      "24153e081067478a811683dd7dd5fe9a",
      "0491dcb5309c4d9c9592637a6c789336",
      "7160605ed3c84c9d8956d844fb979e2c",
      "bb2404a455ff458c81a6db6e4f73ad47",
      "82a0b82417ee4c4f9682e6ad5caeb96a",
      "a68c4aece2774740b1b5fddf0a1c4303",
      "54603da336ac4ac1af50b0147bb12b5e",
      "16feb06130804a76ba1b098c198f9cb5",
      "aae30c7055fb428bab5f9b3b22dfae67",
      "f22e42c2863b428e945d33b317180cdc",
      "79970d709d9d49a986f01a530bb484b3"
     ]
    },
    "id": "9xH2KrOBvMsL",
    "outputId": "dfd11dc7-85ef-4148-d86b-1d67f5750bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a2a70569584e2b98793c60d8ec0d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4163fd03b474b9b9a424439f82e90cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d0794f2dc244e9889aad909e707923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4065504afae14da9ae54802a1f3f147d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e24bb084a14842b2536c5d9066c226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479b5be973bc4b7fa64db04925840084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed407f0637104485965adf3f2b6b2fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b0a3841d96420e9485b1c3144ca87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941a582dd4004dca89705d634ee6d7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeBERTa Training (Pro Mode, Seeded)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 31:55, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.943651</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.319641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.916770</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.272324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.096900</td>\n",
       "      <td>1.821929</td>\n",
       "      <td>0.578778</td>\n",
       "      <td>0.445404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.096900</td>\n",
       "      <td>1.778823</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.515468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.096900</td>\n",
       "      <td>1.744286</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.538181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.694000</td>\n",
       "      <td>1.827270</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.574685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.694000</td>\n",
       "      <td>1.835448</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.594995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.694000</td>\n",
       "      <td>1.923713</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.611393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.403000</td>\n",
       "      <td>2.009198</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>0.616455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.403000</td>\n",
       "      <td>2.143360</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.597195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.403000</td>\n",
       "      <td>2.162982</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.616763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.154600</td>\n",
       "      <td>2.240135</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.616722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.154600</td>\n",
       "      <td>2.252587</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.620014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.154600</td>\n",
       "      <td>2.249161</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.615237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.071900</td>\n",
       "      <td>2.236633</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.612216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.6259\n",
      "\n",
      "Generating submission.csv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24153e081067478a811683dd7dd5fe9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_e17f5836-3076-45ce-bdcb-e8a6592138c3\", \"submission_pro.csv\", 4221)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism helps reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#  config\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "#  data\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "#  tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "#  class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "class_weights_tensor = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "#  trainer\n",
    "class ProTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = ProTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "print(\"Starting DeBERTa Training (Pro Mode, Seeded)...\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "print(\"\\n\" + \"=\" * 40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\nFINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "\n",
    "\n",
    "#  submission\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission_pro.csv...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    out_df = pd.DataFrame(\n",
    "        {\n",
    "            \"index\": comp_test_ds[\"index\"],\n",
    "            \"evasion_label\": pred_labels,\n",
    "        }\n",
    "    )\n",
    "    out_df.to_csv(\"submission_pro.csv\", index=False)\n",
    "    files.download(\"submission_pro.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEmLvnJzW-OO"
   },
   "source": [
    "Set seed run below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "277e379681434b6f9f6be6b213d38bee",
      "affcfd36a6924582910320dfaded5fe3",
      "ae056aa78e3e4d3bba8f80e924a521cc",
      "a9519598af194b5bbb96daef181dd25d",
      "4da0c68c6d544f779c345961ab96923b",
      "cf55bc22122348fabf56a2d9ec2df650",
      "e7be5660b3264fc4ab1aa3d4b6e6e3c1",
      "e3e943f3568a497a80ac65a20e36631f",
      "4cc4065c256a4dc6b5cbbfbe1a1a6250",
      "5716657d8dcd46e18fff577f16fbefbf",
      "e83886841f894a508441be65be86c148",
      "9b084aaa0fac4d54b9262c39882df1d2",
      "0c0e273dbf15495298433497cd90898e",
      "81dd58e9aaa2471c81b51a56dd5dd999",
      "ad97b1aa8b67485bb07b4603391fe338",
      "368f3ce04ab4498894ab9efbd80f98aa",
      "ce2a016672694a28acb22b46ffd24fa8",
      "73eef1f10cb94df9b2ed6a9642fa688d",
      "a0617122fcdf4d61b34bce06fb768344",
      "0ed49cf470c74900b670002f190e3eed",
      "808f85d6b9954be2969ca9098725a040",
      "96da2b9a37514b90acf81493bd071566",
      "3de2baaacc1f4ffb9496ea561fdc1cd1",
      "3b98e90ab0d6406aa102cedecabc45e3",
      "254c1a6c3e784bc4804ded708c0330ab",
      "2a7e66bf2c3449f5bcfa911188d1972f",
      "2e5a2021da054c2e954838dc1cbc990d",
      "1d60bf79ec4d4ebaa45633273255c2ae",
      "3eb260ebceda4c719d3f2aecd46fad2d",
      "6d5c56d1c0d24bbeb46814dd9bf72b72",
      "15706e9d37924201a681200acd89c3fc",
      "984670cdc91a4019b50968de608e1d92",
      "0a3ab63497b743d49271ec02bf7f638e",
      "c233c435010845c8a49ff410ed4e50e5",
      "f5ff0954a47441c7b8040a06dba4a5f9",
      "fcc9fc7777e84bf8b774579ee7a26e80",
      "32549cd498fa42968ba9a32bf00fef4c",
      "e294c4c26cde4a61a2588828eceaa2ca",
      "124ca3e273bb41949e5aaf50c76a48cc",
      "376ed0735aea4bd79651c7ef8133c9e8",
      "a256d8aa3dd341a4af1e07236cfd3c12",
      "33ce254cadb84f6b917567cb71cb19b6",
      "0125861ce628471791f255bc56415fa0",
      "ed15bc257d3648c39681c6e4878020d1",
      "1b59222a175c42cea0f9b70f0d8cf1f4",
      "7966983f226d43248d1fa92916c47e30",
      "d8e221711ba34505b7f5a8bf3fa4cf2d",
      "6591289cfcc84953a1b00c66727cdc4a",
      "26298f5509f6435080f8ec531e559eb8",
      "342efce677224b819776551e0da5bd1d",
      "2a5312e1e79c4bd3870c35a05d444a47",
      "cd85c5f96fca49ffb8ebc53d37598d4b",
      "a49124da9df14a729375709c83c35df0",
      "84828671f849472b838af545d153aab4",
      "445d16a3bdb4453c90ea82f3085cd67b",
      "670223e70c204f9dbb92123e0421625b",
      "1fb8d9514e2b4c07b58946b042aae838",
      "69de06fc2fd04b02af2e96f3cb6f4936",
      "f35db4b1bca64e2d85b9666554a7958c",
      "d57773087f7249aea952277e8fc612c1",
      "0f611f541714491daf42e8625d183845",
      "57e1181b64bc44dca6a4d866cd3ca0c6",
      "cacabae4ba374776a9ae28bfeef77518",
      "75d266e4ed104bc1afab79a905e66d69",
      "8e54f85a7eb84721ad60c42cc0eefdf7",
      "eb858e478c494444b23e5c7acf630c32",
      "9ead3bc7079a4a21ab8f01e489316588",
      "045feb0a0035455aa8cb4a74e4c3977c",
      "4cb0f0960dce46efac4fa3d1ca1c9772",
      "cf1a4cf23f39463883f7d1fd5c0ea02d",
      "9fac4ca57fd045ef8adf07881b3d26cd",
      "46a150cbb3164043a61bf077a0842286",
      "e6c1f39ba60a417f98f8890aa54998cd",
      "027ad0bc7acf461b954754227d728cf8",
      "03f947f9943c418c94215225a3cb7b93",
      "1dd4a11ca1174e3383427c1a4fcdf2bf",
      "79d0810624b34a67a6a658a4cf1ad492",
      "80727e8d4f5c4291b931863381914bef",
      "4285bb5caf1745d09515d1683c563b04",
      "1c44acd45a6b44c580a9c048cbe6c7c1",
      "2a2f0b59b7464e2eb93a27e8ab631b5a",
      "17816ae544d145f08e90dbbe34a0c91c",
      "3fe91e6bd2024b0fb9ce363f7f3594f5",
      "0c0edd046abf4f8a8169af1985723662",
      "7db5724e1f704582a394a1f865eba8fe",
      "53d34c05ad414c599379726a9d00f6f9",
      "478a5c9e28854be49d6ff00f7e525102",
      "8a486fc97f4a4aca83128cf814cdeada",
      "1adbecc8a2cd4a63aa353407b32dfd9d",
      "00b2ae458e2f4ea39df6938d44c8019e",
      "2b3bd0f652364069af656595c8678c6c",
      "db2728c672d04b9bb9115566c739f278",
      "93ea9c2fb47d475687262abd3ab14f50",
      "6a7dd16ef6214f53ae3f79f59c6ac550",
      "7a9bca6e63d542539480a31215c12263",
      "70c08d486cce4c99a57d47a53559398b",
      "6c1c66262d174f84b91ef4f8b67847f9",
      "02aad5299a3d460795dc64daf3d24ed8",
      "23ac2a32fa4e43feae7af7905b25fd28",
      "6c6c92eff9384d73ae5ac937efba6c98",
      "6eb7b0b17237425ea80df8370b1c561c",
      "b34f01def71249b7be8eed2fb1f33926",
      "7b5fa941c0d64ae49877a6e1f59659d2",
      "d05db1033e934ee997751330199ccc5d",
      "6a62c26c34d7499aaa2845834776debf",
      "4612f601164748b4b560c9b49b660059",
      "cbca258a5d2849f794c08221a09bc7cb",
      "13cff2bc7e8b4387bd111c214f07bb85",
      "086f8039b22f48c6afddba7ca591411c",
      "80b56ff571df43a1a813650fa1c6ecd6"
     ]
    },
    "id": "LYd6glHgfB-q",
    "outputId": "f765af80-9475-4d57-eda0-7cfd4aa7f206"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Preprocessing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277e379681434b6f9f6be6b213d38bee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b084aaa0fac4d54b9262c39882df1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de2baaacc1f4ffb9496ea561fdc1cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2792\n",
      "Dev size: 311\n",
      "Held-out test size: 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c233c435010845c8a49ff410ed4e50e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b59222a175c42cea0f9b70f0d8cf1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670223e70c204f9dbb92123e0421625b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ead3bc7079a4a21ab8f01e489316588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80727e8d4f5c4291b931863381914bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adbecc8a2cd4a63aa353407b32dfd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DeBERTa Training (Pro Mode, Seeded)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2275' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2275/2625 27:42 < 04:15, 1.37 it/s, Epoch 13/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.348490</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.383592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.276027</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.285075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.472500</td>\n",
       "      <td>1.196895</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.377871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.472500</td>\n",
       "      <td>1.149883</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.485560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.472500</td>\n",
       "      <td>1.028996</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.495564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.004000</td>\n",
       "      <td>1.023135</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.547306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.004000</td>\n",
       "      <td>1.131010</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>0.578387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.004000</td>\n",
       "      <td>1.182748</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.571967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>1.308887</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.557543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>1.494811</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.568193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.607200</td>\n",
       "      <td>1.606463</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.567967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>1.747418</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.553640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>1.863150</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.560401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "COMPUTING FINAL HELD-OUT TEST SCORE\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " FINAL TEST SET MACRO F1: 0.5505\n",
      "   FINAL TEST SET ACCURACY: 0.6261\n",
      "\n",
      "Per-class performance on dev set:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   Claims ignorance       0.82      0.82      0.82        11\n",
      "      Clarification       0.78      0.88      0.82         8\n",
      "Declining to answer       0.83      0.77      0.80        13\n",
      "         Deflection       0.55      0.32      0.41        34\n",
      "            Dodging       0.64      0.56      0.60        64\n",
      "           Explicit       1.00      1.00      1.00        95\n",
      "            General       0.32      0.69      0.44        35\n",
      "           Implicit       0.39      0.27      0.32        44\n",
      "Partial/half-answer       0.00      0.00      0.00         7\n",
      "\n",
      "           accuracy                           0.66       311\n",
      "          macro avg       0.59      0.59      0.58       311\n",
      "       weighted avg       0.67      0.66      0.65       311\n",
      "\n",
      "\n",
      "Generating submission_pro.csv...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6c92eff9384d73ae5ac937efba6c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_87f939f0-7816-4975-b279-38dd09cfec6e\", \"submission_pro.csv\", 3984)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "#impliments global seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "try:\n",
    "    from transformers import set_seed\n",
    "    set_seed(SEED)\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# cuDNN determinism\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_v3_pro_results\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "\n",
    "# Slightly higher LR to help convergence / minority classes\n",
    "LR = 1e-5\n",
    "EPOCHS = 15\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess_text(example):\n",
    "    # Context Injection with clearer task wording\n",
    "    clarity = example.get('clarity_label', 'Unknown')\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "\n",
    "    text = (\n",
    "        f\"Clarity label: {clarity}.\\n\"\n",
    "        f\"Question: {example['question']}\\n\"\n",
    "        f\"Answer: {example['interview_answer']}\\n\"\n",
    "        \"Task: classify the evasion strategy used in the answer.\"\n",
    "    )\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "full_data = dataset[\"train\"].map(preprocess_text)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess_text)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# Double Split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\"\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Dev size:\", len(eval_ds))\n",
    "print(\"Held-out test size:\", len(held_out_test_ds))\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weights_tensor = torch.tensor(\n",
    "    class_weights,\n",
    "    dtype=torch.float\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class ProTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.0\n",
    "        )\n",
    "\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, self.model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (predictions == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, predictions, average=\"macro\")\n",
    "    }\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE*2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "\n",
    "    weight_decay=0.05,             # Higher decay = less overfitting\n",
    "    warmup_ratio=0.1,              # Warmup prevents early confusion\n",
    "    lr_scheduler_type=\"cosine\",    # Smooth curve = better convergence\n",
    "    max_grad_norm=1.0,             # Gradient clipping for stability\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "trainer = ProTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=6)] # increased patience\n",
    ")\n",
    "\n",
    "print(\"Starting DeBERTa Training (Pro Mode, Seeded)...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"COMPUTING FINAL HELD-OUT TEST SCORE\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(f\"\\n FINAL TEST SET MACRO F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "print(f\"   FINAL TEST SET ACCURACY: {test_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\nPer-class performance on dev set:\")\n",
    "dev_preds = trainer.predict(eval_ds)\n",
    "y_true = dev_preds.label_ids\n",
    "y_pred = np.argmax(dev_preds.predictions, axis=-1)\n",
    "print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "\n",
    "if \"test\" in dataset:\n",
    "    print(\"\\nGenerating submission_pro.csv...\")\n",
    "    comp_test_ds = comp_test_ds.map(tokenize_fn, batched=True)\n",
    "    if \"index\" not in comp_test_ds.column_names:\n",
    "        comp_test_ds = comp_test_ds.add_column(\"index\", range(len(comp_test_ds)))\n",
    "\n",
    "    comp_preds = trainer.predict(comp_test_ds)\n",
    "    pred_ids = np.argmax(comp_preds.predictions, axis=-1)\n",
    "    pred_labels = [id2label[p] for p in pred_ids]\n",
    "\n",
    "    out_df = pd.DataFrame({\n",
    "        \"index\": comp_test_ds[\"index\"],\n",
    "        \"evasion_label\": pred_labels\n",
    "    })\n",
    "    out_df.to_csv(\"submission_pro.csv\", index=False)\n",
    "    files.download(\"submission_pro.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVj31NnaW8L2"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}