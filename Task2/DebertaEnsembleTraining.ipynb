{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "88f699935dcf48d182a07d61496685a9",
      "837fe3162d5e4a6396ff7b9395625fc9",
      "df3663d04f81439c804f5448648908f0",
      "3143966869194a5fb441362ca4a608a6",
      "1308fce21ea84e52905d255e6eee7930",
      "d0bb420e49d146b882de3e3dfc494a57",
      "2b6c97cc66ca49c2a418c6df74969d82",
      "3a6fee6939ad42c4b67fb7c4d4fa0a74",
      "593a39c3b80d44e8bd59b19173b6f6ab",
      "9ca33f559af846d2854d58aeeb373ec9",
      "00cec12e9dd14ec695055d6370175c4c",
      "8b0127a0821a4ff9b35b3f35edc611c7",
      "7ebf94266cb34e74a0e47e47b61b5f08",
      "d77b5585d7664dfe95f4a937f607d959",
      "5771797ae6a14c94913351b427df20be",
      "578e15ea28d74f949ad2d9b1f115b325",
      "a731e0628df642019b0adbc912e4fbab",
      "c7ec407ed8354d389367ca58650575c0",
      "e336f638eeba4cf3bfafeb2272564b83",
      "be088850309d478b826a184820aad4a6",
      "c445faf4219a42f9ac44ac83b8e462ad",
      "52dc1898bef74daebd168fb5737e9137",
      "7ce8e4e50ed84282b681e9a1d75e9484",
      "a898249372614f3583b9f7b3350e4684",
      "2d3d90523e1d4202ad309b03e46c7d1a",
      "c2adae6820d844ec9fc9f9ad0ef26fb0",
      "1459fd2ddeef4480919c72c60a120a99",
      "8e917a39f02c43ed969b64b943900b93",
      "efd63a167ef141e28e92c65bd1c26d8c",
      "974baf94153941fab6ba149e7de02067",
      "e9c1e2e336964169a89b64d638688c73",
      "12ab834d2359415b978078e3e6dc1d23",
      "fffb400247e645389670dbacab13d9b6",
      "50282b70568a459f825aa3d5f7e16f80",
      "87b46b48b2ed4630a7906dfdd6c599d9",
      "a1c165a9dcd44322b7ed5d1148d5f92f",
      "54af452eecf04fad872843036fd00e1d",
      "d9d7baf7ae394788aff6851a32319810",
      "0ee635e071d1462ca4d93298ebaff8e0",
      "a0b3517e08a041049d0f88ac18c5c0d0",
      "cbb936f8406442c9a0dd7609c86ceaf2",
      "428e9df5ea1745d49b62f505b05c16e7",
      "cfb08736aaa54ed8b1a4f44bca3cecb4",
      "f7c742b5666741a3acb328e5d5ef2549",
      "7f28e0043eb8418aa31ac0db2b489cfb",
      "92414da9a9a24633ac99656c8caef141",
      "7ec3d9df23c44db2b9ebcfe1ee8db91b",
      "45fb49770465496bac9bee816d6ecee8",
      "60349f13c03f45ca998f7559dbaf9fba",
      "45f950168d824488a8b49bee70a35ae9",
      "2740f8c1d8714eb59309a1de778141f4",
      "1cac51c7e83948a7a7059a9cb523c45b",
      "220c18d0862e4477ae0af17355be62a1",
      "5f9d9da38d08429e83e3cb3deaf8d05d",
      "acb28e89ddc74fffaf4a7aa08629e508",
      "bd4cb7a851e94845bc38cd0b4a0d75de",
      "84a0763b51d54821a11ed490d081d220",
      "22faa517e5104c4eaef2b01e7f86b5b6",
      "d3578ed61872449095e5b2d594e4b3c0",
      "368401fc4a764c70b82661e7af214adf",
      "be23b9c3647146538e4aef986836e44a",
      "6d6d5a013c2f4898808dfbed476efef7",
      "70285f1c51404b92ad0a0521dec36ffb",
      "365dd28b7cf94ef79ce1af06bb1a3197",
      "d4019e22423644e189d88700af80be19",
      "6d60e9241c8245f4a80ee25635a3ce1b",
      "39ba66bb14cb413c9c7c18dc40251435",
      "7f4ae6d5daeb4abf9e1a789b125f1839",
      "be62dd29c7c34235b38a3040107c8141",
      "39a0b276dc2344a1a08540c48111e66b",
      "d7497be531af4e528d1a2a8c85802d49",
      "b97c6cc03caa4f5e9732f9e29f807ce2",
      "43533193ca4949e7bdd404f0dca2ca69",
      "b54aaccaeb8544a2ad0af57410f673d0",
      "fb79df87db6f4875bd22348dbbd7340a",
      "611fbc2dd6ba44e68cd6a7c5019d7c27",
      "3a9201b28f2c4fbcb0513052167d628a",
      "af5fd606bdb942229c46fc15f30ec086",
      "6bdcce9c4f1b418cb6f22313575d4a33",
      "c216716dfb4440dbacfcca6d0829f784",
      "4485c22aa29c4bf48decd7ad866646ba",
      "1ba90e744e3c494cb159dbdf8b69e7c2",
      "e49cb5678ac942e5aa403ad6bfd42cf4",
      "522e54754b9c4ef7b8073f23d80830b1",
      "759411ee738c4bf89044df25cb130241",
      "7cd34376fe384d3ea6d1949c06903e01",
      "e3f80e23ceff4e6ea97a0faff06c711f",
      "35e75a40db914e64a2843c3f170fe336",
      "f4a02c5de1d74a298e3dbaf37434a416",
      "3b54864ad0b44da69a7b5aa2030182b5",
      "12cb8e5dec554f26991c3ff5647a3904",
      "2e7b226e5a9d446faf10b319628a1c8e",
      "c051e8d8e1bc43e8bceb790c22f18d05",
      "f359dc175cf449a7b1ea77cd4b6e644a",
      "3e850bc52d754e9a984e570c7d0b0b1c",
      "63461bcf37fa48dc90aa3cf9f045ab91",
      "43f727c672834e3c8fc19ca65d8d7f46",
      "706c8ffb4fdd4616b3133255fa47021a",
      "8e99977a8b9d44728287da7bfe31b5d6",
      "b0499cb9638a444db2eda2cfe5e04e73",
      "5c987aafd4f14a358a0504c842acb4f7",
      "a50b702539bf4c0dae2525ee4767fad5",
      "640a6a47c080425b92c4a0e94da6535f",
      "fd4d333f6afb422da63112a6f59f5dcf",
      "064b5a08b0ee408087c8d08db7cbe442",
      "96e472c6f0ca486e8a4851f3d1e8c058",
      "73de110ebfa44af69e8da5124f084cd2",
      "c4741d0e612b444eaabd4d4897e9d0ff",
      "cbd8b95185b84a73bf13e3bcbe902f52",
      "95f65cceef184482b92015474756ced8"
     ]
    },
    "id": "MnNLAoOjRFTF",
    "outputId": "e787bf2a-cc5f-4ac7-db1f-a74dee74d9a2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88f699935dcf48d182a07d61496685a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0127a0821a4ff9b35b3f35edc611c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ce8e4e50ed84282b681e9a1d75e9484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50282b70568a459f825aa3d5f7e16f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f28e0043eb8418aa31ac0db2b489cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4cb7a851e94845bc38cd0b4a0d75de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ba66bb14cb413c9c7c18dc40251435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5fd606bdb942229c46fc15f30ec086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a02c5de1d74a298e3dbaf37434a416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 40:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.034497</td>\n",
       "      <td>0.517685</td>\n",
       "      <td>0.282873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.907372</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.273221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.132000</td>\n",
       "      <td>1.841862</td>\n",
       "      <td>0.575563</td>\n",
       "      <td>0.425551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.132000</td>\n",
       "      <td>1.767888</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.526254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.132000</td>\n",
       "      <td>1.752607</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.543375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.754700</td>\n",
       "      <td>1.778494</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.589780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.754700</td>\n",
       "      <td>1.923069</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.569459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.754700</td>\n",
       "      <td>1.905835</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.588083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.472100</td>\n",
       "      <td>2.034865</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.564591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.472100</td>\n",
       "      <td>2.082523</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.595771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.472100</td>\n",
       "      <td>2.209235</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>0.591886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.197100</td>\n",
       "      <td>2.221098</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.605176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.197100</td>\n",
       "      <td>2.290432</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.596467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.197100</td>\n",
       "      <td>2.322799</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.604294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.077100</td>\n",
       "      <td>2.343504</td>\n",
       "      <td>0.668810</td>\n",
       "      <td>0.610196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.077100</td>\n",
       "      <td>2.362922</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.584418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.077100</td>\n",
       "      <td>2.334267</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.595066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.024200</td>\n",
       "      <td>2.340362</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.607111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.024200</td>\n",
       "      <td>2.336068</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.600968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.022400</td>\n",
       "      <td>2.336205</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.600968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 42 F1: 0.5685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 40:36, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.030439</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.297377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.898445</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.396686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.115900</td>\n",
       "      <td>1.814815</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.481339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.115900</td>\n",
       "      <td>1.728893</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.541438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.115900</td>\n",
       "      <td>1.689038</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.589067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.714000</td>\n",
       "      <td>1.748508</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.566442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.714000</td>\n",
       "      <td>1.869324</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.566812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.714000</td>\n",
       "      <td>1.940056</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.581212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.376100</td>\n",
       "      <td>2.128870</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.592479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.376100</td>\n",
       "      <td>2.237271</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.581313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.376100</td>\n",
       "      <td>2.333869</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.554684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.102400</td>\n",
       "      <td>2.376917</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.562022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.102400</td>\n",
       "      <td>2.419646</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.574533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.102400</td>\n",
       "      <td>2.418585</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.581167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>2.472551</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.558394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>2.452293</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.556980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>2.460469</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.563215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>2.444309</td>\n",
       "      <td>0.591640</td>\n",
       "      <td>0.550792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.027000</td>\n",
       "      <td>2.450159</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.555761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.011600</td>\n",
       "      <td>2.451309</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.555761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 777 F1: 0.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 40:52, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.232351</td>\n",
       "      <td>0.350482</td>\n",
       "      <td>0.171857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.910518</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.373788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>1.845743</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.459024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>1.705118</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.534482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>1.708964</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.509258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.760200</td>\n",
       "      <td>1.786461</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.540759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.760200</td>\n",
       "      <td>1.749328</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.565626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.760200</td>\n",
       "      <td>1.843158</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.548271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.515400</td>\n",
       "      <td>1.936963</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.563723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.515400</td>\n",
       "      <td>2.077690</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.555171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.515400</td>\n",
       "      <td>2.193913</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.563187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.190300</td>\n",
       "      <td>2.261261</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.570761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.190300</td>\n",
       "      <td>2.288626</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.588217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.190300</td>\n",
       "      <td>2.373065</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.552188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>2.397240</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.548823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>2.359121</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.585619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>2.381120</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.564265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.021000</td>\n",
       "      <td>2.353390</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.577232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.021000</td>\n",
       "      <td>2.358409</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.575746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.022000</td>\n",
       "      <td>2.358337</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.575203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 999 F1: 0.5769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 41:25, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.219633</td>\n",
       "      <td>0.405145</td>\n",
       "      <td>0.297808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.895993</td>\n",
       "      <td>0.530547</td>\n",
       "      <td>0.329470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.157100</td>\n",
       "      <td>1.860107</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.399496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.157100</td>\n",
       "      <td>1.807572</td>\n",
       "      <td>0.562701</td>\n",
       "      <td>0.451227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.157100</td>\n",
       "      <td>1.756429</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.483364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.785800</td>\n",
       "      <td>1.713442</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.560035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.785800</td>\n",
       "      <td>1.784536</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.559656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.785800</td>\n",
       "      <td>1.815251</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.572412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.563700</td>\n",
       "      <td>1.930297</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.568050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.563700</td>\n",
       "      <td>2.005022</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.552349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.563700</td>\n",
       "      <td>2.172084</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.576746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.291200</td>\n",
       "      <td>2.192089</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.588608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.291200</td>\n",
       "      <td>2.349552</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.561031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.291200</td>\n",
       "      <td>2.368884</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.573365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.110100</td>\n",
       "      <td>2.409927</td>\n",
       "      <td>0.598071</td>\n",
       "      <td>0.568486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.110100</td>\n",
       "      <td>2.365993</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.593851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.110100</td>\n",
       "      <td>2.390451</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.583989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.037200</td>\n",
       "      <td>2.413953</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.584557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.037200</td>\n",
       "      <td>2.425714</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.590642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.029300</td>\n",
       "      <td>2.423359</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.590642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 2024 F1: 0.5677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 41:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.002906</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.274194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.896110</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.375310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.127200</td>\n",
       "      <td>1.864424</td>\n",
       "      <td>0.527331</td>\n",
       "      <td>0.384170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.127200</td>\n",
       "      <td>1.834897</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.449133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.127200</td>\n",
       "      <td>1.725620</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.545712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>1.686954</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.616034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>1.791248</td>\n",
       "      <td>0.607717</td>\n",
       "      <td>0.564545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>1.837995</td>\n",
       "      <td>0.662379</td>\n",
       "      <td>0.587794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.518200</td>\n",
       "      <td>1.941965</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.575385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.518200</td>\n",
       "      <td>2.004208</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.598545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.518200</td>\n",
       "      <td>2.140358</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.590213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.223600</td>\n",
       "      <td>2.224974</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.600936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.223600</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.594630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.223600</td>\n",
       "      <td>2.268686</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.584207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.067800</td>\n",
       "      <td>2.280656</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.604768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.067800</td>\n",
       "      <td>2.300822</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.588830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.067800</td>\n",
       "      <td>2.308883</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.600592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.038700</td>\n",
       "      <td>2.311306</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.593599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.038700</td>\n",
       "      <td>2.309745</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.574207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.015700</td>\n",
       "      <td>2.309769</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.578400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed 1234 F1: 0.5523\n",
      "\n",
      "Individual Model F1s: ['0.5685', '0.6268', '0.5769', '0.5677', '0.5523']\n",
      "Ensemble Weights: ['0.197', '0.217', '0.199', '0.196', '0.191']\n",
      "\n",
      "Ensemble Results:\n",
      "  Weighted Ensemble F1: 0.5694 | Acc: 0.6087\n",
      "  Simple Average F1: 0.5671\n",
      "  Best Individual F1: 0.6268\n",
      "  Improvement over best: -0.0574\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0499cb9638a444db2eda2cfe5e04e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "MODEL = \"microsoft/deberta-v3-large\"\n",
    "SAVE_DIR = \"./deberta_best_config\"\n",
    "MAXLEN = 512\n",
    "BATCH = 4\n",
    "GRAD_ACC = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 20\n",
    "\n",
    "USE_ENSEMBLE = True\n",
    "SEEDS = [42, 777, 999, 2024, 1234]\n",
    "NUM_MODELS = 5\n",
    "\n",
    "ds = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def prep(x):\n",
    "    c = x.get('clarity_label', 'Unknown') or 'Unknown'\n",
    "    x_text = f\"Context: {c} | Question: {x['question']} Answer: {x['interview_answer']}\"\n",
    "    return {\"text\": x_text, \"evasion_label\": x[\"evasion_label\"]}\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].map(prep)\n",
    "if \"test\" in ds:\n",
    "    test_ds = ds[\"test\"].map(prep)\n",
    "\n",
    "ds[\"train\"] = ds[\"train\"].class_encode_column(\"evasion_label\")\n",
    "\n",
    "split = ds[\"train\"].train_test_split(test_size=0.1, seed=42, stratify_by_column=\"evasion_label\")\n",
    "train_full, holdout = split[\"train\"], split[\"test\"]\n",
    "subsplit = train_full.train_test_split(test_size=0.1, seed=42, stratify_by_column=\"evasion_label\")\n",
    "train_ds, eval_ds = subsplit[\"train\"], subsplit[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "lbl2id = {l: i for i, l in enumerate(labels)}\n",
    "id2lbl = {i: l for l, i in lbl2id.items()}\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def tokenize(x):\n",
    "    return tok(x[\"text\"], padding=\"max_length\", truncation=True, max_length=MAXLEN)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True).map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(tokenize, batched=True).map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "holdout = holdout.map(tokenize, batched=True).map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "def train_model(seed, ensemble_idx=None):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    set_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2lbl,\n",
    "        label2id=lbl2id\n",
    "    )\n",
    "\n",
    "    y = train_ds[\"evasion_label\"]\n",
    "    weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "    w_tensor = torch.tensor(weights, dtype=torch.float)\n",
    "    if torch.cuda.is_available():\n",
    "        w_tensor = w_tensor.cuda()\n",
    "\n",
    "    class MyTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            lbls = inputs.get(\"labels\")\n",
    "            out = model(**inputs)\n",
    "            logits = out.get(\"logits\")\n",
    "            loss_fn = nn.CrossEntropyLoss(weight=w_tensor, label_smoothing=0.1)\n",
    "            loss = loss_fn(logits.view(-1, model.config.num_labels), lbls.view(-1))\n",
    "            return (loss, out) if return_outputs else loss\n",
    "\n",
    "    save_path = f\"{SAVE_DIR}_seed{seed}\" if ensemble_idx is not None else SAVE_DIR\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=save_path,\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH,\n",
    "        per_device_eval_batch_size=BATCH * 2,\n",
    "        gradient_accumulation_steps=GRAD_ACC,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.05,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,  #only keep the best checkpoint\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=2,\n",
    "        seed=seed\n",
    "    )\n",
    "\n",
    "    def metrics(p):\n",
    "        logits, y_true = p\n",
    "        y_pred = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            \"accuracy\": (y_pred == y_true).mean(),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "        }\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        compute_metrics=metrics,\n",
    "        callbacks=[]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    result = trainer.evaluate(holdout)\n",
    "    print(f\"Seed {seed} F1: {result['eval_macro_f1']:.4f}\")\n",
    "    return trainer, result\n",
    "\n",
    "def cleanup_old_checkpoints(base_dir=\"./deberta_best_config\", keep_only_best=True):\n",
    "    \"\"\"Clean up old checkpoint directories to save disk space\"\"\"\n",
    "    if not os.path.exists(base_dir):\n",
    "        return\n",
    "\n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if os.path.isdir(item_path) and \"checkpoint\" in item:\n",
    "            if keep_only_best:\n",
    "                #keeps the best checkpoint\n",
    "                checkpoints = [d for d in os.listdir(item_path) if \"checkpoint\" in d]\n",
    "                if checkpoints:\n",
    "                    checkpoints.sort(key=lambda x: int(x.split(\"-\")[1]) if \"-\" in x else 0, reverse=True)\n",
    "                    for cp in checkpoints[1:]:\n",
    "                        cp_path = os.path.join(item_path, cp)\n",
    "                        if os.path.exists(cp_path):\n",
    "                            shutil.rmtree(cp_path)\n",
    "                            print(cp_path)\n",
    "\n",
    "if USE_ENSEMBLE and NUM_MODELS > 1:\n",
    "    all_preds = []\n",
    "    all_results = []\n",
    "    all_trainers = []\n",
    "    for i, seed in enumerate(SEEDS[:NUM_MODELS]):\n",
    "        tr, res = train_model(seed=seed, ensemble_idx=i)\n",
    "        all_trainers.append(tr)\n",
    "        all_results.append(res)\n",
    "        preds = tr.predict(holdout).predictions\n",
    "        all_preds.append(preds)\n",
    "\n",
    "\n",
    "    individual_f1s = [r['eval_macro_f1'] for r in all_results]\n",
    "    weights = np.array(individual_f1s)\n",
    "    weights = weights / weights.sum()\n",
    "\n",
    "    print(f\"\\nIndividual Model F1s: {[f'{f:.4f}' for f in individual_f1s]}\")\n",
    "    print(f\"Ensemble Weights: {[f'{w:.3f}' for w in weights]}\")\n",
    "\n",
    "    weighted_probs = np.average(all_preds, axis=0, weights=weights)\n",
    "    y_pred = np.argmax(weighted_probs, axis=-1)\n",
    "    y_true = holdout[\"labels\"]\n",
    "\n",
    "    f1_ens = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    acc_ens = (y_pred == y_true).mean()\n",
    "\n",
    "    avg_probs = np.mean(all_preds, axis=0)\n",
    "    y_pred_avg = np.argmax(avg_probs, axis=-1)\n",
    "    f1_ens_avg = f1_score(y_true, y_pred_avg, average=\"macro\")\n",
    "\n",
    "    print(f\"\\nEnsemble Results:\")\n",
    "    print(f\"  Weighted Ensemble F1: {f1_ens:.4f} | Acc: {acc_ens:.4f}\")\n",
    "    print(f\"  Simple Average F1: {f1_ens_avg:.4f}\")\n",
    "    print(f\"  Best Individual F1: {max(individual_f1s):.4f}\")\n",
    "    print(f\"  Improvement over best: {f1_ens - max(individual_f1s):.4f}\")\n",
    "\n",
    "    if \"test\" in ds:\n",
    "        test_ds = test_ds.map(tokenize, batched=True)\n",
    "        if \"index\" not in test_ds.column_names:\n",
    "            test_ds = test_ds.add_column(\"index\", range(len(test_ds)))\n",
    "\n",
    "        all_test_preds = [t.predict(test_ds).predictions for t in all_trainers]\n",
    "        weighted_test = np.average(all_test_preds, axis=0, weights=weights)\n",
    "        pred_ids = np.argmax(weighted_test, axis=-1)\n",
    "        preds_lbl = [id2lbl[i] for i in pred_ids]\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"index\": test_ds[\"index\"],\n",
    "            \"evasion_label\": preds_lbl\n",
    "        }).to_csv(\"ensemble_submission.csv\", index=False)\n",
    "\n",
    "    # for saving disk space\n",
    "    cleanup_old_checkpoints()\n",
    "\n",
    "else:\n",
    "    seed = SEEDS[0]\n",
    "    trainer, result = train_model(seed)\n",
    "\n",
    "    if \"test\" in ds:\n",
    "        test_ds = test_ds.map(tokenize, batched=True)\n",
    "        if \"index\" not in test_ds.column_names:\n",
    "            test_ds = test_ds.add_column(\"index\", range(len(test_ds)))\n",
    "\n",
    "        test_out = trainer.predict(test_ds).predictions\n",
    "        test_pred_ids = np.argmax(test_out, axis=-1)\n",
    "        final_labels = [id2lbl[i] for i in test_pred_ids]\n",
    "\n",
    "        pd.DataFrame({\n",
    "            \"index\": test_ds[\"index\"],\n",
    "            \"evasion_label\": final_labels\n",
    "        }).to_csv(\"best_config_submission.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
