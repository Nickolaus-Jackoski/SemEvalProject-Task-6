{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvkzDdNewTCP",
    "outputId": "36bda36a-0bc1-47fa-ea7e-e078f6fb2a9d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q \"transformers>=4.45.0\" \"datasets>=2.20.0\" \"peft>=0.18.0\" \"accelerate>=1.2.0\" bitsandbytes scikit-learn huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359,
     "referenced_widgets": [
      "39184ebacba4444a955b283d247a9037",
      "83eb79660f584d5086342ba18ff6fcc3",
      "840b072517a947cf81e4e3297648fce3",
      "a6894c750759467e81b3fd88f6698f07",
      "c4dd37c6094c4e46847ec00314520141",
      "07e40333c1f14edfb93b400642e290fb",
      "8442aa1124e8440e9a2c6973f297e195",
      "29aa2e6213f44f90902acc38225d44cd",
      "52315d2e3b584d29a01326cf6cb0b333",
      "b4a04538b3d24367bcad0a2e4144120c",
      "c1f331b41c1d4c8e936f644bbe17d79e",
      "c5aa8ba257084e7586bfadfb554662af",
      "36ad318867ad4d97a306450ce0a461ee",
      "9c5fad8011474bf0bc4412cb231c1cb1",
      "d83d40b4a5f64cada11b6efb4f25b582",
      "20b023a672aa4f11ab9b6d084f93b089",
      "5e79ab8d6d7c45babbdbdd5c620e7772"
     ]
    },
    "id": "v_hRubV6wurG",
    "outputId": "38ad475a-3bae-4190-9c28-3d0b90bdf937"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39184ebacba4444a955b283d247a9037"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/content/hf_cache\""
   ],
   "metadata": {
    "id": "f0zq_OcYxISK"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%%writefile task2_llama_lora_v3.py\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "\n",
    "EVASION_LABEL_DESCRIPTIONS = {\n",
    "    \"Claims ignorance\": \"The speaker says they do not know or are not aware.\",\n",
    "    \"Clarification\": \"The speaker clarifies or asks for clarification.\",\n",
    "    \"Declining to answer\": \"The speaker explicitly refuses to answer.\",\n",
    "    \"Deflection\": \"The speaker changes the subject.\",\n",
    "    \"Dodging\": \"The speaker avoids answering without explicitly refusing.\",\n",
    "    \"Explicit\": \"The speaker directly answers.\",\n",
    "    \"General\": \"The speaker answers vaguely.\",\n",
    "    \"Implicit\": \"The speaker implies the answer without stating it clearly.\",\n",
    "    \"Partial/half-answer\": \"The speaker answers only part of the question.\",\n",
    "}\n",
    "\n",
    "def build_label_mappings(labels: List[str]) -> (Dict[str, int], Dict[int, str]):\n",
    "    uniq = sorted(set(labels))\n",
    "    label2id = {lab: i for i, lab in enumerate(uniq)}\n",
    "    id2label = {i: lab for lab, i in label2id.items()}\n",
    "    return label2id, id2label\n",
    "\n",
    "\n",
    "def balance_dataset(ds, label_column=\"evasion_label\"):\n",
    "    labels = ds[label_column]\n",
    "    counts = Counter(labels)\n",
    "    max_count = max(counts.values())\n",
    "\n",
    "    # Target count: We don't want to explode the dataset size,\n",
    "    # so let's aim for everyone to have at least 30% of the max count.\n",
    "    target_count = int(max_count * 0.3)\n",
    "\n",
    "    shards_to_add = [ds]\n",
    "\n",
    "    print(f\"Original Counts: {dict(counts)}\")\n",
    "\n",
    "    for label, count in counts.items():\n",
    "        if count < target_count:\n",
    "            shortfall = target_count - count\n",
    "            # filter rows for this label\n",
    "            subset = ds.filter(lambda x: x[label_column] == label)\n",
    "\n",
    "            repeats = int(shortfall / count)\n",
    "            if repeats > 0:\n",
    "                for _ in range(repeats):\n",
    "                    shards_to_add.append(subset)\n",
    "\n",
    "    balanced_ds = concatenate_datasets(shards_to_add)\n",
    "    balanced_ds = balanced_ds.shuffle(seed=42)\n",
    "\n",
    "    print(f\"Balanced Dataset Size: {len(balanced_ds)} (Original: {len(ds)})\")\n",
    "    return balanced_ds\n",
    "\n",
    "\n",
    "def build_prompt(question: str, full_question: str, answer: str) -> str:\n",
    "    q = (question or \"\").strip()\n",
    "    if not q: q = (full_question or \"\").strip()\n",
    "\n",
    "    system_instructions = (\n",
    "        \"You are an expert at analyzing political and media interviews.\\n\"\n",
    "        \"Your task is to classify an answer into one of the following evasion techniques:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    label_list_str = \"\"\n",
    "    for name, desc in EVASION_LABEL_DESCRIPTIONS.items():\n",
    "        label_list_str += f\"- {name}: {desc}\\n\"\n",
    "\n",
    "    user_block = (\n",
    "        f\"Question: {q}\\n\"\n",
    "        f\"Answer: {answer.strip()}\\n\\n\"\n",
    "        \"From the list above, which single evasion label best describes this answer?\\n\"\n",
    "        \"Respond with exactly one label name.\\n\"\n",
    "        \"Evasion label:\"\n",
    "    )\n",
    "\n",
    "    return system_instructions + label_list_str + \"\\n\" + user_block\n",
    "\n",
    "def extract_label_robust(text: str, valid_labels: List[str]) -> str:\n",
    "    text = text.strip()\n",
    "    # exact Match\n",
    "    for lab in valid_labels:\n",
    "        if lab.lower() == text.lower(): return lab\n",
    "    # starts With\n",
    "    for lab in valid_labels:\n",
    "        if text.lower().startswith(lab.lower()): return lab\n",
    "    # contains\n",
    "    for lab in valid_labels:\n",
    "        if lab.lower() in text.lower(): return lab\n",
    "    return \"Explicit\"\n",
    "\n",
    "def prepare_splits(seed: int = 42):\n",
    "    raw = load_dataset(\"ailsntua/QEvasion\")\n",
    "    if \"test\" in raw: test_raw = raw[\"test\"]\n",
    "    else: test_raw = raw[\"train\"].select(range(10))\n",
    "\n",
    "    train_raw = raw[\"train\"]\n",
    "    label2id, id2label = build_label_mappings(train_raw[\"evasion_label\"])\n",
    "\n",
    "    # Stratified Split\n",
    "    label_ids = [label2id[l] for l in train_raw[\"evasion_label\"]]\n",
    "    train_raw = train_raw.add_column(\"evasion_label_id\", label_ids)\n",
    "    train_raw = train_raw.class_encode_column(\"evasion_label_id\")\n",
    "\n",
    "    splits = train_raw.train_test_split(test_size=0.1, stratify_by_column=\"evasion_label_id\", seed=seed)\n",
    "    train_ds = splits[\"train\"].remove_columns([\"evasion_label_id\"])\n",
    "    dev_ds = splits[\"test\"].remove_columns([\"evasion_label_id\"])\n",
    "\n",
    "    print(\"Balancing training data...\")\n",
    "    train_ds = balance_dataset(train_ds)\n",
    "\n",
    "    ds = DatasetDict({\"train\": train_ds, \"dev\": dev_ds, \"test\": test_raw})\n",
    "    return ds, label2id, id2label\n",
    "\n",
    "def tokenize_for_causal_lm(ds: DatasetDict, tokenizer, label2id, max_length):\n",
    "    valid_labels = sorted(label2id.keys())\n",
    "\n",
    "    def _tokenize_train(examples):\n",
    "        texts = []\n",
    "        for q, iq, a, lab in zip(examples[\"question\"], examples[\"interview_question\"], examples[\"interview_answer\"], examples[\"evasion_label\"]):\n",
    "            prompt = build_prompt(q, iq, a)\n",
    "            target = lab.strip()\n",
    "            if target not in valid_labels: target = \"Explicit\"\n",
    "            texts.append(prompt + \" \" + target)\n",
    "\n",
    "        enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=max_length)\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        labels = []\n",
    "\n",
    "        for ids, txt in zip(input_ids, texts):\n",
    "            # Masking prompt\n",
    "            labels.append([-100] * len(ids))\n",
    "\n",
    "        # Simplified masking logic for speed/robustness\n",
    "        final_labels = []\n",
    "        for i in range(len(input_ids)):\n",
    "            q = examples[\"question\"][i]\n",
    "            iq = examples[\"interview_question\"][i]\n",
    "            a = examples[\"interview_answer\"][i]\n",
    "            prompt = build_prompt(q, iq, a)\n",
    "\n",
    "            p_len = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n",
    "            row_labels = [-100] * len(input_ids[i])\n",
    "            start_index = min(p_len, len(input_ids[i])-1)\n",
    "            for j in range(start_index, len(input_ids[i])):\n",
    "                if input_ids[i][j] == tokenizer.pad_token_id: break\n",
    "                row_labels[j] = input_ids[i][j]\n",
    "            final_labels.append(row_labels)\n",
    "\n",
    "        enc[\"labels\"] = final_labels\n",
    "        return enc\n",
    "\n",
    "    ds[\"train\"] = ds[\"train\"].map(_tokenize_train, batched=True)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./clarity_task2_llama_lora_v3\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=1)\n",
    "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=16)\n",
    "    parser.add_argument(\"--num_train_epochs\", type=float, default=3.0)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    parser.add_argument(\"--bf16\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    print(\"Loading Data & Balancing...\")\n",
    "    ds, label2id, id2label = prepare_splits(args.seed)\n",
    "\n",
    "    print(\"Loading Model...\")\n",
    "    compute_dtype = torch.bfloat16 if args.bf16 else torch.float16\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_name, quantization_config=bnb_cfg, device_map=\"auto\")\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # increased Dropout to 0.1 to fight overfitting\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "\n",
    "    print(\"Tokenizing...\")\n",
    "    ds = tokenize_for_causal_lm(ds, tokenizer, label2id, args.max_length)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        warmup_ratio=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=not args.bf16,\n",
    "        bf16=args.bf16,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        report_to=\"none\",\n",
    "        neftune_noise_alpha=5,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "    )\n",
    "\n",
    "    print(\"Training...\")\n",
    "    trainer.train()\n",
    "    print(\"Running Inference...\")\n",
    "    model.eval()\n",
    "\n",
    "    gen_config = {\n",
    "        \"max_new_tokens\": 10,\n",
    "        \"do_sample\": False,\n",
    "        \"repetition_penalty\": 1.2, # prevents looping error\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id\n",
    "    }\n",
    "\n",
    "    def run_inference(dataset_split, desc=\"dev\"):\n",
    "        preds = []\n",
    "        golds = []\n",
    "        for i in range(len(dataset_split)):\n",
    "            row = dataset_split[i]\n",
    "            q = row.get(\"question\", row.get(\"interview_question\"))\n",
    "            iq = row.get(\"interview_question\", q)\n",
    "            a = row.get(\"interview_answer\")\n",
    "            prompt = build_prompt(q, iq, a)\n",
    "\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                out = model.generate(**inputs, **gen_config)\n",
    "\n",
    "            gen_text = tokenizer.decode(out[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "            pred_label = extract_label_robust(gen_text, list(label2id.keys()))\n",
    "            preds.append(pred_label)\n",
    "\n",
    "            if \"evasion_label\" in row: golds.append(row[\"evasion_label\"])\n",
    "            if i % 50 == 0:\n",
    "                print(f\"[{desc}] {i}/{len(dataset_split)} | Gen: {gen_text.strip()} -> Pred: {pred_label}\")\n",
    "\n",
    "        return preds, golds\n",
    "\n",
    "    # Dev Eval\n",
    "    dev_preds, dev_golds = run_inference(ds[\"dev\"], \"dev\")\n",
    "    dev_ids = [label2id[g] for g in dev_golds]\n",
    "    pred_ids = [label2id[p] for p in dev_preds]\n",
    "\n",
    "    print(\"\\n=== FINAL MACRO F1: ===\")\n",
    "    print(f1_score(dev_ids, pred_ids, average=\"macro\"))\n",
    "\n",
    "    # Test Eval\n",
    "    print(\"Predicting Test Set...\")\n",
    "    test_preds, _ = run_inference(ds[\"test\"], \"test\")\n",
    "    df = pd.DataFrame({\"index\": ds[\"test\"][\"index\"], \"evasion_label\": test_preds})\n",
    "    df.to_csv(os.path.join(args.output_dir, \"predictions_v3.csv\"), index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssXzui5oxJ6V",
    "outputId": "3030d461-4e83-4bc5-af2a-73d17153fcd5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Writing task2_llama_lora_v3.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!python task2_llama_lora_v3.py \\\n",
    "    --output_dir \"./v3_results_local\" \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --bf16\n",
    "\n",
    "import shutil\n",
    "destination = \"/content/drive/MyDrive/SemEval_Clarity/v3_predictions.csv\"\n",
    "shutil.copy(\"./v3_results_local/predictions_v3.csv\", destination)\n",
    "print(f\"Success! Predictions saved to {destination}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "itYc5u3ZxNpJ",
    "outputId": "a020a4c5-17a6-4757-81e1-b635be1baccc"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2025-11-28 04:42:38.014912: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-28 04:42:38.032056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764304958.053444    7429 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764304958.059999    7429 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764304958.076485    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764304958.076516    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764304958.076519    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764304958.076521    7429 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-28 04:42:38.081413: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Loading Data & Balancing...\n",
      "README.md: 12.7kB [00:00, 23.2MB/s]\n",
      "data/train-00000-of-00001.parquet: 100% 3.90M/3.90M [00:00<00:00, 5.24MB/s]\n",
      "data/test-00000-of-00001.parquet: 100% 259k/259k [00:00<00:00, 1.07MB/s]\n",
      "Generating train split: 100% 3448/3448 [00:00<00:00, 62081.29 examples/s]\n",
      "Generating test split: 100% 308/308 [00:00<00:00, 48185.22 examples/s]\n",
      "Stringifying the column: 100% 3448/3448 [00:00<00:00, 118116.60 examples/s]\n",
      "Casting to class labels: 100% 3448/3448 [00:00<00:00, 116957.89 examples/s]\n",
      "Balancing training data...\n",
      "Original Counts: {'Explicit': 947, 'Dodging': 635, 'Implicit': 439, 'Deflection': 343, 'Clarification': 83, 'Claims ignorance': 107, 'General': 347, 'Declining to answer': 131, 'Partial/half-answer': 71}\n",
      "Filter: 100% 3103/3103 [00:00<00:00, 14532.25 examples/s]\n",
      "Filter: 100% 3103/3103 [00:00<00:00, 14954.44 examples/s]\n",
      "Filter: 100% 3103/3103 [00:00<00:00, 15045.51 examples/s]\n",
      "Filter: 100% 3103/3103 [00:00<00:00, 15016.18 examples/s]\n",
      "Balanced Dataset Size: 3720 (Original: 3103)\n",
      "Loading Model...\n",
      "tokenizer_config.json: 100% 51.0k/51.0k [00:00<00:00, 4.69MB/s]\n",
      "tokenizer.json: 100% 9.09M/9.09M [00:00<00:00, 36.1MB/s]\n",
      "special_tokens_map.json: 100% 73.0/73.0 [00:00<00:00, 579kB/s]\n",
      "config.json: 100% 654/654 [00:00<00:00, 5.92MB/s]\n",
      "model.safetensors.index.json: 100% 23.9k/23.9k [00:00<00:00, 97.7MB/s]\n",
      "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
      "model-00002-of-00004.safetensors:   0% 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0% 0.00/4.92G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 0.00/4.98G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   0% 0.00/1.17G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   0% 54.6k/1.17G [00:00<2:30:28, 129kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   0% 816k/4.92G [00:00<1:04:47, 1.26MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   0% 500k/5.00G [00:00<1:55:49, 719kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   2% 23.6M/1.17G [00:00<00:31, 36.5MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:   4% 50.4M/1.17G [00:00<00:16, 66.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  10% 118M/1.17G [00:01<00:06, 170MB/s]  \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   1% 67.8M/4.92G [00:01<01:07, 71.5MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 15.4k/4.98G [00:01<112:06:46, 12.3kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  16% 185M/1.17G [00:01<00:04, 202MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   0% 1.73M/5.00G [00:01<1:00:59, 1.37MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 23.6k/4.98G [00:01<72:58:52, 18.9kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  31% 363M/1.17G [00:01<00:01, 464MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 86.9k/4.98G [00:01<17:25:12, 79.4kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   3% 136M/5.00G [00:01<00:39, 122MB/s]    \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  43% 497M/1.17G [00:01<00:01, 453MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 114k/4.98G [00:01<14:52:34, 92.9kB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   3% 135M/4.92G [00:01<00:59, 81.0MB/s] \u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   4% 203M/5.00G [00:01<00:32, 149MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  54% 632M/1.17G [00:02<00:01, 493MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  60% 699M/1.17G [00:02<00:01, 460MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   8% 404M/5.00G [00:02<00:14, 317MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   5% 269M/4.92G [00:02<00:28, 163MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 150k/4.98G [00:02<16:30:44, 83.7kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:   9% 471M/5.00G [00:02<00:14, 309MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   7% 336M/4.92G [00:02<00:24, 184MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  66% 766M/1.17G [00:02<00:01, 283MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  11% 538M/5.00G [00:02<00:16, 270MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:   8% 403M/4.92G [00:02<00:26, 173MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 168k/4.98G [00:02<23:14:48, 59.5kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  13% 672M/5.00G [00:02<00:11, 366MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 190k/4.98G [00:03<19:27:18, 71.1kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  15% 740M/5.00G [00:03<00:11, 379MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  77% 900M/1.17G [00:03<00:01, 236MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  10% 470M/4.92G [00:03<00:34, 129MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  16% 807M/5.00G [00:03<00:18, 232MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 211k/4.98G [00:03<26:00:08, 53.2kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  83% 967M/1.17G [00:04<00:01, 185MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  17% 874M/5.00G [00:04<00:19, 207MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  11% 537M/4.92G [00:04<00:35, 125MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  19% 941M/5.00G [00:06<00:53, 75.6MB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors:  89% 1.03G/1.17G [00:06<00:01, 77.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  12% 604M/4.92G [00:06<01:08, 63.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00004-of-00004.safetensors: 100% 1.17G/1.17G [00:06<00:00, 176MB/s] \n",
      "\n",
      "\n",
      "model-00003-of-00004.safetensors:  15% 738M/4.92G [00:06<00:36, 113MB/s] \u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  23% 1.14G/5.00G [00:06<00:25, 153MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 277k/4.98G [00:06<39:24:22, 35.1kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  24% 1.21G/5.00G [00:06<00:21, 180MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26% 1.28G/5.00G [00:06<00:17, 212MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  16% 805M/4.92G [00:07<00:32, 126MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 294k/4.98G [00:07<35:45:44, 38.7kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  28% 1.41G/5.00G [00:07<00:12, 292MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 307k/4.98G [00:07<31:54:03, 43.3kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  19% 940M/4.92G [00:07<00:23, 172MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 323k/4.98G [00:07<29:17:25, 47.2kB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  31% 1.54G/5.00G [00:07<00:10, 328MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  20% 1.01G/4.92G [00:07<00:19, 197MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   0% 1.46M/4.98G [00:07<1:34:30, 877kB/s] \u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  32% 1.61G/5.00G [00:07<00:13, 251MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  22% 1.07G/4.92G [00:08<00:20, 186MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  34% 1.68G/5.00G [00:08<00:13, 241MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  23% 1.14G/4.92G [00:08<00:19, 193MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  36% 1.81G/5.00G [00:10<00:30, 106MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  25% 1.21G/4.92G [00:10<00:49, 75.0MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  39% 1.95G/5.00G [00:10<00:19, 158MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  29% 1.41G/4.92G [00:10<00:23, 149MB/s] \u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  40% 2.02G/5.00G [00:10<00:16, 182MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  31% 1.54G/4.92G [00:11<00:16, 208MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  43% 2.15G/5.00G [00:11<00:11, 250MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  33% 1.61G/4.92G [00:11<00:15, 215MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  44% 2.22G/5.00G [00:11<00:10, 271MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   1% 68.6M/4.98G [00:11<05:49, 14.0MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  36% 1.75G/4.92G [00:11<00:12, 251MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  46% 2.28G/5.00G [00:11<00:11, 239MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  37% 1.81G/4.92G [00:12<00:13, 232MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  47% 2.35G/5.00G [00:12<00:11, 221MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  38% 1.88G/4.92G [00:12<00:13, 221MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  48% 2.42G/5.00G [00:12<00:13, 188MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  40% 1.95G/4.92G [00:14<00:36, 81.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  51% 2.55G/5.00G [00:14<00:25, 96.8MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  44% 2.15G/4.92G [00:14<00:17, 158MB/s] \u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  54% 2.69G/5.00G [00:14<00:15, 149MB/s] \u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  45% 2.21G/4.92G [00:15<00:14, 180MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  55% 2.75G/5.00G [00:15<00:13, 161MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  46% 2.28G/4.92G [00:15<00:12, 205MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  48% 2.35G/4.92G [00:15<00:10, 238MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  59% 2.95G/5.00G [00:15<00:07, 261MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60% 3.02G/5.00G [00:15<00:06, 290MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  49% 2.42G/4.92G [00:15<00:11, 213MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  62% 3.09G/5.00G [00:16<00:08, 216MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   3% 136M/4.98G [00:16<05:37, 14.3MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  52% 2.55G/4.92G [00:16<00:09, 237MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  64% 3.22G/5.00G [00:16<00:07, 246MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  53% 2.62G/4.92G [00:16<00:10, 222MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  55% 2.69G/4.92G [00:18<00:25, 86.7MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  66% 3.29G/5.00G [00:18<00:17, 96.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  57% 2.82G/4.92G [00:19<00:15, 138MB/s] \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   3% 137M/4.98G [00:19<08:22, 9.64MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  70% 3.49G/5.00G [00:19<00:09, 162MB/s] \u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   4% 204M/4.98G [00:19<03:56, 20.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  60% 2.96G/4.92G [00:19<00:10, 184MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  71% 3.56G/5.00G [00:19<00:07, 181MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   5% 271M/4.98G [00:19<02:15, 34.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  63% 3.09G/4.92G [00:19<00:07, 254MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  64% 3.16G/4.92G [00:19<00:07, 249MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  72% 3.62G/5.00G [00:19<00:07, 174MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  67% 3.29G/4.92G [00:20<00:05, 315MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  68% 3.36G/4.92G [00:20<00:05, 307MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  74% 3.69G/5.00G [00:20<00:08, 161MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  70% 3.42G/4.92G [00:20<00:05, 288MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  75% 3.76G/5.00G [00:20<00:07, 165MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  71% 3.49G/4.92G [00:20<00:05, 275MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  77% 3.83G/5.00G [00:23<00:16, 70.2MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  72% 3.56G/4.92G [00:23<00:16, 82.9MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  81% 4.03G/5.00G [00:23<00:06, 142MB/s] \u001b[A\n",
      "model-00002-of-00004.safetensors:  83% 4.16G/5.00G [00:23<00:04, 202MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  76% 3.76G/4.92G [00:23<00:07, 151MB/s] \u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  85% 4.26G/5.00G [00:23<00:03, 230MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  79% 3.89G/4.92G [00:23<00:04, 207MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  82% 4.03G/4.92G [00:23<00:03, 284MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  85% 4.18G/4.92G [00:24<00:02, 315MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  88% 4.40G/5.00G [00:24<00:03, 195MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   6% 297M/4.98G [00:24<04:51, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  86% 4.25G/4.92G [00:24<00:02, 259MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  89% 4.47G/5.00G [00:25<00:02, 181MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  88% 4.32G/4.92G [00:25<00:02, 226MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:   8% 416M/4.98G [00:27<03:02, 25.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  91% 4.53G/5.00G [00:27<00:05, 86.5MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  89% 4.38G/4.92G [00:27<00:05, 91.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  11% 560M/4.98G [00:27<01:31, 48.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  93% 4.67G/5.00G [00:27<00:02, 130MB/s] \u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  92% 4.52G/4.92G [00:27<00:02, 140MB/s] \u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  95% 4.65G/4.92G [00:27<00:01, 203MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  95% 4.73G/5.00G [00:27<00:01, 146MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  96% 4.71G/4.92G [00:28<00:01, 198MB/s]\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  96% 4.80G/5.00G [00:28<00:01, 149MB/s]\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  14% 672M/4.98G [00:28<01:08, 62.6MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  97% 4.78G/4.92G [00:28<00:00, 199MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  16% 773M/4.98G [00:28<00:49, 85.3MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  97% 4.87G/5.00G [00:28<00:00, 142MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors:  99% 4.85G/4.92G [00:28<00:00, 178MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  17% 840M/4.98G [00:29<00:44, 93.1MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors:  99% 4.93G/5.00G [00:29<00:00, 136MB/s]\u001b[A\n",
      "\n",
      "model-00003-of-00004.safetensors: 100% 4.92G/4.92G [00:31<00:00, 156MB/s] \n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  19% 931M/4.98G [00:31<01:03, 64.2MB/s]\u001b[A\u001b[A\u001b[A\n",
      "model-00002-of-00004.safetensors: 100% 5.00G/5.00G [00:31<00:00, 159MB/s] \n",
      "\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  20% 1.01G/4.98G [00:31<00:45, 87.0MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  22% 1.11G/4.98G [00:31<00:31, 122MB/s] \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  24% 1.18G/4.98G [00:31<00:25, 146MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  25% 1.26G/4.98G [00:32<00:19, 190MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  29% 1.46G/4.98G [00:32<00:10, 344MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  33% 1.66G/4.98G [00:32<00:06, 496MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  36% 1.79G/4.98G [00:32<00:05, 541MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  39% 1.93G/4.98G [00:32<00:04, 627MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  42% 2.07G/4.98G [00:33<00:06, 447MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  44% 2.20G/4.98G [00:35<00:20, 132MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  50% 2.46G/4.98G [00:36<00:11, 226MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  54% 2.70G/4.98G [00:36<00:06, 328MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  58% 2.89G/4.98G [00:36<00:04, 429MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  61% 3.02G/4.98G [00:36<00:03, 505MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  64% 3.18G/4.98G [00:36<00:03, 577MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  68% 3.38G/4.98G [00:36<00:02, 746MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  71% 3.52G/4.98G [00:36<00:01, 832MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  74% 3.67G/4.98G [00:37<00:02, 644MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  77% 3.84G/4.98G [00:37<00:02, 516MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  80% 3.98G/4.98G [00:38<00:02, 445MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  83% 4.11G/4.98G [00:38<00:02, 421MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  85% 4.24G/4.98G [00:38<00:01, 383MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  87% 4.31G/4.98G [00:39<00:01, 346MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  88% 4.37G/4.98G [00:39<00:01, 317MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  89% 4.44G/4.98G [00:39<00:02, 261MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  91% 4.51G/4.98G [00:40<00:01, 284MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors:  96% 4.78G/4.98G [00:40<00:00, 557MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model-00001-of-00004.safetensors: 100% 4.98G/4.98G [00:40<00:00, 123MB/s]\n",
      "Fetching 4 files: 100% 4/4 [00:40<00:00, 10.12s/it]\n",
      "Loading checkpoint shards: 100% 4/4 [00:17<00:00,  4.32s/it]\n",
      "generation_config.json: 100% 187/187 [00:00<00:00, 1.51MB/s]\n",
      "Tokenizing...\n",
      "Map: 100% 3720/3720 [00:07<00:00, 514.51 examples/s]\n",
      "/content/task2_llama_lora_v3.py:278: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Training...\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 128009}.\n",
      "  0% 0/699 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "{'loss': 1.6699, 'grad_norm': 9.995765686035156, 'learning_rate': 2.5714285714285714e-05, 'epoch': 0.04}\n",
      "{'loss': 1.0029, 'grad_norm': 5.450831890106201, 'learning_rate': 5.428571428571428e-05, 'epoch': 0.09}\n",
      "{'loss': 1.0423, 'grad_norm': 7.149123668670654, 'learning_rate': 8.285714285714287e-05, 'epoch': 0.13}\n",
      "{'loss': 0.9668, 'grad_norm': 6.47587776184082, 'learning_rate': 0.00011142857142857144, 'epoch': 0.17}\n",
      "{'loss': 0.9254, 'grad_norm': 9.841341018676758, 'learning_rate': 0.00014, 'epoch': 0.22}\n",
      "{'loss': 1.0516, 'grad_norm': 4.245250225067139, 'learning_rate': 0.00016857142857142857, 'epoch': 0.26}\n",
      "{'loss': 1.0023, 'grad_norm': 4.068371772766113, 'learning_rate': 0.00019714285714285716, 'epoch': 0.3}\n",
      "{'loss': 1.1462, 'grad_norm': 11.05547046661377, 'learning_rate': 0.0001998989862839597, 'epoch': 0.34}\n",
      "{'loss': 1.0838, 'grad_norm': 5.996129035949707, 'learning_rate': 0.00019955006506197783, 'epoch': 0.39}\n",
      "{'loss': 1.1131, 'grad_norm': 4.952399253845215, 'learning_rate': 0.00019895285927673705, 'epoch': 0.43}\n",
      "{'loss': 1.1255, 'grad_norm': 7.4153971672058105, 'learning_rate': 0.00019810885839966532, 'epoch': 0.47}\n",
      "{'loss': 1.1265, 'grad_norm': 5.827591419219971, 'learning_rate': 0.00019702016742576212, 'epoch': 0.52}\n",
      "{'loss': 0.9584, 'grad_norm': 4.696887016296387, 'learning_rate': 0.00019568950162359924, 'epoch': 0.56}\n",
      "{'loss': 1.123, 'grad_norm': 5.804991722106934, 'learning_rate': 0.00019412017976325814, 'epoch': 0.6}\n",
      "{'loss': 0.9735, 'grad_norm': 3.8324623107910156, 'learning_rate': 0.0001923161158390939, 'epoch': 0.65}\n",
      "{'loss': 0.933, 'grad_norm': 4.938389778137207, 'learning_rate': 0.00019028180930797005, 'epoch': 0.69}\n",
      "{'loss': 0.884, 'grad_norm': 5.040443420410156, 'learning_rate': 0.00018802233386731008, 'epoch': 0.73}\n",
      "{'loss': 0.8878, 'grad_norm': 6.3159942626953125, 'learning_rate': 0.0001855433248009546, 'epoch': 0.77}\n",
      "{'loss': 1.0356, 'grad_norm': 4.903099060058594, 'learning_rate': 0.00018285096492438424, 'epoch': 0.82}\n",
      "{'loss': 0.9082, 'grad_norm': 7.036496162414551, 'learning_rate': 0.00017995196916436137, 'epoch': 0.86}\n",
      "{'loss': 0.6717, 'grad_norm': 3.8071272373199463, 'learning_rate': 0.00017685356781145113, 'epoch': 0.9}\n",
      "{'loss': 0.9313, 'grad_norm': 4.4903106689453125, 'learning_rate': 0.0001735634884871897, 'epoch': 0.95}\n",
      "{'loss': 0.8382, 'grad_norm': 4.321805477142334, 'learning_rate': 0.00017008993687087627, 'epoch': 0.99}\n",
      " 33% 233/699 [37:25<1:03:56,  8.23s/it]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "{'loss': 0.6393, 'grad_norm': 2.5110738277435303, 'learning_rate': 0.00016644157623405646, 'epoch': 1.03}\n",
      "{'loss': 0.6274, 'grad_norm': 15.23363208770752, 'learning_rate': 0.0001626275058337399, 'epoch': 1.07}\n",
      "{'loss': 0.497, 'grad_norm': 3.8026461601257324, 'learning_rate': 0.00015865723821824072, 'epoch': 1.12}\n",
      "{'loss': 0.5037, 'grad_norm': 2.2593066692352295, 'learning_rate': 0.00015454067550224192, 'epoch': 1.16}\n",
      "{'loss': 0.5917, 'grad_norm': 3.403982639312744, 'learning_rate': 0.0001502880846702545, 'epoch': 1.2}\n",
      "{'loss': 0.5119, 'grad_norm': 5.816162109375, 'learning_rate': 0.00014591007197006732, 'epoch': 1.25}\n",
      "{'loss': 0.5208, 'grad_norm': 5.053540229797363, 'learning_rate': 0.00014141755646005093, 'epoch': 1.29}\n",
      "{'loss': 0.4921, 'grad_norm': 3.214632272720337, 'learning_rate': 0.00013682174277629124, 'epoch': 1.33}\n",
      "{'loss': 0.4589, 'grad_norm': 2.938060760498047, 'learning_rate': 0.00013213409318747256, 'epoch': 1.37}\n",
      "{'loss': 0.5042, 'grad_norm': 2.707864999771118, 'learning_rate': 0.0001273662990072083, 'epoch': 1.42}\n",
      "{'loss': 0.5723, 'grad_norm': 3.302384853363037, 'learning_rate': 0.00012253025143511766, 'epoch': 1.46}\n",
      "{'loss': 0.4447, 'grad_norm': 2.707763433456421, 'learning_rate': 0.00011763801189937333, 'epoch': 1.5}\n",
      "{'loss': 0.4298, 'grad_norm': 3.8793277740478516, 'learning_rate': 0.00011270178197468789, 'epoch': 1.55}\n",
      "{'loss': 0.5375, 'grad_norm': 3.6267902851104736, 'learning_rate': 0.00010773387295076496, 'epoch': 1.59}\n",
      "{'loss': 0.3895, 'grad_norm': 3.58396315574646, 'learning_rate': 0.00010274667512711413, 'epoch': 1.63}\n",
      "{'loss': 0.4044, 'grad_norm': 2.645843744277954, 'learning_rate': 9.775262691080987e-05, 'epoch': 1.68}\n",
      "{'loss': 0.3895, 'grad_norm': 2.280693292617798, 'learning_rate': 9.276418379426702e-05, 'epoch': 1.72}\n",
      "{'loss': 0.3726, 'grad_norm': 3.7391929626464844, 'learning_rate': 8.779378729040438e-05, 'epoch': 1.76}\n",
      "{'loss': 0.4161, 'grad_norm': 5.005954265594482, 'learning_rate': 8.285383390267397e-05, 'epoch': 1.8}\n",
      "{'loss': 0.4181, 'grad_norm': 6.37537956237793, 'learning_rate': 7.795664420734679e-05, 'epoch': 1.85}\n",
      "{'loss': 0.3144, 'grad_norm': 1.9382960796356201, 'learning_rate': 7.311443212516668e-05, 'epoch': 1.89}\n",
      "{'loss': 0.398, 'grad_norm': 12.341818809509277, 'learning_rate': 6.833927445900944e-05, 'epoch': 1.93}\n",
      "{'loss': 0.3381, 'grad_norm': 1.632512092590332, 'learning_rate': 6.364308077352379e-05, 'epoch': 1.98}\n",
      " 67% 466/699 [1:14:58<31:40,  8.16s/it]/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "{'loss': 0.3671, 'grad_norm': 1.439076542854309, 'learning_rate': 5.903756369187525e-05, 'epoch': 2.02}\n",
      "{'loss': 0.2345, 'grad_norm': 2.005272150039673, 'learning_rate': 5.4534209683675727e-05, 'epoch': 2.06}\n",
      "{'loss': 0.2097, 'grad_norm': 2.0575804710388184, 'learning_rate': 5.014425041695518e-05, 'epoch': 2.1}\n",
      "{'loss': 0.1825, 'grad_norm': 2.293384552001953, 'learning_rate': 4.587863474562516e-05, 'epoch': 2.15}\n",
      "{'loss': 0.1615, 'grad_norm': 2.7152209281921387, 'learning_rate': 4.17480014023006e-05, 'epoch': 2.19}\n",
      "{'loss': 0.1649, 'grad_norm': 2.1039235591888428, 'learning_rate': 3.77626524645843e-05, 'epoch': 2.23}\n",
      "{'loss': 0.2048, 'grad_norm': 1.4928644895553589, 'learning_rate': 3.393252766099187e-05, 'epoch': 2.28}\n",
      "{'loss': 0.2033, 'grad_norm': 7.764355182647705, 'learning_rate': 3.026717958059998e-05, 'epoch': 2.32}\n",
      "{'loss': 0.1524, 'grad_norm': 2.715475559234619, 'learning_rate': 2.677574984824598e-05, 'epoch': 2.36}\n",
      "{'loss': 0.1451, 'grad_norm': 1.476837158203125, 'learning_rate': 2.3466946324700312e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1844, 'grad_norm': 4.765015602111816, 'learning_rate': 2.0349021388675537e-05, 'epoch': 2.45}\n",
      "{'loss': 0.1537, 'grad_norm': 3.9230315685272217, 'learning_rate': 1.7429751354838385e-05, 'epoch': 2.49}\n",
      "{'loss': 0.1796, 'grad_norm': 3.753117084503174, 'learning_rate': 1.4716417079157541e-05, 'epoch': 2.53}\n",
      "{'loss': 0.0847, 'grad_norm': 0.2945578694343567, 'learning_rate': 1.2215785799958645e-05, 'epoch': 2.58}\n",
      "{'loss': 0.1328, 'grad_norm': 2.398322105407715, 'learning_rate': 9.934094259976567e-06, 'epoch': 2.62}\n",
      "{'loss': 0.143, 'grad_norm': 0.3330670893192291, 'learning_rate': 7.877033151499081e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1, 'grad_norm': 2.352938413619995, 'learning_rate': 6.049732923397267e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1753, 'grad_norm': 2.0033788681030273, 'learning_rate': 4.456750985440738e-06, 'epoch': 2.75}\n",
      "{'loss': 0.1311, 'grad_norm': 2.623522996902466, 'learning_rate': 3.1020603418108417e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1725, 'grad_norm': 2.3985815048217773, 'learning_rate': 1.9890396821608203e-06, 'epoch': 2.83}\n",
      "{'loss': 0.2313, 'grad_norm': 1.5582118034362793, 'learning_rate': 1.1204649549363844e-06, 'epoch': 2.88}\n",
      "{'loss': 0.124, 'grad_norm': 1.247121810913086, 'learning_rate': 4.985024439734365e-07, 'epoch': 2.92}\n",
      "{'loss': 0.1687, 'grad_norm': 1.0138194561004639, 'learning_rate': 1.2470336564037732e-07, 'epoch': 2.96}\n",
      "{'train_runtime': 6750.4743, 'train_samples_per_second': 1.653, 'train_steps_per_second': 0.104, 'train_loss': 0.5492107123776055, 'epoch': 3.0}\n",
      "100% 699/699 [1:52:30<00:00,  9.66s/it]\n",
      "Running Inference...\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "[dev] 0/345 | Gen: Explicit\n",
      "Evasion label: Explicit\n",
      "\n",
      "This answer -> Pred: Explicit\n",
      "[dev] 50/345 | Gen: Explicit\n",
      "\n",
      "From the list above, which single evasion -> Pred: Explicit\n",
      "[dev] 100/345 | Gen: Partial/half-answer\n",
      "Evasion label: Partial -> Pred: Partial/half-answer\n",
      "[dev] 150/345 | Gen: Implicit\n",
      "\n",
      "The answer acknowledges that busing has been -> Pred: Implicit\n",
      "[dev] 200/345 | Gen: Explicit\n",
      "Evasion label: Explicit\n",
      "\n",
      "This answer -> Pred: Explicit\n",
      "[dev] 250/345 | Gen: Claims ignorance: The speaker claims he doesn't remember -> Pred: Claims ignorance\n",
      "[dev] 300/345 | Gen: Implicit\n",
      "Implicitly, the answer suggests that the -> Pred: Implicit\n",
      "\n",
      "=== FINAL MACRO F1: ===\n",
      "0.36576375579471554\n",
      "Predicting Test Set...\n",
      "[test] 0/308 | Gen: Dodging\n",
      "The answer does not provide any direct -> Pred: Dodging\n",
      "[test] 50/308 | Gen: General\n",
      "From the list above, which single evasion -> Pred: General\n",
      "[test] 100/308 | Gen: General\n",
      "From the list above, which single evasion -> Pred: General\n",
      "[test] 150/308 | Gen: Explicit\n",
      "\n",
      "Note: This answer was given during Barack -> Pred: Explicit\n",
      "[test] 200/308 | Gen: Deflection\n",
      "Evasion label: Dodging -> Pred: Deflection\n",
      "[test] 250/308 | Gen: Implicit: Implicitly communicates trust and confidence in Secretary -> Pred: Implicit\n",
      "[test] 300/308 | Gen: Clarification\n",
      "* Clarification: Clarification -> Pred: Clarification\n",
      "Done!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/SemEval_Clarity/v3_predictions.csv'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1088707456.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdestination\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SemEval_Clarity/v3_predictions.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./v3_results_local/predictions_v3.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Success! Predictions saved to {destination}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m                     \u001b[0;31m# macOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0m_HAS_FCOPYFILE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/SemEval_Clarity/v3_predictions.csv'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "0zXzvBShkNOZ"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}