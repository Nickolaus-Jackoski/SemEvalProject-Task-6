{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mBsbu09rsjo5"
   },
   "source": [
    "Trying different seeds to Find the Best seed For task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f11ea093a50a47f498565f1761917edf",
      "2aedc4d29dd948c6839ce3088155efd5",
      "dc93e6a83c984772a9d925bd7e1e41ee",
      "7c4199edd0b04b9d84f5b941957c37e5",
      "d9c29171c2a34b8aafc96990ab71b9ab",
      "5e9cd2dcab214e479724feed87aa979c",
      "fc8767862a854ca3ba9ddbe74151a962",
      "0ab4f7b96e814bc2a30d5b3ccee38588",
      "2c8f0989b6394329a6d503c98d987f6e",
      "ac26b6d93caa44358e1632beaebcd0d6",
      "8e23a4fd9b334d04b8798ccb597821cd",
      "85d6bde2874c4259962edb7b9e80a2ec",
      "4573fd3c83e14cefbf5a3c49cc6a5e84",
      "cc1b92ccdc5643449aebbc51fa4d397e",
      "8c916b03930a4b63a4b973b0fd412263",
      "f6718c341e5d4ad08b37478e6313baac",
      "7dfeff61e70445cb85a00a823acd2d6c",
      "66580990f8db42a99a39049a8566fc49",
      "05242f9bdcc648e7901f4aa70c1117ce",
      "6c7dbbc17e9f43d5acb0c09925ffab56",
      "1f62d336363644b6b10b3d741a111569",
      "470cfd2a7bd44aa88331d9de90b684d0",
      "5fae6d70ac04452dae9e557088940a10",
      "727dedbbf22d424a9ac345ed18496fda",
      "bc8f896902b343efa0b6209fda904739",
      "30e28a1a6a0e4dd7b24e5a1dc61dcee0",
      "7451ffc58ebb445eaf91b8e1353fd1bf",
      "31e36fde8d5740ea931639581d9679b3",
      "f1e2c538909a44f79852f997c5e14ab4",
      "e495938bb5df4025a6e8b3ee092e0aee",
      "b26176c0fb3a4247be9bfaf04eb372e4",
      "146daacdcae546e58d3879da242e0ace",
      "e0be08d2f8704b6bba947a5de6f893e9",
      "13469e6f59b84c83b2f1a69f2c3dd09f",
      "064175e154254a3d9ced9412a6068760",
      "d507f4a462314278b3636cf836e92505",
      "d4d66f2e868640e4a0f3b9d84c0219d4",
      "439d88d27153481dbac24da879266521",
      "d0942dffb18a483cb23ddfa1f3140d7e",
      "f05c5fe1977d4d40979262e7e8ba0a61",
      "1d8dc5f8d2884f17ba5379bb153f538e",
      "3a2d2404da614ed496c38fb0dba4804d",
      "51c1c4022780435e81c555a94be73985",
      "712bb79f61c64413a746310a2504b77a",
      "1d7dd19dce9c498ca39720363689399a",
      "271d564da3c947ccaf635f99774c915a",
      "cd18c24bfdcf43adaa76de2ac86bbae8",
      "d659de4b019648eaa52baba7a026eb84",
      "17fb46f1acb440249a5fe3527930b7d9",
      "96634aa675f645798b18a5d2d41ef4ef",
      "00ff6c1bcf2e4ff8bed895c99b144beb",
      "f03f96ea11454ad68bd72e48325b3e85",
      "9ed4c727bda34ee8a119d1f6ff937fbc",
      "2edce6e70af8411e8bfaf9e0feebf3e4",
      "37101af454534c1da546fd05f3861da3",
      "92829a009cae474cbfc3cbdbb44bdc3d",
      "7c4fd86257dc4589b1d70fea7cdbc1b0",
      "d0a9491ead51411080c41cda145f5384",
      "f45e4faf12944799b147edc474a12a94",
      "2af0543be4564905ae5b07a0a7315425",
      "c30bbf2bdf574cb7bc6efe67da6dcfec",
      "8de13c7c0d2c45c3948db874c0ab16ec",
      "6c1f431edd8c44429c749498678c7630",
      "8064edfb7e4349d6b1afdaa07be7f36b",
      "b387311bbc35446fb6f1adeb42c53abc",
      "c676ed80e97b492daa56f4bcafffd3cc",
      "2ffe878fb8f34a38b377f5ec62caaf5d",
      "c1af88610e0c409490966798e135b8a4",
      "3de1fb58dfa94113af9d9ff6f369174b",
      "193e4562cf134b1dbfabeda0f57979e0",
      "b8e2492927ac4f63b2aa3153252ca82d",
      "012e88eaf8244e658a5e5b782172da9a",
      "57bf2555c9344276b88309885c7973c9",
      "f035d80bbab54c62961de1eb21bee54f",
      "0dc119fb5d80452dab32f4766e481f30",
      "568129d1a5f34e9a8e16021ee8b50dc7",
      "a5926eff25214e2089d342f2ba110165",
      "e7443771db2f45e18a792f4512eac53d",
      "5276967b09134b5cb6404d7e1365f1fa",
      "686de09e4b34446db6fd20cdfa1dcccb",
      "5d3426052aab46ae8e082c7008a6820e",
      "480ae0940fde4de7b86b64767bb50730",
      "b0ebacaf6cf4472fb718335803c721ce",
      "ae1f70b8f107461aac9585373af0ec50",
      "cf53cb30d13a430197bf990e3a8ee299",
      "65a1989d867544939bfff5e743ce2003",
      "bc512eefc5cf4591bef13bccc0be2988",
      "fcd6b3bc82614a53afb2c25683488cb4",
      "1e6810236f384558b9c4a079f03be751",
      "e26c9f3d947d4d8683bd0e3c2121183f",
      "b468747966504492949e28a15c59f6e8",
      "321b911cdf3c4489a2097404568277e9",
      "25d863e5eccf4f54a2586a5b1195e2c6",
      "2cd518a3bb8b43d0afc43e78431715c0",
      "90c86e486b174e26b3d902d85a5d8aaa",
      "442c2e920a164a4f8589d0d5f9d1d696",
      "b686331199284f86ac53df093488998f",
      "06d050df61af471d8748065ff5e9928d",
      "16e41f5fe5484744b88c3c5061a2b2bd",
      "efb7c3b1422e467b9cc9a8b74a5f8088",
      "1020585ec63d43edbad7ae03db326d69",
      "94b0374302c34901b93327a720158453",
      "128c11a7c8a144ebbdab47ddeca8a1f8",
      "93fe9314d84c42bfb817f1deffc15d38",
      "f82f87ae454a48f2bb44d80ce7e17473",
      "eeaafcec019a435ebce245c9fdbfa4d7",
      "8e37be5d56e843789cbf1be70750264a",
      "7c43765319ad49c18ddaffa06d9b50de",
      "95cd21e6aafb429d97008aa86dd918a1",
      "76ab52a4ddbd402c9c4cfd98f974df43",
      "7cc68c99d6eb44519e497d97f40d8ac7",
      "92b6ee5aac4b45cbba676dfefdd401e1",
      "9b62bd91e8c04f2492718eb42fba8fdf",
      "db44273a76bf429d95b3d46767d3b18e",
      "209756e7d80340dc8eb67de4279e2fcf",
      "d78447ffc7cf4741bfa43ce35af5ae0e",
      "644460421ef14d38b63e7c741c78a001",
      "51c31e32139d4da29837544ddbc47d8d",
      "2cfb9e69e5674c08afe401d28745d22c",
      "d2ad028b72164c5a85e83d2d02d45653",
      "8636904fd9564748a675df4e6c90eb9a",
      "a83303a9844944839493a7e06155d4bf",
      "8580f3ea8b084bc08d0f3dd22d349b4b",
      "98be270e410a40639d03bd55d9d71fdc",
      "f05a6f85b4bf41739cc9a2f3c887ba76",
      "300306463da44affb218b8dfb34ae42c",
      "edbcc67619364ad5907c5d9b9f1ffaf9",
      "fa920c7391604abaae5768f2c8111d7e",
      "bc9ba1341b904820bc131905c65e4099",
      "6609c23c104f412fb2c708b64f238445",
      "c4bd3d7ae52241c18a4a2c3950630927",
      "7f56cea584d149b2aaf2d2627d363545",
      "34e0421f4b564daa986886fd614cb898",
      "eb1660fd2f3542d481bd38ec43cd5a14",
      "ffec2cb51ca6484bb1980f9a188cc274",
      "6648a04d81fc447a90331112bb4b95b5",
      "eedf1f2efcfd4532b1dc1eed6ba1dccb",
      "e15955259c4b4325a7e94da7f227f76f",
      "bf4c1d60ff79404d989b42cd16b1ef2b",
      "c0d0c75a3d8a40f1a2c8f9f3133127f7",
      "b1b2cf1693334b89b2960175f5a254ee",
      "3428d80d40594fbda7dadec58d018f56",
      "62b850e57706451c9823b2724b97caa8",
      "babfaf7640964b8bbc5c7355e84008c6",
      "1e1c7015372e4a4ebe1e5e93277d0c7e",
      "0e193cbc28b140f6a7294421300a98fa",
      "40c0e7be8a6641e6a0e1bea6a980e024",
      "a2cfeeae566e4b84b31e23f5b9506b6c",
      "603a1250ec9f4525ad181587ef649911",
      "f01b787c6fc14160a1695ce69b2eb06a",
      "fe980283cf16417382a4839bd3b77914",
      "ca542e7f53e644619ec3920e8a1da242",
      "3ef31a6c97894c6085c8ca291c463712",
      "c518ed6a14f242ea84789fe752c18578",
      "545f168b8adf4f82a7f4abdfd89c04e6",
      "ffef6411eb5e40dbb7f3d96b4bd82985",
      "c7d1f5646a974a12aeb01d103ced2a83",
      "7bc4ed995d4d4dd7bdca86e1a3ca3346",
      "2f33f32d5b464ca69245b9819169cb8d",
      "f3b47ede976343bd8698641d6c286d86",
      "5d1bee4b046547599dc8fa28ac9b5aa3",
      "9904548eb9714bb58a7cf3cdf3c7aee5",
      "48f9239bd7ce4b45ae1d62496464ee3d",
      "929d87450c7749cea7299585a40b35a5",
      "bf6cbefc01ba4016a7087c47a7765b95",
      "e5a958c1156a4158b51c2ed81dc8b216",
      "fd02c729bd424c69932e66ae3eec4b2e",
      "8beb3ad13a3a45daa1711d87496ef23b",
      "e5b1e98452014c9d9e4edf5779e478f7",
      "9b182d388e1b4f81a08f9604589b42d5",
      "edff1982521d46e29cd0681aea177880",
      "acfdf5c3bf56497dae9be7abf0a571e0",
      "d5c056669aff41a1991f34ad6f152747",
      "229b0fec2f82439eaba8fceeb38ed750",
      "eb263c6cd1544e9ba4ee16284fdcae6e",
      "854eafe95ee34552aff23fc5937456ff",
      "66c619b2343f4ccd89680ca55b812bd5",
      "af4a2b22eba84b31862ac1386a5d4b8f",
      "3d6a276dd5c149618360662e7d1e5d8d",
      "f40508fffaa249f2b941cd3380d09822",
      "581ef969513c474b98ac89c5fb0cd36f",
      "267a068352e54a38ae1d5de21f3071ec",
      "f06449eb3ba6417bb0e7538eec245860",
      "e87c2dd6b86945c4afdf9eadcd37cfe9",
      "102bec0a7e044f86bb389338b8cc827b",
      "827df4c72a0e4fb094632036bbc1ae6b",
      "c861c6d116104a8a8a3aed02e7a55eee",
      "9ba47c9235f249b1917b705f915d7475",
      "3ccc8437b9b449ceae011d7395babee8",
      "9677bcb5a5bb43c784ce304022c2858f",
      "f3958169bcdc473fa8b9e2567225fd4c",
      "ec5293f832d843c3a8880fcbbaa75504",
      "443cd0fb62aa4783a3e854abd829c41e",
      "f182805fdcf244caa43f2b83a4216331",
      "40e0293a34ac4ef68ebbcb656b0b9a2c",
      "b03dfb7fa7984d9ba4671c48c0deb139",
      "e3adabf54ac84bc78a352b1e9ce5fb6f",
      "6763d5ce6e9b4b7c8b9443670a949d6d",
      "4101b3c1eb634683b52a956fbb2dceec",
      "d9dd571183bd4277b290684c574891ca",
      "6e01fbee42ba4ed6b26e0cbbe0606417",
      "6534ff2588054992a61a13ecb5300538",
      "d91d9e10e15e4ecfbc4bb7e4b0249cc1",
      "24a70a33ebcd4fd1938091de85efd97e",
      "279ebb25dac74c1f91e58b38ac6ea327",
      "1f7f3857ea1a419ba1c9d16cdd03efec",
      "ef9602ce2adb406799640d2da25f67f5",
      "9c20b419717a401c8ac74356028770f1",
      "c92b79182f124b29a3730e83b91bca48"
     ]
    },
    "id": "pQjbcV3ww-_4",
    "outputId": "9ed926d0-c18a-4b80-ebea-1abeb2de9faf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11ea093a50a47f498565f1761917edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d6bde2874c4259962edb7b9e80a2ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fae6d70ac04452dae9e557088940a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13469e6f59b84c83b2f1a69f2c3dd09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7dd19dce9c498ca39720363689399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92829a009cae474cbfc3cbdbb44bdc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffe878fb8f34a38b377f5ec62caaf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7443771db2f45e18a792f4512eac53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6810236f384558b9c4a079f03be751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efb7c3b1422e467b9cc9a8b74a5f8088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc68c99d6eb44519e497d97f40d8ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83303a9844944839493a7e06155d4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e0421f4b564daa986886fd614cb898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babfaf7640964b8bbc5c7355e84008c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545f168b8adf4f82a7f4abdfd89c04e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a958c1156a4158b51c2ed81dc8b216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c619b2343f4ccd89680ca55b812bd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba47c9235f249b1917b705f915d7475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4101b3c1eb634683b52a956fbb2dceec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1925' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1925/2625 23:14 < 08:27, 1.38 it/s, Epoch 11/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.948397</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.322958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.900236</td>\n",
       "      <td>0.559486</td>\n",
       "      <td>0.298461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.100200</td>\n",
       "      <td>1.818542</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.420284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.100200</td>\n",
       "      <td>1.751822</td>\n",
       "      <td>0.591640</td>\n",
       "      <td>0.525598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.100200</td>\n",
       "      <td>1.766358</td>\n",
       "      <td>0.591640</td>\n",
       "      <td>0.513822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.696600</td>\n",
       "      <td>1.814388</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.587137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.696600</td>\n",
       "      <td>1.848479</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.591121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.696600</td>\n",
       "      <td>1.975815</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.586729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.428800</td>\n",
       "      <td>2.090328</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.539135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.428800</td>\n",
       "      <td>2.068061</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.587726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.428800</td>\n",
       "      <td>2.111578</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.579064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5699\n",
      "Final accuracy: 0.6145\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663,
     "referenced_widgets": [
      "74be094acc814c289767ccf82c523021",
      "bad8a419439f479792691133b02893e4",
      "a27837fce2e84c05bb965d2fd55186fd",
      "0efe2dac1b754eb1a07195674b73fdf4",
      "c317aff0c73444f1a714335c3e22bb05",
      "05684b50991b45c2b82b6607d73f9c58",
      "79572aa9a2a0413b9fa6e3e5896aa3d3",
      "dd976303a7d7420581312986a92dc5f5",
      "6a85ffaf019b4c70beb8c79715f49ae8",
      "e6f5ec70a10140b4b87c7c535c5a01fc",
      "950e57611ec64ed58d3fe1201a098850",
      "8b214abe087e43c690fc1dc5865fbffa",
      "c375f2bf877149a9914a2617822b86e0",
      "fc04432eeac14080807e9c065f947231",
      "ae2a8a2030634416802372ca0e9d4878",
      "5c77544a328e4854ad00560b9d6b9226",
      "3125b4ce5fdd44f8944f772a3b58b379",
      "f76dc378a5184765b2a33ef4f54d0b05",
      "80347535be41482c99d33368594d6fc0",
      "b0bdbb4b8dd145e2bb62d01c181e5fa3",
      "bb2fe9cf52d24246b6062976d08872c1",
      "31efedddfb3e4cc38e8c0ba6c760e54e"
     ]
    },
    "id": "YVUucq8I1fMJ",
    "outputId": "e77ffe9b-3c2e-4fe9-c5a7-fea187094046"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74be094acc814c289767ccf82c523021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b214abe087e43c690fc1dc5865fbffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2100' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2100/2625 25:53 < 06:28, 1.35 it/s, Epoch 12/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.948949</td>\n",
       "      <td>0.520900</td>\n",
       "      <td>0.331406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.894140</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.300257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.096400</td>\n",
       "      <td>1.810067</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.426659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.096400</td>\n",
       "      <td>1.769935</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.492653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.096400</td>\n",
       "      <td>1.755671</td>\n",
       "      <td>0.591640</td>\n",
       "      <td>0.532261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.704500</td>\n",
       "      <td>1.828986</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.563667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.704500</td>\n",
       "      <td>1.917431</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.579545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.704500</td>\n",
       "      <td>1.921234</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.590356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>2.061334</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.554631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>2.170806</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.564979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>2.244513</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.575906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.159200</td>\n",
       "      <td>2.321379</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.560193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5761\n",
      "Final accuracy: 0.6116\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 631,
     "referenced_widgets": [
      "ecc92e97e60543c5904f2c3e6f67997b",
      "577bbf2c109644fc984f15f6dab60705",
      "062e81009a6c43da911b0a40d859becb",
      "0ada9dc74e3642a1aba8dd87b0d60103",
      "c56b6027f19a461a9cf570cd499febd2",
      "a07f48be670a409ebe84cbe9fe7f797b",
      "df355e04558247ffb835509d1bb0515c",
      "c1e0433f5c2245cb9b584797f668aa8b",
      "ecf7d5a457b94f2b9b8ddb14025d3543",
      "bdd58199efc7443d9c8faa6b49c41c33",
      "84eb93da2e6241f586d6fd12c41aabc2",
      "179f8ced49944456b64741f077037d58",
      "e55cebd995704c9baa681c299cd0206e",
      "13f57c73baf54b70910ffba9865bed0d",
      "3de4c726541a4ac88e0442883be03b83",
      "a0c816c4acc04f2fac37da986f5b9af6",
      "23f7bbe6872d4f419a56f333803c81fc",
      "721c45896ba44219a7bb298d26f881eb",
      "331a25baded14887ad7747b51561a3f3",
      "58db270b6f2f43c7a543b7061c4ab87a",
      "f043129186c741c6bbbccb350a6430b7",
      "3832a50360aa45eab5943dd90495e1ab"
     ]
    },
    "id": "aovjwuhy1fp1",
    "outputId": "93846a31-a0e1-4eba-f58c-676cf42bdb83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc92e97e60543c5904f2c3e6f67997b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179f8ced49944456b64741f077037d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1925' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1925/2625 23:35 < 08:35, 1.36 it/s, Epoch 11/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.952104</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.298867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.881836</td>\n",
       "      <td>0.530547</td>\n",
       "      <td>0.355114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.099800</td>\n",
       "      <td>1.792232</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.453506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.099800</td>\n",
       "      <td>1.754387</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.557207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.099800</td>\n",
       "      <td>1.740115</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.538358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.700500</td>\n",
       "      <td>1.796460</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.567378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.700500</td>\n",
       "      <td>1.856348</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.588237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.700500</td>\n",
       "      <td>1.982192</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.554325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.423300</td>\n",
       "      <td>2.049776</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.568475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.423300</td>\n",
       "      <td>2.078332</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.588120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.423300</td>\n",
       "      <td>2.169920</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.582421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5883\n",
      "Final accuracy: 0.6377\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "id": "bM8ygfoF1gHj",
    "outputId": "23f264aa-1437-44dd-8210-16cdfbb81e6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2275' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2275/2625 28:17 < 04:21, 1.34 it/s, Epoch 13/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.948488</td>\n",
       "      <td>0.514469</td>\n",
       "      <td>0.326850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.901810</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.320731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.098300</td>\n",
       "      <td>1.829542</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.424648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.098300</td>\n",
       "      <td>1.780861</td>\n",
       "      <td>0.569132</td>\n",
       "      <td>0.492306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.098300</td>\n",
       "      <td>1.726985</td>\n",
       "      <td>0.594855</td>\n",
       "      <td>0.543635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>1.821057</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.545668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>1.875029</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.549112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.713200</td>\n",
       "      <td>1.928129</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.607936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.397300</td>\n",
       "      <td>1.996667</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.613192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.397300</td>\n",
       "      <td>2.079600</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.606348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.397300</td>\n",
       "      <td>2.158153</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.585005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.148300</td>\n",
       "      <td>2.224424</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.596411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.148300</td>\n",
       "      <td>2.243670</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.598584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.6011\n",
      "Final accuracy: 0.6464\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "8wrw6tzW1gcN",
    "outputId": "f309fbbe-1370-4655-9292-b91e4883b8f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 33:04, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.950180</td>\n",
       "      <td>0.508039</td>\n",
       "      <td>0.310358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.914850</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.304123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.103400</td>\n",
       "      <td>1.844788</td>\n",
       "      <td>0.556270</td>\n",
       "      <td>0.412314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.103400</td>\n",
       "      <td>1.788387</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.448916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.103400</td>\n",
       "      <td>1.745798</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.535222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.741400</td>\n",
       "      <td>1.779895</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.555152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.741400</td>\n",
       "      <td>1.838009</td>\n",
       "      <td>0.610932</td>\n",
       "      <td>0.560332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.741400</td>\n",
       "      <td>1.924872</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.586628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.427700</td>\n",
       "      <td>2.061715</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.588561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.427700</td>\n",
       "      <td>2.089193</td>\n",
       "      <td>0.649518</td>\n",
       "      <td>0.597052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.427700</td>\n",
       "      <td>2.171653</td>\n",
       "      <td>0.633441</td>\n",
       "      <td>0.577986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>2.253817</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.589223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>2.267046</td>\n",
       "      <td>0.652733</td>\n",
       "      <td>0.601494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.150300</td>\n",
       "      <td>2.275890</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.595944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.065400</td>\n",
       "      <td>2.281318</td>\n",
       "      <td>0.646302</td>\n",
       "      <td>0.596334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.563\n",
      "Final accuracy: 0.6029\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "30d96c18a29c470e9d249fa256777f1b",
      "fb9a1608ac2a4c16bff735b4d6b63f89",
      "d1a8732d14d34b249fb286fd520a7f9f",
      "fea2db1269af4c7394dfac8534d5fd99",
      "4288bbbfb9dd470da1bbff0cd02bf2e3",
      "bae31588b8c44ccc826b64c0584ad3db",
      "a17c2a4dd7dc4e9580980656f01b27fa",
      "9edf21e8753f4ebd8cb464ebaa64ef4d",
      "892e9be978e4473cbe8b359bbeb8e3e5",
      "e2ab1487135e4707a1375cbed78e960f",
      "f7ac6867fd034f98aa066cb2fa27c7bf",
      "35f38fac3a834a42ad0334405da473f5",
      "673b896baf5b4a13b60d22e0eee82011",
      "5a2527c3985744bd92535f244fef0b4c",
      "5ed847f7f8304e329fe882bd932ab74d",
      "bd9ca047084f4e0cbcd6ccac2ce156f4",
      "7df305f77a4647ee8cdec11212ea3740",
      "29d05a22cecb4d099f1f528d72ad318f",
      "4f7f955ff4434c13893a538c4e4aa3e0",
      "d74155c20ecd4b1a8a3aab6f2765c9f1",
      "79917655032b46b382b4b00faf6e7ff1",
      "30d7b087c5e14cfbb65bc9216399e60e",
      "1d68a8c6454f4ae88cfadc600966fbe8",
      "b298c1bda6f444de8abed4f5f1f6ec4b",
      "635613d665db4a32a4decf182c8292ce",
      "3e900ffb69354957a10a3da109caecde",
      "2a73f3418473446ab12e3c5b688ac0a2",
      "6f4ee5617ee74ff4bc0a602bf311cec1",
      "46d3b9f037f640409f2aff546e68ab86",
      "16c0a6cc2f1142ad875c07106dd8b5ac",
      "2a17bd37ef214ca7b79cf99233689344",
      "a93d2862a848499198fd687580a67e3e",
      "64c1a6b6e0ab48718945f287a0a61179",
      "72efbfeb861a47bc92e8565a84ef0a8f",
      "edd06ff68f9d41d487ea3396a6a01293",
      "d42fd87378e444879476bb6d86df526f",
      "122a92a4a06745568f076cc90a7dacd6",
      "fd94e199d41447b89075bbda92332a22",
      "d65eb46613c74691b1c1abea7dfccde5",
      "43dc6d723f9b49568a2463873e2eed02",
      "f708cdaaf05c4b6baac31037b2464f8b",
      "b0540aa0876a4e41a71794203a1cd277",
      "53fbd46c72504c7089b16c5f9af553f6",
      "b35fb4d798fe465d8e7b59fbe5ec9e3d",
      "67c9368621b04af286287a8b0ff8ffae",
      "4372335bcb814028bf76e3a9cb8d866d",
      "2f27069fa8bd4e40b71d1bc1a80c8880",
      "ba1d973f93c04caa80933d19e3c39a97",
      "144b297a47bc4c42ad4096085d4d3e38",
      "72e4d5ca12b54892b97f3479382cb4e4",
      "b1c2dfbca0f14ae6bdcd4a590ccaa0a5",
      "3742289f00fb4f579f0a9c01439f030c",
      "d7a6d5a9f3bd4498808cd83002c5aa8b",
      "d88450daa2c24ed2bba0a4a011dd5d67",
      "cd6f0066de6f447a99e645ccdda97582",
      "0a928e1eb0c44956a5bcc90c9551ccc4",
      "befbfc30c06e422ab528f6b954ffe645",
      "8d1f3b95b9fa497a81e5189557314149",
      "a0c6f96862e343039f5385051f1dc242",
      "4c8b303f303e450e862ae5c77c497836",
      "39516240b5454077b77e902074e4670d",
      "c16f0fd08c784255a088ffe31102e883",
      "9be1786b0cf942bea385f23b46f6d115",
      "0fb804425f974c108e5a5902aa396401",
      "f33ca4280a6d4b2fa0a37ebf832de747",
      "c3f743a4270b402fac2e9efbc87e2800",
      "3c0c4962e83c4112820dbef696139775",
      "975728159a8a4b3a9a142359e7583641",
      "cb029fbb8441416d8d7655b9c5ca687d",
      "3e9cbd1f5d3a4d1f935253698100acbf",
      "b2c415aa03914471a8b81ad403c18a2f",
      "587349f2fce8499c86ff29f721db7d75",
      "b4af060cb7034bedb2c68c2ff31028e7",
      "3f2263f2c3134772a32758e228fed94d",
      "b4a49f3a221a428a852ca0edfbc364ff",
      "f4a4e70a677245a186baf4345021bd51",
      "08936a381e614d9593274e1acc01eeb3",
      "1d890670645a4fc99c1cee62323e7d49",
      "694bdb0845f147c58e539592f4c722f7",
      "76908916ffcd49fca0aa9775b018de54",
      "15616589169044bdbfbed4971c391c82",
      "e68eaba93a2e40a3a986e90791063e0c",
      "d695a75e4e94413f987b5b37932a16f5",
      "ed7749afe0f4408e9dae28aa0a68b9b8",
      "8862092e719445878a644ff3fd3cdacf",
      "da01223755744150a39aed7c200bf873",
      "9b690db90ff243ffa4de529857e8552c",
      "b65402d8507e497da3171088e71c7baa",
      "57687db523004322be50c5bb32a7b667",
      "c37e0ce0f16b47d1af2669dff1e7ebd9",
      "3e0416edfceb400fbd58bbf0261233eb",
      "008a47fb54ff4dedafe44250e1d26d41",
      "d15d98c9ad26493e90f00b5b1381df85",
      "5db97d63b9b641d5b80370f32aca8a0b",
      "fac5ffb4096f4fcca72d71044074a49e",
      "fab0a9ff6a494c1eaa7756a8ea726bbc",
      "6230887f63c0489ba26ab8ab748ec733",
      "fb833636f6a14285b16e265e6f70c8f0",
      "a4d24095191348539db0d7cfee0acbed",
      "92438aa83448425f8bd84a79888989f8",
      "ee36a2d585964f7391f0e3bfea7304af",
      "6dcece470abd451999c8112aae626a48",
      "8c78a708b7f8412c8a244b7e3a2cb8ed",
      "84bc257516874aaebd867eb5777077b3",
      "acb3d41e81604669981d5d9313773ffe",
      "20553870a0ef42d89b957ff333f71fc4",
      "24bfad5204a7417195617140cf35a61b",
      "c65b57a9f76e40088ef4127e321ec4cd",
      "297de5a646f546c1b48877276e94d519",
      "e9f546e5f4d6492993624d13e6e6c9ce",
      "3b79b56887c54e0ca73756fbf7eee514",
      "8bb1f0f61327481fa42351e1310c3f95",
      "45c3a1e82af448e1b5a5eae3b2267497",
      "b8f0e4011743460a826d51965818c754",
      "ce9e3d57c8d446668d9a53800751d5ba",
      "cf77fbd025504c5a9474e07efbba83ed",
      "1210edade34a4d26a473fee04b1dc0df",
      "f8a3d4962d884b10aee3cd2e2d6b9ec8",
      "fefc2a8595c34990bb5375d8b8fb51ef",
      "3dc713241c684e05a6d6ca2eba6a438c",
      "baf04f735b4f4bc0b77500b7414dde67",
      "1286e9831ccf4d5fb2bec9903ca1248b",
      "9e55ba8210d545a1aa44143baa4678da",
      "444be72c634e4d1d84cb37dc12d1c751",
      "b7924a8cd18b4026819804c946946d01",
      "226f9e2382de4c989b1e1ad3dc29df4a",
      "0325035b4ec14327915691c9eed55706",
      "13d186e52ec04f72854c6050062ba7ad",
      "3406962051f54321b1c60f7b9c43f5e7",
      "e13cf8493cb94d86a08e2f798cedde46",
      "4117ee5eebbc4e01a14ffde86041d6ef",
      "4e32c97170154957ad01362eabab1ac2",
      "d2bca7f2d79a45ce84b5c6fd1934d94a",
      "27393323596a4aa7924a31951b644b4b",
      "f48c314cacf44b30a5496284ce26495c",
      "e34ddd0a483a410c8f3b3d981b45e921",
      "ba5ae495a3d04f1ca6909d4ec0292fd7",
      "cf6616bddfe04a9092f3f9e1526341bb",
      "3bef5926a6d742928aa348ec44e189dd",
      "8eca63a6a85142e58253ffbd1a5b132b",
      "db04e204dffa4e19a3598e334aae511d",
      "4116617fbdf648ceb82454a70b0fa1a9",
      "c6b808f93e0c435e80d3c1865e4af4c1",
      "b4176694278c4419a2d10bfa66883fd4",
      "443a8707f1054c8d9aa29ddb1b4cffd9",
      "a0bf504f4da147b8aee0edbe1558a138",
      "ffac91c00bd0459f89f3e7ae28d10d7a",
      "5c1eee062a314889b000ac3ea2f38cab",
      "90676d87612f4549b9711470d6358a00",
      "475d2d68d84841bb94abf2b7a85ebb19",
      "5c79ea7ffbfe46918aaea3f12f13e688",
      "17dfa96e114247e18d376077506bc84c",
      "738d8bfed75846e7aa11f76192683256",
      "b06a3c9aa56e426ab8fbf80e32c65682",
      "27f234053dc24be1ac5bbf8ca67ce70e",
      "31bc173327e8479e9e8468892abd4d3b",
      "4bd022993f834919a3512c3351802f0d",
      "1f2fc63df51746b68dfb0e518940d6d1",
      "495bc5b3fa6f4d77a3531d36534fa66f",
      "f6369b37e67f45d0b6be2b2ce2bbd9e8",
      "264d9572750c46598746fa95bc7b594b",
      "fc4815571d0041938a165ec111e9eab1",
      "b65420c527a54197a88b990c431a5fe3",
      "dc214f1af7ee4124a9ce9751bc54c66b",
      "ce46d94af52d462fb7c0edd651de2d2b",
      "d8000434312646cab2051c42ddab1c08",
      "fb2bb8b5587c4f53b624514ac4b8c85e",
      "74762f3876924d2a82816c116ac7421e",
      "0fd9ac85b08b496dace04554569f4855",
      "42d71d2d96584da987cca2c3ce91375e",
      "b102703a926240acafe0671b5fd2e616",
      "de639784d72a414c939a373fd1e2cc69",
      "6390906c1cfe46e9b5ed7aa2bcdceadf",
      "3bf40e8d9e544bf1896e4359c730cbee",
      "1510302dab8c4f1382de5429da0f7f5c",
      "6c77184f096c4aa5b870b2954a366940",
      "20ac4d7461f04edc8002e7471f442bb6",
      "41de00a8c2f44b0bb6cc35be8ef536d9",
      "927bbc8bdd544d14a1fdaebb642f9f64",
      "d36d614272744b64ae43c32c4871376b",
      "f3f5efccb3664ee58d39de0a936c071d",
      "ad7e1243281f448eb18d3a476cb88af9",
      "cf575e9f532644088f07737979eb385f",
      "6eb21602434741a1a20bad4886a290b7",
      "e236efd187ad4bba855a083e83b1dcc1",
      "8ca909f9e2e94e52a9951bf3bbdf7fa3",
      "2dfe832c7fb343b1aac0a197dcdfe34b",
      "dd3cc46dbe5541489f0649fcb635b41b",
      "e731f04094a64c8dac941898607db959",
      "0f0e7ed2c1424ffeb5b9a5dfd1edbe2c",
      "fac878a299384c7b9d51e3b14688b058",
      "9695fcf54d1a41cab1232eee19d49627",
      "5b926c29f34c47f5b0754e8397640ad0",
      "16987725b85e4084b9699ba081086683",
      "ddd9253f7f004fff88f941145e2cda67",
      "67d41562d04c4692bba686a329780ea6",
      "3d6abe60e42d4301959d2d6a079720d4",
      "74740ee62e8d4da79e719b3112051940",
      "8b6240da4cf8492aa2f5384e4019e5c1",
      "5e49dc2f9ea84c1a926672e872a9a588",
      "4df99d1c44774130abcc80c65df471fd",
      "fe20de482685492fa5afab37af655d77",
      "04d21b4fa3654610916dbbdc66ea8ba4",
      "e78b0b3cbc424a5e8d04421809c3949b",
      "b51e38baab544ab29ce2027b68a38d70",
      "004e165be49a40079f4b3509508d7552",
      "0436410ea6ff454281d50e8d13f6d119",
      "e27e00810c9347c38538c0a470947cfb",
      "a9838056b2eb4ce6a1a360535cdf7413"
     ]
    },
    "id": "022XES4M1jbz",
    "outputId": "d5f5690c-fcec-4d58-8319-9055dadc85cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30d96c18a29c470e9d249fa256777f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f38fac3a834a42ad0334405da473f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68a8c6454f4ae88cfadc600966fbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72efbfeb861a47bc92e8565a84ef0a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c9368621b04af286287a8b0ff8ffae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a928e1eb0c44956a5bcc90c9551ccc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0c4962e83c4112820dbef696139775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d890670645a4fc99c1cee62323e7d49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57687db523004322be50c5bb32a7b667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92438aa83448425f8bd84a79888989f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b79b56887c54e0ca73756fbf7eee514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1286e9831ccf4d5fb2bec9903ca1248b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bca7f2d79a45ce84b5c6fd1934d94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4176694278c4419a2d10bfa66883fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f234053dc24be1ac5bbf8ca67ce70e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8000434312646cab2051c42ddab1c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ac4d7461f04edc8002e7471f442bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3cc46dbe5541489f0649fcb635b41b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6240da4cf8492aa2f5384e4019e5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1925' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1925/2625 22:26 < 08:10, 1.43 it/s, Epoch 11/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.927502</td>\n",
       "      <td>0.572347</td>\n",
       "      <td>0.338694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.869543</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.321398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.083200</td>\n",
       "      <td>1.754803</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.512653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.083200</td>\n",
       "      <td>1.746235</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.549967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.083200</td>\n",
       "      <td>1.661434</td>\n",
       "      <td>0.684887</td>\n",
       "      <td>0.649421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.688900</td>\n",
       "      <td>1.738210</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.611021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.688900</td>\n",
       "      <td>1.802966</td>\n",
       "      <td>0.688103</td>\n",
       "      <td>0.654966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.688900</td>\n",
       "      <td>1.925128</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.594607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.325600</td>\n",
       "      <td>2.038062</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.580934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.325600</td>\n",
       "      <td>2.046163</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.599114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.325600</td>\n",
       "      <td>2.080995</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.617096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5857\n",
      "Final accuracy: 0.6116\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 777\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1d4cf83f4e144e6a8c7aca57bb4e62ff",
      "85e5069cd2f44963beb249c24e409e66",
      "daebda24750048f9ab5597c7634db486",
      "4c406d4a0cf44b7c90709165db4cd546",
      "cc098404d3024f0dacc92a9c97726e23",
      "040e3f2e35ed41db8160528a6a191ff2",
      "b5b5ee5c4569429a9fde9a0c0e9109a3",
      "a55677dc514d48dda62aae04e62f9be8",
      "735bfc59f6fb4de8b0ff9b798179fe9f",
      "9a63cc17624044719aad36b5e52e48f7",
      "cb2e94a165ac4a5db842af5436f8b0e8",
      "f1d7ed258195474faefb0227d3dca726",
      "ba4f8c90e474485d9d01305d48892e4e",
      "8c039cb9917b45caa1f1122f712b91e9",
      "9718f8d2f7474a059621d2a626c7f6a1",
      "215ebd228c9d4edfabb74be7ced81b57",
      "88afa1275c6548fabc2437c9a52c00e7",
      "8d686f5fe8e645d09fa6f4d5516b949e",
      "da2723eace99416d82573641b351d17e",
      "1c064933786f482d980b11daf592c0da",
      "c331a1bd25d3455ab048822c270d0c3c",
      "7003b33f595e4e40b490846a423abdc7",
      "49aaec0815144dca8a9f0ca12464a162",
      "5c0d7d31da7e40e5b9a823568ddb2078",
      "30fd2db9517a4ef9b2316ee532f9c09e",
      "6794ec8df4f446c0a6faee95fcdb1af4",
      "f0847a3327934d5da3ae0c67b00242da",
      "a6ccbf0f7ea44b3ca19c8385a9ad22af",
      "3f133568bf934ba1b709e38db831c36e",
      "83e9800dca1d4ea9b7d7fef975177fe9",
      "f4434ed24c4a4325ab59310de616d072",
      "df289d67555a4a8a97120403c8c7f462",
      "1c335dfb34fd44e699a15536b511d4eb",
      "4b7665d5f3a749f3a5a620f3f9c84b56",
      "57a5a3c42aac4e6d98be98f8714afa11",
      "5174c96864c14a138259d269bf903723",
      "97ae694cad3c4c3292e3d7a537620085",
      "83ec052d8fe34c72aac6d0168ba650f9",
      "97ed5a721a804142ae4933168f3b15b5",
      "70d28859c1084aa08383a864fff5618b",
      "4f6d33e0e3c04c8f905be64e4cc51394",
      "301ce2e40c474cd79cc87266cb079bd8",
      "f20eca506ec94e48b014fb54d183d3dc",
      "af1dba52e11943f8b8fbd5f297b156f2",
      "c10e6cb20fce49cb9a9d767607b7978f",
      "832e98e278bf4ddcba2f38471a0fbf79",
      "e23b90b93a0f4bb8836e7d3abacd3913",
      "d0c240a2ad5e476c930de8ac77195285",
      "2994493a02734e02a86719d968c0d164",
      "de6f98570559463b90653282ace0c863",
      "2c1dcf03c0a94935983109ddd42cc68f",
      "c6bf87ce2ee64315bed4c6286f51b831",
      "4d9c423964c64ca7a2f8e1017c5cfa6e",
      "902cc11dc41b4acbb70a306f088a5e05",
      "830914ac838449a1bb4dbac97199101c",
      "8248f0ea5f91447693ae26fe44fdd0c8",
      "1b6c1884a9484a3aa08373a3cea81469",
      "06c1870b03b7483687617e36a50e797d",
      "b15722fc4978408eb895b28e35900f8e",
      "5caba9acdce14cfc916e2c72e43b3c4e",
      "c27cfa06e2b24648a2b206302f74cdd8",
      "83932c97377e42358d202e43980b41cb",
      "198ce42cb52b42fd95d044dfb8252281",
      "afc9d0bf998143fe8260871ded7ff8e5",
      "4a98b71e274648b5a1af46af499d2ba5",
      "3a97a8ebb2ea4b3296ebbeb6d090587a",
      "153ee6f4d9294eb7b135cd0ef7f12e11",
      "39d287a95c824a06bff4f1d3d84d780a",
      "0a2109a440824f68a48508052247d9e2",
      "2682f780c2654717b3b11dd790b1d15a",
      "34998c9bb1434520b4826e1a9ce81099",
      "06cf6ad58ab042138639799c50dbdbc8",
      "d71020aa73b243e3bd379694799385f8",
      "829efa32cd1a44b4a44a7b8f40f45bed",
      "aff85389f24c434a9bab2933ead3270d",
      "dcc4ba8dba1f4626a1ba212d10a2a2f1",
      "0b8239501c4e4c668772776cdca0c9a9",
      "73a74d92d34f4a3fa97449e8fe33f69f",
      "944022aa975144a19cc83092964bef03",
      "b9f72cc5b0d140cdb82692463cd68777",
      "c704b4c4210a4fa182ddb58b00c8b8d5",
      "0309b5acddf142c6a481b9494e0ffccf",
      "01a28ee7be97429fb24c8b0adfd0b3de",
      "8629c2d7d4c84c519803d6d7e5b8d8c5",
      "23ba89a726c44b4c9cacd053138b2f68",
      "79796fd8ba6845d38d4c51a3afd886b7",
      "d43861efd2d54b658e25005d1b64768c",
      "f05412ef176f4c02a3338a0448bdb959",
      "6a3755e32bc74b6ba1c5fe6d8aa9c6b2",
      "585da4bf235b43b0a98980c2e7947d30",
      "d605ef4c9ff14207a99529ab3921a244",
      "a83ac97af80540e290612e0bfae490e4",
      "474d3682d24c4bc1825f33d4b6505768",
      "3fdd8e85557841e6a3410720e9df19bd",
      "3db25b7719294bb499af3b8c640a2de4",
      "e974a4aa6bc74187939220b555110050",
      "9863f939ff4549afac29b91b1ebe4e36",
      "1262c19a75624b41ab32e586f4661cef",
      "2d7b5f30503d4248836a594e7e2844df",
      "1f309813e9a4411abc70f79c9f65838e",
      "eb964db191464694a5ed52a4528bc864",
      "20c0fe3e54ac47c8a5fbe2b96abd7a2d",
      "34754fe806e6465f92e177fc11fc6980",
      "c95c5c85c9064510a1c0ff338ddc61d7",
      "1f986cd9183847419ffd882891a84a21",
      "1731bf90c26a4e70b5b3b5f6b26c0e2f",
      "7e9004e82d5d4846a67f5dc4bdea63ba",
      "74aa1072bd5648e6a1e98f3457225cb0",
      "cd358453c65040f1929ea6e9d4c0a61f",
      "e07db52912f44bf782fa21a3f7345315",
      "4cb7706474a54b498a94946b47620f8e",
      "eeec062414a6410581da385e93ace681",
      "dc66fd17f50048ad9b2d7c6947d53a1d",
      "c47e7469e5644e6bb6cb0536a6ee2c4d",
      "4b264ca582e8457e99f98e52de1099bf",
      "cfb53920fd1642f4ad030d52cd0e51e3",
      "b51a9eadf049482b994f686aaeb33fa6",
      "c924cf7556c34aeb9f2be275b2870479",
      "bc095b52cc074a759c0620663f0c801f",
      "00e1c592a80345f7997adb3d6ae6edb7",
      "10d5c303ec63467e9d4ac3afe975651c",
      "416a5efeebc74104bb8c77f2bd416068",
      "f13d7576783643adbe20840381eb4d70",
      "f75ae96e865643258e0f4bf2b65bc956",
      "2534a3e40da24585838f4bd009c978ea",
      "0c15929e0949406b9b77157397dfcffb",
      "d18e3a1abe01434996107c6622cc534c",
      "3fd00eb493a244cbb0a1d3bd0c9c1b5d",
      "04f68d10508a451da5de927331306d62",
      "dda13f0a009e4b3f9e63fc80a5b18ae9",
      "e1398d5506014d9bbbde9abcdc97e3a5",
      "0e58bd500e4d4cbb9f006d97d0566973",
      "9e9e7f0b0c444c53b3ed7d792e2c6779",
      "e2ca07a1fcac4df3b50dc7353699e16a",
      "464f618a4d414d1f8791d0b296dc528c",
      "b707048ddb674aef9a7eb3d2653ea80a",
      "e7852406d0634b6c8eb2c00156d38e95",
      "e57988f168c34120a9aa6bb4dc39b362",
      "c4205ab3b97b4ec781e0e1238489414c",
      "09b3e662356c4d5297d8e5e6e3f9be5e",
      "9ddc3934d7134e3b8ad6d4e44ac7b593",
      "fc0e4abe57bf494c94cfab6fc653dc41",
      "4486d82aa3b8441cb059c6997b8a4140",
      "42f141d9a326466b8866ae44af70bf30",
      "11eb801ba4b744b8ad1dbac868dfa769",
      "94baad2c1c6b4d91a38586cc441db0d0",
      "46c02ba0bc5d4c29a167f0afd8c2cef4",
      "a3661cd0a35f4556820ccb3822f61e1b",
      "4d872b0c001f42f0a7859fb668b22e21",
      "eed0ed488e8c404d9de562916d702bcb",
      "6d86fe24391f43ce84d0349c5625ca95",
      "3a81e3f8d50a466e8d73f86854d0fa10",
      "fcbd8952c01c436796ace0720131cdd0",
      "9c2b20661d7d4b8dbc451a1481fecb06",
      "4adad06fa837424d8e47163565ba0233",
      "e9e273d70d8945999dc4f7eeed9a3a04",
      "410ccd82ab824c5a83e289ff7da67edd",
      "e0a06f68ae5e49e99ababef6eb5df51a",
      "fe7e88253300449f8e54ece819a2416a",
      "3e710d9edbd54580846174b0c4881e03",
      "aa34e320874848a486a84eeac35f1aa0",
      "ac801f36611f46f69f3c9a557b7409e3",
      "5af2bc8b4d4346d8ae3e85052d1a8a6c",
      "a84e151ffc65406b98d98b9af6e2aec5",
      "573619eeb0904c16b84560c2fbbc4e09",
      "38df17e3e8344972ab07013f983f17a8",
      "d6d9b617ebea48dda72efaa5bbd4a241",
      "ff6a97ddfcf04838b8096a98faf844fb",
      "8efd59a424d14abb8490e8920281ba8c",
      "a90091f9b55e4e5e871be5a257ba4981",
      "e6f0758414344d5da1940ff1b6298eb5",
      "66f8ba00709445ada1009f0ce3f1015d",
      "d2aa5cc621d74c3fa870fc2dd8a1d3e8",
      "0a20d38808304dab9496dc4560a91965",
      "7091e9a6a1004ba6b7d8c245cfb80893",
      "ea071406114341c99f3c828fe1ae8aff",
      "3a22c610eb814a11acb319e06b730764",
      "fcd99e6aff7d4f0f8ea68c8bce9e3cd8",
      "fe1daaebd5844513a73716ee609e3231",
      "16b4b42dc865410e8f731ca03920bb42",
      "b3a2e851f9b742aaa58a11a585bafd35",
      "574850751a7141eea90b1c0f289b3865",
      "7cd274f976c54779a4f4671026d71056",
      "659ceb0f0d8c4a7898fac738ed44a67c",
      "4792e34bdc8e4dbcbfbeda855374e9fa",
      "71a31c64ee3d4c7ab8455e1769170e79",
      "3fbbfcb1a0784ce892e55b8c073ce4eb",
      "0d79328e73c54c2c8b992b142c303864",
      "97844b2ed7e448d9b297bc7671fb7a9d",
      "a4c336603aa7498794a4c7eb393d9718",
      "70d9eb73ca9d4f7986eb7383e29641e5",
      "a17748159e1549948d831111419b0ec1",
      "cf60bc3dd0314c92a9c119b2c7aabbb9",
      "8cdbae05ef1443b2b5b76a165cade6d8",
      "085179ab5e614d2f86fb959347df7e11",
      "a3ef0b946a5a470cb9cda31577a1f215",
      "c834ded2efea4c56901a82fdf20efdce",
      "9af646bc70ff409cb2c33ee442f6bdde",
      "d2aa796495e043b68c01825f60a23909",
      "054f4d56fc8449b1aefca79356cbe2f3",
      "eb06601d2add460c83b65e05a41c147a",
      "02bb76f2da06419d977a919075afaa8f",
      "ba938ae5c5d1457d9082c7b59a13fa10",
      "708a77f991534dc7915834dde39212eb",
      "098bc6613a36450abb3845f90879545d",
      "3aa5f60229a6419486373c944cdfc827",
      "61b3fb6a1e8e4edbbea43b9e09bd946a",
      "b69d7d6700b6492a936b41e16f9fba6f",
      "9e25e82e25c34c26980d4e68cec932f0"
     ]
    },
    "id": "M3Rq9IT9Xfbj",
    "outputId": "c08b1961-8838-46e6-d4aa-f8b0caa9e607"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4cf83f4e144e6a8c7aca57bb4e62ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d7ed258195474faefb0227d3dca726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.90M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49aaec0815144dca8a9f0ca12464a162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7665d5f3a749f3a5a620f3f9c84b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e6cb20fce49cb9a9d767607b7978f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8248f0ea5f91447693ae26fe44fdd0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153ee6f4d9294eb7b135cd0ef7f12e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a74d92d34f4a3fa97449e8fe33f69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3448 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3755e32bc74b6ba1c5fe6d8aa9c6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f309813e9a4411abc70f79c9f65838e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb7706474a54b498a94946b47620f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416a5efeebc74104bb8c77f2bd416068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9e7f0b0c444c53b3ed7d792e2c6779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f141d9a326466b8866ae44af70bf30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4adad06fa837424d8e47163565ba0233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2792 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38df17e3e8344972ab07013f983f17a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a22c610eb814a11acb319e06b730764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d79328e73c54c2c8b992b142c303864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2aa796495e043b68c01825f60a23909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1925' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1925/2625 22:47 < 08:17, 1.41 it/s, Epoch 11/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.048775</td>\n",
       "      <td>0.511254</td>\n",
       "      <td>0.296836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.851926</td>\n",
       "      <td>0.546624</td>\n",
       "      <td>0.361918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.094800</td>\n",
       "      <td>1.817466</td>\n",
       "      <td>0.553055</td>\n",
       "      <td>0.471038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.094800</td>\n",
       "      <td>1.707093</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.552554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.094800</td>\n",
       "      <td>1.692440</td>\n",
       "      <td>0.585209</td>\n",
       "      <td>0.542180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.718200</td>\n",
       "      <td>1.776757</td>\n",
       "      <td>0.665595</td>\n",
       "      <td>0.589332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.718200</td>\n",
       "      <td>1.812355</td>\n",
       "      <td>0.659164</td>\n",
       "      <td>0.612058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.718200</td>\n",
       "      <td>1.929861</td>\n",
       "      <td>0.639871</td>\n",
       "      <td>0.580361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.438700</td>\n",
       "      <td>2.006598</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.583387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.438700</td>\n",
       "      <td>2.112850</td>\n",
       "      <td>0.617363</td>\n",
       "      <td>0.562856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.438700</td>\n",
       "      <td>2.172122</td>\n",
       "      <td>0.623794</td>\n",
       "      <td>0.567312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5901\n",
      "Final accuracy: 0.6406\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 77\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 757,
     "referenced_widgets": [
      "3bca31a09a4048c1b2685effaa61962d",
      "cfbf5d4849734779984660f8e4f9c313",
      "6f50970fb6b64086b8b5092b7af61b57",
      "56d29060ac98475489573db036e3f130",
      "7b2148b7b8a9417dae845539f5eb27e6",
      "776ce74ebb6c4a6abc9fb6e775831178",
      "2c684d4bc92148fe9a89ccb01603b920",
      "a520557dee964b55aa6fcd252afded17",
      "c58484a85c1b4ed9b2077974ae1c225d",
      "d38886ad4fa64e3b82222e982225964e",
      "657075218ef24cb2ae42b2f6f62c9c9c",
      "b2f19bd68b7641cdb5ba24ddee06f07e",
      "a99ec4e306d64c3b98acaa7bba6111ba",
      "9a9d96af83694f598b90c559b251bb76",
      "83ed01506d4d4f3bb15edf86dc1d03c1",
      "0dfdad6424534a98af8354eacf6ddade",
      "57fa0ae4cd2548b6888b5cf3aaa5339a",
      "013820ba4a9a445b961bb54e062aa4ed",
      "e590aec727ca4c58a412341b00892dff",
      "eecd86bc6bdb4dfbaece8c43287f2713",
      "0756ab4d43da48468c7cf79c53a23e67",
      "392126eb27824d91a41aa82d91dc669b"
     ]
    },
    "id": "iicaJlS0jlbI",
    "outputId": "ee2f9e57-bdc4-4105-8db9-44e663a2ea3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bca31a09a4048c1b2685effaa61962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f19bd68b7641cdb5ba24ddee06f07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 32:26, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.065142</td>\n",
       "      <td>0.527331</td>\n",
       "      <td>0.287469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.853186</td>\n",
       "      <td>0.549839</td>\n",
       "      <td>0.346790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.093400</td>\n",
       "      <td>1.767118</td>\n",
       "      <td>0.565916</td>\n",
       "      <td>0.486074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.093400</td>\n",
       "      <td>1.705550</td>\n",
       "      <td>0.601286</td>\n",
       "      <td>0.544831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.093400</td>\n",
       "      <td>1.728383</td>\n",
       "      <td>0.604502</td>\n",
       "      <td>0.535767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.725600</td>\n",
       "      <td>1.770541</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.560313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.725600</td>\n",
       "      <td>1.816776</td>\n",
       "      <td>0.588424</td>\n",
       "      <td>0.553472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.725600</td>\n",
       "      <td>1.800744</td>\n",
       "      <td>0.620579</td>\n",
       "      <td>0.617610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.902146</td>\n",
       "      <td>0.630225</td>\n",
       "      <td>0.622627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>1.984187</td>\n",
       "      <td>0.614148</td>\n",
       "      <td>0.614356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.460700</td>\n",
       "      <td>2.018240</td>\n",
       "      <td>0.627010</td>\n",
       "      <td>0.623141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>2.069046</td>\n",
       "      <td>0.636656</td>\n",
       "      <td>0.636203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>2.095290</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.639327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.209200</td>\n",
       "      <td>2.098442</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.638178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.086100</td>\n",
       "      <td>2.099358</td>\n",
       "      <td>0.643087</td>\n",
       "      <td>0.637913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final macro-F1: 0.5953\n",
      "Final accuracy: 0.6377\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets scikit-learn accelerate torch pandas\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "# seeding\n",
    "SEED = 77\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# cuDNN determinism for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-large\"\n",
    "OUTPUT_DIR = \"./deberta_task2\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "GRAD_ACCUMULATION = 4\n",
    "LR = 8e-6\n",
    "EPOCHS = 15\n",
    "\n",
    "\n",
    "# data\n",
    "dataset = load_dataset(\"ailsntua/QEvasion\")\n",
    "\n",
    "def preprocess(example):\n",
    "    clarity = example.get(\"clarity_label\", \"Unknown\")\n",
    "    if clarity is None:\n",
    "        clarity = \"Unknown\"\n",
    "    text = f\"Context: {clarity} | Question: {example['question']} Answer: {example['interview_answer']}\"\n",
    "    return {\"text\": text, \"evasion_label\": example[\"evasion_label\"]}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": (preds == labels).mean(),\n",
    "        \"macro_f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "    }\n",
    "\n",
    "full_data = dataset[\"train\"].map(preprocess)\n",
    "if \"test\" in dataset:\n",
    "    comp_test_ds = dataset[\"test\"].map(preprocess)\n",
    "\n",
    "full_data = full_data.class_encode_column(\"evasion_label\")\n",
    "\n",
    "# train / dev / held-out split\n",
    "split1 = full_data.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_dev_ds = split1[\"train\"]\n",
    "held_out_test_ds = split1[\"test\"]\n",
    "\n",
    "split2 = train_dev_ds.train_test_split(\n",
    "    test_size=0.1,\n",
    "    seed=SEED,\n",
    "    stratify_by_column=\"evasion_label\",\n",
    ")\n",
    "train_ds = split2[\"train\"]\n",
    "eval_ds = split2[\"test\"]\n",
    "\n",
    "labels = train_ds.features[\"evasion_label\"].names\n",
    "label2id = {name: i for i, name in enumerate(labels)}\n",
    "id2label = {i: name for name, i in label2id.items()}\n",
    "\n",
    "\n",
    "# tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "eval_ds = eval_ds.map(tokenize_fn, batched=True)\n",
    "held_out_test_ds = held_out_test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "train_ds = train_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "eval_ds = eval_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "held_out_test_ds = held_out_test_ds.map(lambda x: {\"labels\": x[\"evasion_label\"]})\n",
    "\n",
    "\n",
    "# class weights\n",
    "y_train = train_ds[\"evasion_label\"]\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# trainer\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_tensor,\n",
    "            label_smoothing=0.1,\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.05,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# evaluation\n",
    "test_results = trainer.evaluate(held_out_test_ds)\n",
    "print(\"Final macro-F1:\", round(test_results[\"eval_macro_f1\"], 4))\n",
    "print(\"Final accuracy:\", round(test_results[\"eval_accuracy\"], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHqOzfRDjlrf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
